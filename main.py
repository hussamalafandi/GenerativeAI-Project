# ðŸ“¦ Ð˜Ð¼Ð¿Ð¾Ñ€Ñ‚Ñ‹ 

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch import optim
from torch.optim.lr_scheduler import ReduceLROnPlateau
from torch.utils.data import Dataset, DataLoader, random_split
from transformers import AutoTokenizer
import wandb
import requests
import gc

# ========== Ð“Ð»Ð¾Ð±Ð°Ð»ÑŒÐ½Ñ‹Ðµ Ð¿Ð°Ñ€Ð°Ð¼ÐµÑ‚Ñ€Ñ‹ ==========
# MAX_SEQ_LEN â€” Ð¼Ð°ÐºÑÐ¸Ð¼Ð°Ð»ÑŒÐ½Ð°Ñ Ð´Ð»Ð¸Ð½Ð°, ÐºÐ¾Ñ‚Ð¾Ñ€ÑƒÑŽ Ð¼Ð¾Ð´ÐµÐ»ÑŒ Ð¼Ð¾Ð¶ÐµÑ‚ Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚Ð°Ñ‚ÑŒ
# BLOCK_SIZE â€” Ñ€Ð°Ð·Ð¼ÐµÑ€ Ð²Ñ…Ð¾Ð´Ð° Ð´Ð»Ñ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ñ: MAX_SEQ_LEN + 1
MAX_SEQ_LEN = 128
BLOCK_SIZE = MAX_SEQ_LEN + 1

def clear_gpu_memory():
    gc.collect()
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        torch.cuda.ipc_collect()

# ========== 1. ÐÑ€Ñ…Ð¸Ñ‚ÐµÐºÑ‚ÑƒÑ€Ð° Ð¼Ð¾Ð´ÐµÐ»Ð¸ ==========
class MyDecoderModel(nn.Module):
    def __init__(self, vocab_size, d_model=256, nhead=4, num_layers=4, dim_feedforward=512, dropout=0.1, max_seq_len=128):
        super().__init__()
        self.token_embedding = nn.Embedding(vocab_size, d_model)
        self.position_embedding = nn.Embedding(max_seq_len, d_model)
        decoder_layer = nn.TransformerDecoderLayer(d_model, nhead, dim_feedforward, dropout, activation='gelu')
        self.decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_layers)
        self.output_projection = nn.Linear(d_model, vocab_size)

    def forward(self, input_ids, attention_mask=None):
        batch_size, seq_len = input_ids.size()
        positions = torch.arange(0, seq_len, device=input_ids.device).unsqueeze(0).expand(batch_size, seq_len)
        x = self.token_embedding(input_ids) + self.position_embedding(positions)
        tgt_mask = torch.triu(torch.ones(seq_len, seq_len, device=input_ids.device) * float('-inf'), diagonal=1)
        tgt_key_padding_mask = (attention_mask == 0) if attention_mask is not None else None
        # Ð¡Ð¾Ð·Ð´Ð°Ñ‘Ð¼ Ñ„Ð¸ÐºÑ‚Ð¸Ð²Ð½Ñ‹Ð¹ memory (Ð½ÑƒÐ»ÐµÐ²Ð¾Ð¹, Ñ‚Ð¾Ð¹ Ð¶Ðµ Ñ„Ð¾Ñ€Ð¼Ñ‹)
        memory = torch.zeros_like(x).transpose(0, 1)

        x = self.decoder(
            tgt=x.transpose(0, 1),
            memory=memory,
            tgt_mask=tgt_mask,
            tgt_key_padding_mask=tgt_key_padding_mask
        )
        x = x.transpose(0, 1)
        return self.output_projection(x)

# ========== 2. Dataset Ð¸ Ð·Ð°Ð³Ñ€ÑƒÐ·ÐºÐ° ==========
class TextDataset(Dataset):
    def __init__(self, input_ids):
        self.input_ids = input_ids

    def __len__(self):
        return len(self.input_ids)

    def __getitem__(self, idx):
        x = self.input_ids[idx][:-1]
        y = self.input_ids[idx][1:]
        return x, y

def chunk_dataset(input_ids, block_size=129):
    num_chunks = len(input_ids) // block_size
    input_ids = input_ids[:num_chunks * block_size]
    input_ids = input_ids.view(num_chunks, block_size)
    return input_ids

def load_dataset(tokenizer, block_size=129):
    url = "https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
    text = requests.get(url).text
    encodings = tokenizer(text, return_tensors="pt", truncation=False, padding=False)
    input_ids = encodings["input_ids"].squeeze()
    chunks = chunk_dataset(input_ids, block_size)
    dataset = TextDataset(chunks)
    return dataset

# ========== 3. Ð’Ð°Ð»Ð¸Ð´Ð°Ñ†Ð¸Ñ ==========
def validate(model, val_loader, criterion, device):
    model.eval()
    val_loss = 0
    with torch.no_grad():
        for x, y in val_loader:
            x, y = x.to(device), y.to(device)
            logits = model(x)
            loss = criterion(logits.view(-1, logits.size(-1)), y.view(-1))
            val_loss += loss.item()
    return val_loss / len(val_loader)

# ========== 4. ÐžÐ±ÑƒÑ‡ÐµÐ½Ð¸Ðµ ==========
def train():
    tokenizer = AutoTokenizer.from_pretrained("gpt2")
    tokenizer.pad_token = tokenizer.eos_token

    dataset = load_dataset(tokenizer)
    train_size = int(0.9 * len(dataset))
    val_size = len(dataset) - train_size
    train_dataset, val_dataset = random_split(dataset, [train_size, val_size])
    train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=8)

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    
    model = MyDecoderModel(vocab_size=tokenizer.vocab_size, max_seq_len=MAX_SEQ_LEN).to(device)
    
    optimizer = optim.Adam(model.parameters(), lr=3e-4)
    criterion = nn.CrossEntropyLoss()

    # Ð”Ð¾Ð±Ð°Ð²Ð»ÐµÐ½ Ð°Ð´Ð°Ð¿Ñ‚Ð¸Ð²Ð½Ñ‹Ð¹ scheduler
    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=1)

    wandb.init(project="my-language-model")
    epochs = 15

    for epoch in range(epochs):
        model.train()
        total_loss = 0

        for x, y in train_loader:
            x, y = x.to(device), y.to(device)
            logits = model(x)
            loss = criterion(logits.view(-1, logits.size(-1)), y.view(-1))

            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            total_loss += loss.item()

        avg_train_loss = total_loss / len(train_loader)
        avg_val_loss = validate(model, val_loader, criterion, device)

        # ÐžÐ±Ð½Ð¾Ð²Ð»ÐµÐ½Ð¸Ðµ scheduler'Ð°
        scheduler.step(avg_val_loss)

        current_lr = optimizer.param_groups[0]['lr']

        print(f"Epoch {epoch+1} | Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f} | LR: {current_lr:.6f}")
        wandb.log({
            "epoch": epoch+1,
            "train_loss": avg_train_loss,
            "val_loss": avg_val_loss,
            "lr": current_lr
        })
        # Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÑÐµÐ¼ Ð¼Ð¾Ð´ÐµÐ»ÑŒ Ð´Ð»Ñ ÐºÐ°Ð¶Ð´Ð¾Ð¹ ÑÐ¿Ð¾Ñ…Ð¸
        # torch.save(model.state_dict(), f"model_epoch_{epoch+1}.pt")
        # print(f"ðŸ“¦ ÐœÐ¾Ð´ÐµÐ»ÑŒ ÑÐ¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ð°: model_epoch_{epoch+1}.pt")

    torch.save(model.state_dict(), "my_model.pt")
    print("âœ… ÐœÐ¾Ð´ÐµÐ»ÑŒ ÑÐ¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ð° Ð² my_model.pt")

    clear_gpu_memory()


# ========== 5. Ð“ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ñ ==========
def sample_logits(logits, temperature=1.0, top_k=0):
    if temperature != 1.0:
        logits = logits / temperature
    if top_k > 0:
        top_k = min(top_k, logits.size(-1))
        values, _ = torch.topk(logits, top_k)
        min_threshold = values[:, -1].unsqueeze(-1)
        logits[logits < min_threshold] = -float("Inf")
    probs = F.softmax(logits, dim=-1)
    return torch.multinomial(probs, num_samples=1)

def generate():
    tokenizer = AutoTokenizer.from_pretrained("gpt2")
    tokenizer.pad_token = tokenizer.eos_token
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    # ðŸ”¹ ÐÐ¾Ð²Ñ‹Ð¹ Ð²Ð²Ð¾Ð´: Ð²Ñ‹Ð±Ð¾Ñ€ Ð¼Ð¾Ð´ÐµÐ»Ð¸
    model_path = input("Ð£ÐºÐ°Ð¶Ð¸Ñ‚Ðµ Ð¿ÑƒÑ‚ÑŒ Ðº Ñ„Ð°Ð¹Ð»Ñƒ Ð¼Ð¾Ð´ÐµÐ»Ð¸ (Enter Ð´Ð»Ñ 'my_model.pt'): ").strip()
    if model_path == "":
        model_path = "my_model.pt"

    # Ð—Ð°Ð³Ñ€ÑƒÐ¶Ð°ÐµÐ¼ Ð¼Ð¾Ð´ÐµÐ»ÑŒ
    model = MyDecoderModel(
        vocab_size=tokenizer.vocab_size,
        max_seq_len=MAX_SEQ_LEN
    ).to(device)
    model.load_state_dict(torch.load(model_path, map_location=device))
    model.eval()

    # ÐŸÐ°Ñ€Ð°Ð¼ÐµÑ‚Ñ€Ñ‹ Ð³ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ð¸
    prompt = input("Ð’Ð²ÐµÐ´Ð¸Ñ‚Ðµ Ð½Ð°Ñ‡Ð°Ð»ÑŒÐ½Ñ‹Ð¹ Ñ‚ÐµÐºÑÑ‚: ")
    temperature = float(input("Ð¢ÐµÐ¼Ð¿ÐµÑ€Ð°Ñ‚ÑƒÑ€Ð° (Ð½Ð°Ð¿Ñ€Ð¸Ð¼ÐµÑ€ 1.0): ") or "1.0")
    top_k = int(input("Top-k (0 = Ð±ÐµÐ· Ð¾Ð³Ñ€Ð°Ð½Ð¸Ñ‡ÐµÐ½Ð¸Ñ): ") or "0")
    max_new_tokens = int(input("Ð¡ÐºÐ¾Ð»ÑŒÐºÐ¾ Ñ‚Ð¾ÐºÐµÐ½Ð¾Ð² ÑÐ³ÐµÐ½ÐµÑ€Ð¸Ñ€Ð¾Ð²Ð°Ñ‚ÑŒ? ") or "50")

    input_ids = tokenizer(prompt, return_tensors="pt")["input_ids"].to(device)

    for _ in range(max_new_tokens):
        with torch.no_grad():
            logits = model(input_ids)
            next_token_logits = logits[:, -1, :]
            next_token = sample_logits(next_token_logits, temperature=temperature, top_k=top_k)
            input_ids = torch.cat([input_ids, next_token], dim=1)

    result = tokenizer.decode(input_ids[0], skip_special_tokens=True)
    print("\nÐ¡Ð³ÐµÐ½ÐµÑ€Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ñ‹Ð¹ Ñ‚ÐµÐºÑÑ‚:\n", result)

    # ÐžÑ‡Ð¸ÑÑ‚ÐºÐ° Ð¿Ð°Ð¼ÑÑ‚Ð¸
    clear_gpu_memory()

# ========== 6. Ð“Ð»Ð°Ð²Ð½Ð¾Ðµ Ð¼ÐµÐ½ÑŽ ==========
def main():
    print("Ð’Ñ‹Ð±ÐµÑ€Ð¸Ñ‚Ðµ Ñ€ÐµÐ¶Ð¸Ð¼:")
    print("1 â€” ÐžÐ±ÑƒÑ‡ÐµÐ½Ð¸Ðµ Ð¼Ð¾Ð´ÐµÐ»Ð¸")
    print("2 â€” Ð“ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ñ Ñ‚ÐµÐºÑÑ‚Ð°")
    choice = input("Ð’Ð°Ñˆ Ð²Ñ‹Ð±Ð¾Ñ€ (1/2): ")
    if choice == "1":
        train()
    elif choice == "2":
        generate()
    else:
        print("ÐÐµÐ²ÐµÑ€Ð½Ñ‹Ð¹ Ð²Ñ‹Ð±Ð¾Ñ€")

if __name__ == "__main__":
    main()