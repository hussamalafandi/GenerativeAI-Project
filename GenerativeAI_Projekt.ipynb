{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c027491",
   "metadata": {},
   "source": [
    "# GenerativeAI \"Sprachmodell\" Projekt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb4417d7",
   "metadata": {},
   "source": [
    "## Verbing mit wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e5700284",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter:\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: C:\\Users\\Adel\\_netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33madel-haj-jumah\u001b[0m (\u001b[33madel-haj-jumah-hochschule-hannover\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "017a1dc1",
   "metadata": {},
   "source": [
    "1. import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "63c66021",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05ce5b3cfbac418a907d11361783fbbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/10.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Adel\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:144: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Adel\\.cache\\huggingface\\hub\\datasets--wikitext. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e1246d3be64471f9da7c015f405a8f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test-00000-of-00001.parquet:   0%|          | 0.00/722k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1f8f319d73d4b5a93f38028067eff18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00000-of-00002.parquet:   0%|          | 0.00/156M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bad402f57614bcc9609b7271e898020",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00001-of-00002.parquet:   0%|          | 0.00/156M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e6b346359e2428b8f8d7507fec69969",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation-00000-of-00001.parquet:   0%|          | 0.00/655k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a53aeddd50884b6c9715ec4e5476666e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/4358 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99624ea6f1a345f1bccf1753c85c2658",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/1801350 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "698cb000ede043c4bc82d790d5d4e9c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/3760 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"wikitext\", \"wikitext-103-v1\", split=\"train[:5%]\")\n",
    "text = \"\\n\".join(ds['text'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe187fce",
   "metadata": {},
   "source": [
    "2. Importieren die notwendigen Bibliotheken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "149a539f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer\n",
    "import wandb\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from pathlib import Path\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe1cf68",
   "metadata": {},
   "source": [
    "3. Token And Positional Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ed6d9690",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TokenAndPositionalEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, max_len):\n",
    "        super().__init__()\n",
    "        # ID der Token in einen Vektorraum\n",
    "        self.token_embed = nn.Embedding(vocab_size, d_model)\n",
    "        # Positionale Einbettungen (lernen relative Positionen \"das ist learnable\")\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, 512, d_model)) # batch 1 , bis zu 512 token und vektor größe\n",
    "    def forward(self, x):\n",
    "        seq_len = x.size(1) # 1 ist Anzahl der Token\"Sequenzlänge\" (0 ist batch)\n",
    "        token_emb = self.token_embed(x)\n",
    "        pos_emb = self.pos_embedding[:, :seq_len, :]\n",
    "        return token_emb + pos_emb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d7e418",
   "metadata": {},
   "source": [
    "4. Masked Multi-Head Self-Attention\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a8c7f6f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedSelfAttention(nn.Module): # Modell nur auf Wörter schaut, die es schon kennen darf(besucht)\n",
    "    def __init__(self, d_model, n_heads):\n",
    "        super().__init__()\n",
    "        # Ensure the model dimension is divisible by the number of heads\n",
    "        assert d_model % n_heads == 0\n",
    "        self.head_dim = d_model // n_heads  # Dimension per attention head\n",
    "        self.n_heads = n_heads  # Number of attention heads\n",
    "\n",
    "        # Linear layers for query, key, and value transformations\n",
    "        self.query = nn.Linear(d_model, d_model)\n",
    "        self.key = nn.Linear(d_model, d_model)\n",
    "        self.value = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        # Linear layer for the output transformation\n",
    "        self.out = nn.Linear(d_model, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed2db8f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
