{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c027491",
   "metadata": {},
   "source": [
    "# GenerativeAI \"Sprachmodell\" Projekt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb4417d7",
   "metadata": {},
   "source": [
    "## Verbing mit wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e5700284",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33madel-haj-jumah\u001b[0m (\u001b[33madel-haj-jumah-hochschule-hannover\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe187fce",
   "metadata": {},
   "source": [
    "1. Importieren die notwendigen Bibliotheken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "149a539f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from pathlib import Path\n",
    "import random\n",
    "from datasets import load_dataset\n",
    "import torch.optim as optim\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe1cf68",
   "metadata": {},
   "source": [
    "2. Token And Positional Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ed6d9690",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TokenAndPositionalEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, max_len):\n",
    "        super().__init__()\n",
    "        # ID der Token in einen Vektorraum\n",
    "        self.token_embed = nn.Embedding(vocab_size, d_model)\n",
    "        # Positionale Einbettungen (lernen relative Positionen \"das ist learnable\")\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, 512, d_model)) # batch 1 , bis zu 512 token und vektor größe\n",
    "    def forward(self, x):\n",
    "        seq_len = x.size(1) # 1 ist Anzahl der Token\"Sequenzlänge\" (0 ist batch)\n",
    "        token_emb = self.token_embed(x)\n",
    "        pos_emb = self.pos_embedding[:, :seq_len, :]\n",
    "        return token_emb + pos_emb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d7e418",
   "metadata": {},
   "source": [
    "3. Masked Multi-Head Self-Attention\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ed2db8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MaskedSelfAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Masked (Causal) Self-Attention Layer\n",
    "    → Modell kann nur auf vergangene Tokens schauen\n",
    "    → Verwendet PyTorch nn.MultiheadAttention\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, n_heads):\n",
    "        super().__init__()\n",
    "        self.attn = nn.MultiheadAttention(\n",
    "            embed_dim=d_model, \n",
    "            num_heads=n_heads, \n",
    "            batch_first=True  # wichtig! Damit x.shape = (B, T, C) funktioniert\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        T = x.size(1)  # Sequence Length\n",
    "\n",
    "        # Causal Mask (obere Dreiecksmatrix)\n",
    "        mask = torch.triu(torch.ones(T, T), diagonal=1).bool().to(x.device)\n",
    "\n",
    "        # MultiheadAttention erwartet: (query, key, value, attn_mask)\n",
    "        out, _ = self.attn(x, x, x, attn_mask=mask)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7abb1c6",
   "metadata": {},
   "source": [
    "4. Add & Norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9cf72fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddNorm(nn.Module):\n",
    "    def __init__(self, d_model):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x, sublayer_output):\n",
    "        return self.norm(x + sublayer_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a2036d",
   "metadata": {},
   "source": [
    "5. Feedforward Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ebdfb3b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_ff, d_model)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db87d56",
   "metadata": {},
   "source": [
    "6. Alles zusammenfügen: Transformator-Decoderblock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "39bb6180",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, d_ff):\n",
    "        super().__init__()\n",
    "        self.attn = MaskedSelfAttention(d_model, n_heads)  # PyTorch MultiheadAttention inside\n",
    "        self.addnorm1 = AddNorm(d_model)\n",
    "        self.ff = FeedForward(d_model, d_ff)\n",
    "        self.addnorm2 = AddNorm(d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.addnorm1(x, self.attn(x))  # Attention + Residual + LayerNorm\n",
    "        x = self.addnorm2(x, self.ff(x))    # FeedForward + Residual + LayerNorm\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b91e5a",
   "metadata": {},
   "source": [
    "7. Zusammenbau des NanoTransformers (nur Decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "977967f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class NanoTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, n_heads, d_ff, max_len, num_layers):\n",
    "        super().__init__()\n",
    "        self.embed = TokenAndPositionalEmbedding(vocab_size, d_model, max_len)\n",
    "        self.blocks = nn.ModuleList([\n",
    "            TransformerBlock(d_model, n_heads, d_ff) for _ in range(num_layers)\n",
    "        ])\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        self.output_proj = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embed(x)\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        x = self.norm(x)\n",
    "        logits = self.output_proj(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e17f607c",
   "metadata": {},
   "source": [
    "8. Trainieren des Modells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4704706d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def train(model, dataloader, vocab_size, device, epochs=10, lr=1e-4):\n",
    "    # Wandb initialisieren, nur einmal zu Beginn des Trainings\n",
    "    wandb.init(project=\"nano-transformer\", config={\n",
    "        \"epochs\": epochs,\n",
    "        \"lr\": lr,\n",
    "        \"batch_size\": dataloader.batch_size,\n",
    "        \"seq_len\": dataloader.dataset.seq_len,\n",
    "        \"vocab_size\": vocab_size\n",
    "    })\n",
    "    \n",
    "    # Modell auf das richtige Gerät (GPU oder CPU) verschieben\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Optimizer und Loss-Funktion\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    model.train()  # Modell in Trainingsmodus versetzen\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for batch in tqdm(dataloader, desc=\"Training Progress\"):\n",
    "            # Eingabe- und Ziel-Tensoren\n",
    "            inputs = batch[:, :-1].to(device)\n",
    "            targets = batch[:, 1:].to(device)\n",
    "\n",
    "            # Vorwärtsdurchlauf\n",
    "            logits = model(inputs)\n",
    "            logits = logits.view(-1, vocab_size)  # Umformen für CrossEntropyLoss\n",
    "            targets = targets.view(-1)  # Umformen für CrossEntropyLoss\n",
    "\n",
    "            # Verlustberechnung\n",
    "            loss = loss_fn(logits, targets)\n",
    "\n",
    "            # Backpropagation und Optimierung\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Gesamten Verlust summieren\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        # Durchschnittlichen Verlust für die Epoche berechnen\n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        print(f\"Epoch {epoch+1}: Loss = {avg_loss:.4f}\")\n",
    "\n",
    "        # Verlust an Wandb senden\n",
    "        wandb.log({\"epoch\": epoch+1, \"loss\": avg_loss})\n",
    "    \n",
    "    # Modell speichern nach Training (optional)\n",
    "    torch.save(model.state_dict(), \"nano_transformer_trained.pth\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4700336",
   "metadata": {},
   "source": [
    "9. Dataset Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4293020e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (301966 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataLoader bereit!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "\n",
    "# 1. Tokenizer laden\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# 2. Tiny Shakespeare Dataset laden\n",
    "dataset = load_dataset(\"tiny_shakespeare\", split=\"train\", trust_remote_code=True)\n",
    "text = dataset['text'][0]\n",
    "\n",
    "# 3. Text in Token IDs umwandeln\n",
    "tokens = tokenizer.encode(text, truncation=False)\n",
    "tokens = torch.tensor(tokens)\n",
    "\n",
    "# 4. Dataset Klasse\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, tokens, seq_len):\n",
    "        self.tokens = tokens\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tokens) - self.seq_len\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.tokens[idx:idx+self.seq_len+1]\n",
    "\n",
    "# 5. DataLoader bauen\n",
    "seq_len = 64\n",
    "batch_size = 32\n",
    "dataset = TextDataset(tokens, seq_len)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "print(\"DataLoader bereit!\")\n",
    "\n",
    "# 6. Den Modell (Mini NanoTransformer)\n",
    "vocab_size = tokenizer.vocab_size\n",
    "d_model = 32\n",
    "n_heads = 1\n",
    "d_ff = 256\n",
    "max_len = 64\n",
    "num_layers = 1\n",
    "\n",
    "model = NanoTransformer(vocab_size, d_model, n_heads, d_ff, max_len, num_layers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e654952a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>loss</td><td>█▅▄▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>56</td></tr><tr><td>loss</td><td>2.64159</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">chocolate-sun-2</strong> at: <a href='https://wandb.ai/adel-haj-jumah-hochschule-hannover/nano-transformer/runs/3we05big' target=\"_blank\">https://wandb.ai/adel-haj-jumah-hochschule-hannover/nano-transformer/runs/3we05big</a><br> View project at: <a href='https://wandb.ai/adel-haj-jumah-hochschule-hannover/nano-transformer' target=\"_blank\">https://wandb.ai/adel-haj-jumah-hochschule-hannover/nano-transformer</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250423_214658-3we05big\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\Adel\\Desktop\\GenerativeAI-Project\\GenerativeAI-Project\\wandb\\run-20250423_233902-isrzoe9g</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/adel-haj-jumah-hochschule-hannover/nano-transformer/runs/isrzoe9g' target=\"_blank\">happy-terrain-3</a></strong> to <a href='https://wandb.ai/adel-haj-jumah-hochschule-hannover/nano-transformer' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/adel-haj-jumah-hochschule-hannover/nano-transformer' target=\"_blank\">https://wandb.ai/adel-haj-jumah-hochschule-hannover/nano-transformer</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/adel-haj-jumah-hochschule-hannover/nano-transformer/runs/isrzoe9g' target=\"_blank\">https://wandb.ai/adel-haj-jumah-hochschule-hannover/nano-transformer/runs/isrzoe9g</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  10%|█         | 989/9435 [00:12<01:42, 82.05it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m device = torch.device(\u001b[33m\"\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch.cuda.is_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvocab_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m3e-4\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 44\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(model, dataloader, vocab_size, device, epochs, lr)\u001b[39m\n\u001b[32m     41\u001b[39m     optimizer.step()\n\u001b[32m     43\u001b[39m     \u001b[38;5;66;03m# Gesamten Verlust summieren\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m44\u001b[39m     total_loss += \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     46\u001b[39m \u001b[38;5;66;03m# Durchschnittlichen Verlust für die Epoche berechnen\u001b[39;00m\n\u001b[32m     47\u001b[39m avg_loss = total_loss / \u001b[38;5;28mlen\u001b[39m(dataloader)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "train(model, dataloader, vocab_size, device, epochs=10, lr=3e-4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dd26ac85",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (301966 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataLoader bereit!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "\n",
    "# 1. Tokenizer laden\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# 2. Tiny Shakespeare Dataset laden\n",
    "dataset = load_dataset(\"tiny_shakespeare\", split=\"train\", trust_remote_code=True)\n",
    "text = dataset['text'][0]\n",
    "\n",
    "# 3. Text in Token IDs umwandeln\n",
    "tokens = tokenizer.encode(text, truncation=False)\n",
    "tokens = torch.tensor(tokens)\n",
    "\n",
    "# 4. Dataset Klasse\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, tokens, seq_len):\n",
    "        self.tokens = tokens\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tokens) - self.seq_len\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.tokens[idx:idx+self.seq_len+1]\n",
    "\n",
    "# 5. DataLoader bauen\n",
    "seq_len = 64\n",
    "batch_size = 32\n",
    "dataset = TextDataset(tokens, seq_len)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "print(\"DataLoader bereit!\")\n",
    "\n",
    "\n",
    "d_model = 128\n",
    "n_heads = 4\n",
    "d_ff = 512\n",
    "num_layers = 2\n",
    "seq_len = 128 # 128 Token pro Sequenz\n",
    "max_len = 128 # 128 Token pro Sequenz\n",
    "\n",
    "\n",
    "model = NanoTransformer(vocab_size, d_model, n_heads, d_ff, max_len, num_layers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "194df713",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">happy-terrain-3</strong> at: <a href='https://wandb.ai/adel-haj-jumah-hochschule-hannover/nano-transformer/runs/isrzoe9g' target=\"_blank\">https://wandb.ai/adel-haj-jumah-hochschule-hannover/nano-transformer/runs/isrzoe9g</a><br> View project at: <a href='https://wandb.ai/adel-haj-jumah-hochschule-hannover/nano-transformer' target=\"_blank\">https://wandb.ai/adel-haj-jumah-hochschule-hannover/nano-transformer</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250423_233902-isrzoe9g\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\Adel\\Desktop\\GenerativeAI-Project\\GenerativeAI-Project\\wandb\\run-20250423_234209-ukz60m8f</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/adel-haj-jumah-hochschule-hannover/nano-transformer/runs/ukz60m8f' target=\"_blank\">logical-bee-4</a></strong> to <a href='https://wandb.ai/adel-haj-jumah-hochschule-hannover/nano-transformer' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/adel-haj-jumah-hochschule-hannover/nano-transformer' target=\"_blank\">https://wandb.ai/adel-haj-jumah-hochschule-hannover/nano-transformer</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/adel-haj-jumah-hochschule-hannover/nano-transformer/runs/ukz60m8f' target=\"_blank\">https://wandb.ai/adel-haj-jumah-hochschule-hannover/nano-transformer/runs/ukz60m8f</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  81%|████████  | 7654/9435 [02:10<00:30, 58.77it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m device = torch.device(\u001b[33m\"\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch.cuda.is_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvocab_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m3e-4\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 40\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(model, dataloader, vocab_size, device, epochs, lr)\u001b[39m\n\u001b[32m     38\u001b[39m \u001b[38;5;66;03m# Backpropagation und Optimierung\u001b[39;00m\n\u001b[32m     39\u001b[39m optimizer.zero_grad()\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     41\u001b[39m optimizer.step()\n\u001b[32m     43\u001b[39m \u001b[38;5;66;03m# Gesamten Verlust summieren\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Adel\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\_tensor.py:626\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    616\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    617\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    618\u001b[39m         Tensor.backward,\n\u001b[32m    619\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    624\u001b[39m         inputs=inputs,\n\u001b[32m    625\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m626\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    627\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    628\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Adel\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    342\u001b[39m     retain_graph = create_graph\n\u001b[32m    344\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    345\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    346\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m347\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    348\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    349\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    350\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    351\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    352\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    353\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Adel\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\autograd\\graph.py:823\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    821\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    822\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m823\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    824\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    825\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    826\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    827\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "train(model, dataloader, vocab_size, device, epochs=10, lr=3e-4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e4c2289",
   "metadata": {},
   "source": [
    "## MaskedSelfAttention, AddNorm und FeedForward ersetzen durch nn.TransformerDecoderLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e0ad38c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (301966 > 1024). Running this sequence through the model will result in indexing errors\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataLoader bereit!\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">firm-yogurt-7</strong> at: <a href='https://wandb.ai/adel-haj-jumah-hochschule-hannover/nano-transformer/runs/xasgv55f' target=\"_blank\">https://wandb.ai/adel-haj-jumah-hochschule-hannover/nano-transformer/runs/xasgv55f</a><br> View project at: <a href='https://wandb.ai/adel-haj-jumah-hochschule-hannover/nano-transformer' target=\"_blank\">https://wandb.ai/adel-haj-jumah-hochschule-hannover/nano-transformer</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250424_170616-xasgv55f\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\Adel\\Desktop\\GenerativeAI-Project\\GenerativeAI-Project\\wandb\\run-20250424_170714-7k65u329</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/adel-haj-jumah-hochschule-hannover/nano-transformer/runs/7k65u329' target=\"_blank\">devout-silence-8</a></strong> to <a href='https://wandb.ai/adel-haj-jumah-hochschule-hannover/nano-transformer' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/adel-haj-jumah-hochschule-hannover/nano-transformer' target=\"_blank\">https://wandb.ai/adel-haj-jumah-hochschule-hannover/nano-transformer</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/adel-haj-jumah-hochschule-hannover/nano-transformer/runs/7k65u329' target=\"_blank\">https://wandb.ai/adel-haj-jumah-hochschule-hannover/nano-transformer/runs/7k65u329</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 9435/9435 [01:58<00:00, 79.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Training Loss = 5.0648\n",
      "Epoch 1: Evaluation Loss = 4.2630\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 9435/9435 [02:05<00:00, 75.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Training Loss = 4.0063\n",
      "Epoch 2: Evaluation Loss = 3.8241\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 9435/9435 [02:08<00:00, 73.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Training Loss = 3.7242\n",
      "Epoch 3: Evaluation Loss = 3.6314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 9435/9435 [02:08<00:00, 73.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Training Loss = 3.5718\n",
      "Epoch 4: Evaluation Loss = 3.5107\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 9435/9435 [02:09<00:00, 73.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Training Loss = 3.4676\n",
      "Epoch 5: Evaluation Loss = 3.4190\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 9435/9435 [02:03<00:00, 76.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: Training Loss = 3.3874\n",
      "Epoch 6: Evaluation Loss = 3.3489\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 9435/9435 [01:58<00:00, 79.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: Training Loss = 3.3221\n",
      "Epoch 7: Evaluation Loss = 3.2895\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|██████████| 9435/9435 [01:55<00:00, 81.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: Training Loss = 3.2673\n",
      "Epoch 8: Evaluation Loss = 3.2444\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 9435/9435 [01:53<00:00, 83.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: Training Loss = 3.2204\n",
      "Epoch 9: Evaluation Loss = 3.1943\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|██████████| 9435/9435 [01:54<00:00, 82.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: Training Loss = 3.1796\n",
      "Epoch 10: Evaluation Loss = 3.1547\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11: 100%|██████████| 9435/9435 [01:54<00:00, 82.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11: Training Loss = 3.1437\n",
      "Epoch 11: Evaluation Loss = 3.1232\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12: 100%|██████████| 9435/9435 [02:00<00:00, 78.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12: Training Loss = 3.1115\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 148\u001b[39m\n\u001b[32m    146\u001b[39m \u001b[38;5;66;03m# Training starten\u001b[39;00m\n\u001b[32m    147\u001b[39m device = torch.device(\u001b[33m\"\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch.cuda.is_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m148\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvocab_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m3e-4\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 92\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(model, train_dataloader, eval_dataloader, vocab_size, device, epochs, lr)\u001b[39m\n\u001b[32m     89\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: Training Loss = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mavg_train_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     91\u001b[39m \u001b[38;5;66;03m# Evaluierung nach jedem Trainingsepochendurchgang\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m92\u001b[39m avg_eval_loss = \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvocab_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     93\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: Evaluation Loss = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mavg_eval_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     95\u001b[39m \u001b[38;5;66;03m# Logs an WandB senden\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 42\u001b[39m, in \u001b[36mevaluate\u001b[39m\u001b[34m(model, dataloader, vocab_size, device)\u001b[39m\n\u001b[32m     39\u001b[39m inputs = batch[:, :-\u001b[32m1\u001b[39m].to(device)\n\u001b[32m     40\u001b[39m targets = batch[:, \u001b[32m1\u001b[39m:].to(device)\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m logits = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     43\u001b[39m logits = logits.view(-\u001b[32m1\u001b[39m, vocab_size)\n\u001b[32m     44\u001b[39m targets = targets.view(-\u001b[32m1\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Adel\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Adel\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 21\u001b[39m, in \u001b[36mNanoTransformer.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     18\u001b[39m T = x.size(\u001b[32m1\u001b[39m)\n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# Causal Mask erstellen\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m causal_mask = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtriu\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mones\u001b[49m\u001b[43m(\u001b[49m\u001b[43mT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mT\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdiagonal\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m.bool().to(x.device)\n\u001b[32m     23\u001b[39m \u001b[38;5;66;03m# Dummy Memory\u001b[39;00m\n\u001b[32m     24\u001b[39m memory = torch.zeros(x.size(\u001b[32m0\u001b[39m), \u001b[32m1\u001b[39m, x.size(\u001b[32m2\u001b[39m)).to(x.device)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "#  NanoTransformer mit PyTorch Decoder\n",
    "from torch.nn import TransformerDecoder, TransformerDecoderLayer\n",
    "\n",
    "class NanoTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, n_heads, d_ff, max_len, num_layers):\n",
    "        super().__init__()\n",
    "        self.embed = TokenAndPositionalEmbedding(vocab_size, d_model, max_len)\n",
    "\n",
    "        decoder_layer = TransformerDecoderLayer(d_model, n_heads, d_ff, batch_first=True)\n",
    "        self.decoder = TransformerDecoder(decoder_layer, num_layers)\n",
    "\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        self.output_proj = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embed(x)\n",
    "        T = x.size(1)\n",
    "\n",
    "        # Causal Mask erstellen\n",
    "        causal_mask = torch.triu(torch.ones(T, T), diagonal=1).bool().to(x.device)\n",
    "\n",
    "        # Dummy Memory\n",
    "        memory = torch.zeros(x.size(0), 1, x.size(2)).to(x.device)\n",
    "\n",
    "        x = self.decoder(tgt=x, memory=memory, tgt_mask=causal_mask)\n",
    "        x = self.norm(x)\n",
    "        logits = self.output_proj(x)\n",
    "        return logits\n",
    "\n",
    "# Training und Evaluierung\n",
    "def evaluate(model, dataloader, vocab_size, device):\n",
    "    model.eval()  # Wechsel in den Evaluationsmodus\n",
    "    total_loss = 0\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    with torch.no_grad():  # Kein Gradientenberechnung\n",
    "        for batch in dataloader:\n",
    "            inputs = batch[:, :-1].to(device)\n",
    "            targets = batch[:, 1:].to(device)\n",
    "\n",
    "            logits = model(inputs)\n",
    "            logits = logits.view(-1, vocab_size)\n",
    "            targets = targets.view(-1)\n",
    "\n",
    "            loss = loss_fn(logits, targets)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    return avg_loss\n",
    "\n",
    "def train(model, train_dataloader, eval_dataloader, vocab_size, device, epochs=10, lr=1e-4):\n",
    "    wandb.init(project=\"nano-transformer\", config={\n",
    "        \"epochs\": epochs,\n",
    "        \"lr\": lr,\n",
    "        \"batch_size\": train_dataloader.batch_size,\n",
    "        \"seq_len\": train_dataloader.dataset.seq_len,\n",
    "        \"vocab_size\": vocab_size,\n",
    "        \"dataset\": \"tiny_shakespeare\",\n",
    "        \"d_ff\": d_ff,\n",
    "        \"max_len\": max_len,\n",
    "        \"num_layers\": num_layers\n",
    "    })\n",
    "\n",
    "    model = model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for batch in tqdm(train_dataloader, desc=f\"Epoch {epoch+1}\"):\n",
    "            inputs = batch[:, :-1].to(device)\n",
    "            targets = batch[:, 1:].to(device)\n",
    "\n",
    "            logits = model(inputs)\n",
    "            logits = logits.view(-1, vocab_size)\n",
    "            targets = targets.view(-1)\n",
    "\n",
    "            loss = loss_fn(logits, targets)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = total_loss / len(train_dataloader)\n",
    "        print(f\"Epoch {epoch+1}: Training Loss = {avg_train_loss:.4f}\")\n",
    "\n",
    "        # Evaluierung nach jedem Trainingsepochendurchgang\n",
    "        avg_eval_loss = evaluate(model, eval_dataloader, vocab_size, device)\n",
    "        print(f\"Epoch {epoch+1}: Evaluation Loss = {avg_eval_loss:.4f}\")\n",
    "\n",
    "        # Logs an WandB senden\n",
    "        wandb.log({\n",
    "            \"epoch\": epoch+1,\n",
    "            \"train_loss\": avg_train_loss,\n",
    "            \"eval_loss\": avg_eval_loss\n",
    "        })\n",
    "\n",
    "    # Modell speichern nach Training\n",
    "    torch.save(model.state_dict(), \"nano_transformer_trained.pth\")\n",
    "\n",
    "# Dataset vorbereiten\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "dataset = load_dataset(\"tiny_shakespeare\", split=\"train\", trust_remote_code=True)\n",
    "text = dataset['text'][0]\n",
    "tokens = tokenizer.encode(text, truncation=False)\n",
    "tokens = torch.tensor(tokens)\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, tokens, seq_len):\n",
    "        self.tokens = tokens\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tokens) - self.seq_len\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.tokens[idx:idx+self.seq_len+1]\n",
    "\n",
    "seq_len = 64\n",
    "batch_size = 32\n",
    "dataset = TextDataset(tokens, seq_len)\n",
    "train_dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Erstellen eines Evaluationsdatensatzes (z.B. indem wir die ersten 20% des Datensatzes verwenden)\n",
    "eval_dataset = TextDataset(tokens[dataset.seq_len:], seq_len)  # Hier könnten auch speziellere Evaluierungsdaten genutzt werden\n",
    "eval_dataloader = DataLoader(eval_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(\"DataLoader bereit!\")\n",
    "\n",
    "# Modell konfigurieren\n",
    "vocab_size = tokenizer.vocab_size\n",
    "d_model = 32\n",
    "n_heads = 1\n",
    "d_ff = 256\n",
    "max_len = 64\n",
    "num_layers = 1\n",
    "\n",
    "model = NanoTransformer(vocab_size, d_model, n_heads, d_ff, max_len, num_layers)\n",
    "\n",
    "# Training starten\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "train(model, train_dataloader, eval_dataloader, vocab_size, device, epochs=100, lr=3e-4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "368eb653",
   "metadata": {},
   "source": [
    "### lr statt 0.0003 zu 0.003"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4b46d989",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataLoader bereit!\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▂▃▄▅▅▆▇▇█</td></tr><tr><td>eval_loss</td><td>█▅▄▃▃▂▂▂▁▁▁</td></tr><tr><td>train_loss</td><td>█▄▃▃▂▂▂▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>11</td></tr><tr><td>eval_loss</td><td>3.12321</td></tr><tr><td>train_loss</td><td>3.14373</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">devout-silence-8</strong> at: <a href='https://wandb.ai/adel-haj-jumah-hochschule-hannover/nano-transformer/runs/7k65u329' target=\"_blank\">https://wandb.ai/adel-haj-jumah-hochschule-hannover/nano-transformer/runs/7k65u329</a><br> View project at: <a href='https://wandb.ai/adel-haj-jumah-hochschule-hannover/nano-transformer' target=\"_blank\">https://wandb.ai/adel-haj-jumah-hochschule-hannover/nano-transformer</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250424_170714-7k65u329\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\Adel\\Desktop\\GenerativeAI-Project\\GenerativeAI-Project\\wandb\\run-20250424_174145-fq7ea85h</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/adel-haj-jumah-hochschule-hannover/nano-transformer/runs/fq7ea85h' target=\"_blank\">winter-brook-9</a></strong> to <a href='https://wandb.ai/adel-haj-jumah-hochschule-hannover/nano-transformer' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/adel-haj-jumah-hochschule-hannover/nano-transformer' target=\"_blank\">https://wandb.ai/adel-haj-jumah-hochschule-hannover/nano-transformer</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/adel-haj-jumah-hochschule-hannover/nano-transformer/runs/fq7ea85h' target=\"_blank\">https://wandb.ai/adel-haj-jumah-hochschule-hannover/nano-transformer/runs/fq7ea85h</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 9435/9435 [02:01<00:00, 77.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Training Loss = 3.8809\n",
      "Epoch 1: Evaluation Loss = 3.4163\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 9435/9435 [01:57<00:00, 80.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Training Loss = 3.2139\n",
      "Epoch 2: Evaluation Loss = 3.0782\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 9435/9435 [01:54<00:00, 82.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Training Loss = 3.0033\n",
      "Epoch 3: Evaluation Loss = 2.9272\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 9435/9435 [01:54<00:00, 82.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Training Loss = 2.8861\n",
      "Epoch 4: Evaluation Loss = 2.8302\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 9435/9435 [01:56<00:00, 80.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Training Loss = 2.8056\n",
      "Epoch 5: Evaluation Loss = 2.7626\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 9435/9435 [01:50<00:00, 85.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: Training Loss = 2.7456\n",
      "Epoch 6: Evaluation Loss = 2.7118\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 9435/9435 [01:50<00:00, 85.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: Training Loss = 2.6989\n",
      "Epoch 7: Evaluation Loss = 2.6711\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|██████████| 9435/9435 [01:52<00:00, 84.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: Training Loss = 2.6617\n",
      "Epoch 8: Evaluation Loss = 2.6357\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 9435/9435 [01:49<00:00, 85.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: Training Loss = 2.6311\n",
      "Epoch 9: Evaluation Loss = 2.6102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|██████████| 9435/9435 [01:53<00:00, 82.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: Training Loss = 2.6050\n",
      "Epoch 10: Evaluation Loss = 2.5891\n"
     ]
    }
   ],
   "source": [
    "# Erstellen eines Evaluationsdatensatzes (z.B. indem wir die ersten 20% des Datensatzes verwenden)\n",
    "eval_dataset = TextDataset(tokens[dataset.seq_len:], seq_len)  # Hier könnten auch speziellere Evaluierungsdaten genutzt werden\n",
    "eval_dataloader = DataLoader(eval_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(\"DataLoader bereit!\")\n",
    "\n",
    "# Modell konfigurieren\n",
    "vocab_size = tokenizer.vocab_size\n",
    "d_model = 32\n",
    "n_heads = 1\n",
    "d_ff = 256\n",
    "max_len = 64\n",
    "num_layers = 1\n",
    "\n",
    "model = NanoTransformer(vocab_size, d_model, n_heads, d_ff, max_len, num_layers)\n",
    "\n",
    "# Training starten\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "train(model, train_dataloader, eval_dataloader, vocab_size, device, epochs=10, lr=3e-3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33785a79",
   "metadata": {},
   "source": [
    "### num_layers =von 1 zu  2 / n_heads = von 1 zu 4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9803a3cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataLoader bereit!\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>eval_loss</td><td>█▅▄▃▂▂▂▁▁▁</td></tr><tr><td>train_loss</td><td>█▄▃▃▂▂▂▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>10</td></tr><tr><td>eval_loss</td><td>2.58912</td></tr><tr><td>train_loss</td><td>2.60498</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">winter-brook-9</strong> at: <a href='https://wandb.ai/adel-haj-jumah-hochschule-hannover/nano-transformer/runs/fq7ea85h' target=\"_blank\">https://wandb.ai/adel-haj-jumah-hochschule-hannover/nano-transformer/runs/fq7ea85h</a><br> View project at: <a href='https://wandb.ai/adel-haj-jumah-hochschule-hannover/nano-transformer' target=\"_blank\">https://wandb.ai/adel-haj-jumah-hochschule-hannover/nano-transformer</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250424_174145-fq7ea85h\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\Adel\\Desktop\\GenerativeAI-Project\\GenerativeAI-Project\\wandb\\run-20250424_181244-69egtk7f</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/adel-haj-jumah-hochschule-hannover/nano-transformer/runs/69egtk7f' target=\"_blank\">radiant-bird-10</a></strong> to <a href='https://wandb.ai/adel-haj-jumah-hochschule-hannover/nano-transformer' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/adel-haj-jumah-hochschule-hannover/nano-transformer' target=\"_blank\">https://wandb.ai/adel-haj-jumah-hochschule-hannover/nano-transformer</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/adel-haj-jumah-hochschule-hannover/nano-transformer/runs/69egtk7f' target=\"_blank\">https://wandb.ai/adel-haj-jumah-hochschule-hannover/nano-transformer/runs/69egtk7f</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 9435/9435 [02:18<00:00, 68.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Training Loss = 3.8056\n",
      "Epoch 1: Evaluation Loss = 3.2602\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 9435/9435 [02:16<00:00, 69.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Training Loss = 2.9659\n",
      "Epoch 2: Evaluation Loss = 2.7749\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 9435/9435 [02:20<00:00, 67.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Training Loss = 2.6883\n",
      "Epoch 3: Evaluation Loss = 2.6001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 9435/9435 [02:17<00:00, 68.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Training Loss = 2.5610\n",
      "Epoch 4: Evaluation Loss = 2.5110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 9435/9435 [02:14<00:00, 70.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Training Loss = 2.4837\n",
      "Epoch 5: Evaluation Loss = 2.4507\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 9435/9435 [02:14<00:00, 69.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: Training Loss = 2.4301\n",
      "Epoch 6: Evaluation Loss = 2.3999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 9435/9435 [02:11<00:00, 71.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: Training Loss = 2.3896\n",
      "Epoch 7: Evaluation Loss = 2.3705\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|██████████| 9435/9435 [02:11<00:00, 71.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: Training Loss = 2.3576\n",
      "Epoch 8: Evaluation Loss = 2.3354\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 9435/9435 [02:12<00:00, 71.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: Training Loss = 2.3321\n",
      "Epoch 9: Evaluation Loss = 2.3148\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|██████████| 9435/9435 [02:21<00:00, 66.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: Training Loss = 2.3096\n",
      "Epoch 10: Evaluation Loss = 2.2918\n"
     ]
    }
   ],
   "source": [
    "# Erstellen eines Evaluationsdatensatzes (z.B. indem wir die ersten 20% des Datensatzes verwenden)\n",
    "eval_dataset = TextDataset(tokens[dataset.seq_len:], seq_len)  # Hier könnten auch speziellere Evaluierungsdaten genutzt werden\n",
    "eval_dataloader = DataLoader(eval_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(\"DataLoader bereit!\")\n",
    "\n",
    "# Modell konfigurieren\n",
    "vocab_size = tokenizer.vocab_size\n",
    "d_model = 32\n",
    "n_heads = 4\n",
    "d_ff = 256\n",
    "max_len = 64\n",
    "num_layers = 2\n",
    "\n",
    "model = NanoTransformer(vocab_size, d_model, n_heads, d_ff, max_len, num_layers)\n",
    "\n",
    "# Training starten\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "train(model, train_dataloader, eval_dataloader, vocab_size, device, epochs=10, lr=3e-3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da90c58",
   "metadata": {},
   "source": [
    "### max_len = von 64 zu 128 / d_ff = von 256 zu 512  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5bcdb2c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataLoader bereit!\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>eval_loss</td><td>█▄▃▃▂▂▂▁▁▁</td></tr><tr><td>train_loss</td><td>█▄▃▂▂▂▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>10</td></tr><tr><td>eval_loss</td><td>2.2918</td></tr><tr><td>train_loss</td><td>2.30964</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">radiant-bird-10</strong> at: <a href='https://wandb.ai/adel-haj-jumah-hochschule-hannover/nano-transformer/runs/69egtk7f' target=\"_blank\">https://wandb.ai/adel-haj-jumah-hochschule-hannover/nano-transformer/runs/69egtk7f</a><br> View project at: <a href='https://wandb.ai/adel-haj-jumah-hochschule-hannover/nano-transformer' target=\"_blank\">https://wandb.ai/adel-haj-jumah-hochschule-hannover/nano-transformer</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250424_181244-69egtk7f\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\Adel\\Desktop\\GenerativeAI-Project\\GenerativeAI-Project\\wandb\\run-20250424_184854-mni0nra1</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/adel-haj-jumah-hochschule-hannover/nano-transformer/runs/mni0nra1' target=\"_blank\">blooming-frost-11</a></strong> to <a href='https://wandb.ai/adel-haj-jumah-hochschule-hannover/nano-transformer' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/adel-haj-jumah-hochschule-hannover/nano-transformer' target=\"_blank\">https://wandb.ai/adel-haj-jumah-hochschule-hannover/nano-transformer</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/adel-haj-jumah-hochschule-hannover/nano-transformer/runs/mni0nra1' target=\"_blank\">https://wandb.ai/adel-haj-jumah-hochschule-hannover/nano-transformer/runs/mni0nra1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 9435/9435 [02:24<00:00, 65.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Training Loss = 3.8457\n",
      "Epoch 1: Evaluation Loss = 3.2873\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 9435/9435 [02:20<00:00, 67.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Training Loss = 2.9488\n",
      "Epoch 2: Evaluation Loss = 2.7384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 9435/9435 [02:20<00:00, 67.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Training Loss = 2.6451\n",
      "Epoch 3: Evaluation Loss = 2.5582\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 9435/9435 [02:20<00:00, 67.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Training Loss = 2.5092\n",
      "Epoch 4: Evaluation Loss = 2.4452\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 9435/9435 [02:19<00:00, 67.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Training Loss = 2.4271\n",
      "Epoch 5: Evaluation Loss = 2.3794\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 9435/9435 [02:19<00:00, 67.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: Training Loss = 2.3698\n",
      "Epoch 6: Evaluation Loss = 2.3415\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 9435/9435 [02:19<00:00, 67.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: Training Loss = 2.3280\n",
      "Epoch 7: Evaluation Loss = 2.2969\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|██████████| 9435/9435 [02:19<00:00, 67.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: Training Loss = 2.2949\n",
      "Epoch 8: Evaluation Loss = 2.2728\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 9435/9435 [02:19<00:00, 67.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: Training Loss = 2.2676\n",
      "Epoch 9: Evaluation Loss = 2.2426\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|██████████| 9435/9435 [02:17<00:00, 68.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: Training Loss = 2.2442\n",
      "Epoch 10: Evaluation Loss = 2.2160\n"
     ]
    }
   ],
   "source": [
    "# Erstellen eines Evaluationsdatensatzes (z.B. indem wir die ersten 20% des Datensatzes verwenden)\n",
    "eval_dataset = TextDataset(tokens[dataset.seq_len:], seq_len)  # Hier könnten auch speziellere Evaluierungsdaten genutzt werden\n",
    "eval_dataloader = DataLoader(eval_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(\"DataLoader bereit!\")\n",
    "\n",
    "# Modell konfigurieren\n",
    "vocab_size = tokenizer.vocab_size\n",
    "d_model = 32\n",
    "n_heads = 4\n",
    "d_ff = 512  \n",
    "max_len = 128\n",
    "num_layers = 2\n",
    "\n",
    "model = NanoTransformer(vocab_size, d_model, n_heads, d_ff, max_len, num_layers)\n",
    "\n",
    "# Training starten\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "train(model, train_dataloader, eval_dataloader, vocab_size, device, epochs=10, lr=3e-3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "278d4a56",
   "metadata": {},
   "source": [
    "### d_model = von 32 zu 128 / num_layers = von 2 zu 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "74d4a55c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>eval_loss</td><td>█▄▃▂▂▂▂▁▁▁</td></tr><tr><td>train_loss</td><td>█▄▃▂▂▂▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>10</td></tr><tr><td>eval_loss</td><td>2.216</td></tr><tr><td>train_loss</td><td>2.24425</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">blooming-frost-11</strong> at: <a href='https://wandb.ai/adel-haj-jumah-hochschule-hannover/nano-transformer/runs/mni0nra1' target=\"_blank\">https://wandb.ai/adel-haj-jumah-hochschule-hannover/nano-transformer/runs/mni0nra1</a><br> View project at: <a href='https://wandb.ai/adel-haj-jumah-hochschule-hannover/nano-transformer' target=\"_blank\">https://wandb.ai/adel-haj-jumah-hochschule-hannover/nano-transformer</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250424_184854-mni0nra1\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\Adel\\Desktop\\GenerativeAI-Project\\GenerativeAI-Project\\wandb\\run-20250424_192755-20um92u3</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/adel-haj-jumah-hochschule-hannover/nano-transformer/runs/20um92u3' target=\"_blank\">apricot-surf-12</a></strong> to <a href='https://wandb.ai/adel-haj-jumah-hochschule-hannover/nano-transformer' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/adel-haj-jumah-hochschule-hannover/nano-transformer' target=\"_blank\">https://wandb.ai/adel-haj-jumah-hochschule-hannover/nano-transformer</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/adel-haj-jumah-hochschule-hannover/nano-transformer/runs/20um92u3' target=\"_blank\">https://wandb.ai/adel-haj-jumah-hochschule-hannover/nano-transformer/runs/20um92u3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 9435/9435 [03:48<00:00, 41.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Training Loss = 5.9286\n",
      "Epoch 1: Evaluation Loss = 5.8530\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 9435/9435 [03:46<00:00, 41.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Training Loss = 5.4539\n",
      "Epoch 2: Evaluation Loss = 5.0933\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 9435/9435 [03:45<00:00, 41.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Training Loss = 5.1421\n",
      "Epoch 3: Evaluation Loss = 5.0835\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 9435/9435 [03:45<00:00, 41.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Training Loss = 5.0913\n",
      "Epoch 4: Evaluation Loss = 5.0308\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 9435/9435 [03:54<00:00, 40.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Training Loss = 4.7355\n",
      "Epoch 5: Evaluation Loss = 4.5584\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 9435/9435 [03:36<00:00, 43.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: Training Loss = 4.4312\n",
      "Epoch 6: Evaluation Loss = 4.2819\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 9435/9435 [03:45<00:00, 41.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: Training Loss = 4.2090\n",
      "Epoch 7: Evaluation Loss = 4.1360\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|██████████| 9435/9435 [03:34<00:00, 44.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: Training Loss = 4.0530\n",
      "Epoch 8: Evaluation Loss = 3.9643\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 9435/9435 [03:32<00:00, 44.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: Training Loss = 3.9459\n",
      "Epoch 9: Evaluation Loss = 3.8622\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|██████████| 9435/9435 [03:32<00:00, 44.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: Training Loss = 3.8531\n",
      "Epoch 10: Evaluation Loss = 3.8033\n"
     ]
    }
   ],
   "source": [
    "# Modell konfigurieren\n",
    "vocab_size = tokenizer.vocab_size\n",
    "d_model = 128\n",
    "n_heads = 4\n",
    "d_ff = 512  \n",
    "max_len = 128\n",
    "num_layers = 4\n",
    "\n",
    "model = NanoTransformer(vocab_size, d_model, n_heads, d_ff, max_len, num_layers)\n",
    "\n",
    "# Training starten\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "train(model, train_dataloader, eval_dataloader, vocab_size, device, epochs=10, lr=3e-3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b2612cf",
   "metadata": {},
   "source": [
    "### Optimierungsalgorithmus von  Adam zu SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "31efa2da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (301966 > 1024). Running this sequence through the model will result in indexing errors\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataLoader bereit!\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>eval_loss</td><td>█▆▅▄▃▃▂▂▁▁</td></tr><tr><td>train_loss</td><td>█▅▄▃▃▂▂▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>10</td></tr><tr><td>eval_loss</td><td>7.46348</td></tr><tr><td>train_loss</td><td>7.50356</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">zesty-wind-13</strong> at: <a href='https://wandb.ai/adel-haj-jumah-hochschule-hannover/nano-transformer/runs/vraq85ye' target=\"_blank\">https://wandb.ai/adel-haj-jumah-hochschule-hannover/nano-transformer/runs/vraq85ye</a><br> View project at: <a href='https://wandb.ai/adel-haj-jumah-hochschule-hannover/nano-transformer' target=\"_blank\">https://wandb.ai/adel-haj-jumah-hochschule-hannover/nano-transformer</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250424_202300-vraq85ye\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\Adel\\Desktop\\GenerativeAI-Project\\GenerativeAI-Project\\wandb\\run-20250424_205659-9dap3nes</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/adel-haj-jumah-hochschule-hannover/nano-transformer/runs/9dap3nes' target=\"_blank\">cosmic-water-14</a></strong> to <a href='https://wandb.ai/adel-haj-jumah-hochschule-hannover/nano-transformer' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/adel-haj-jumah-hochschule-hannover/nano-transformer' target=\"_blank\">https://wandb.ai/adel-haj-jumah-hochschule-hannover/nano-transformer</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/adel-haj-jumah-hochschule-hannover/nano-transformer/runs/9dap3nes' target=\"_blank\">https://wandb.ai/adel-haj-jumah-hochschule-hannover/nano-transformer/runs/9dap3nes</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 9435/9435 [02:15<00:00, 69.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Training Loss = 8.3323\n",
      "Epoch 1: Evaluation Loss = 7.5116\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 9435/9435 [02:13<00:00, 70.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Training Loss = 7.2151\n",
      "Epoch 2: Evaluation Loss = 6.9832\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 9435/9435 [02:12<00:00, 71.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Training Loss = 6.8284\n",
      "Epoch 3: Evaluation Loss = 6.6963\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 9435/9435 [02:14<00:00, 70.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Training Loss = 6.5963\n",
      "Epoch 4: Evaluation Loss = 6.5087\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 9435/9435 [02:13<00:00, 70.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Training Loss = 6.4402\n",
      "Epoch 5: Evaluation Loss = 6.3793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 9435/9435 [02:08<00:00, 73.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: Training Loss = 6.3292\n",
      "Epoch 6: Evaluation Loss = 6.2830\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 9435/9435 [02:10<00:00, 72.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: Training Loss = 6.2421\n",
      "Epoch 7: Evaluation Loss = 6.2033\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|██████████| 9435/9435 [02:15<00:00, 69.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: Training Loss = 6.1660\n",
      "Epoch 8: Evaluation Loss = 6.1289\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 9435/9435 [02:14<00:00, 69.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: Training Loss = 6.0917\n",
      "Epoch 9: Evaluation Loss = 6.0546\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|██████████| 9435/9435 [02:14<00:00, 70.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: Training Loss = 6.0164\n",
      "Epoch 10: Evaluation Loss = 5.9775\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "#  NanoTransformer mit PyTorch Decoder\n",
    "from torch.nn import TransformerDecoder, TransformerDecoderLayer\n",
    "\n",
    "class NanoTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, n_heads, d_ff, max_len, num_layers):\n",
    "        super().__init__()\n",
    "        self.embed = TokenAndPositionalEmbedding(vocab_size, d_model, max_len)\n",
    "\n",
    "        decoder_layer = TransformerDecoderLayer(d_model, n_heads, d_ff, batch_first=True)\n",
    "        self.decoder = TransformerDecoder(decoder_layer, num_layers)\n",
    "\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        self.output_proj = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embed(x)\n",
    "        T = x.size(1)\n",
    "\n",
    "        # Causal Mask erstellen\n",
    "        causal_mask = torch.triu(torch.ones(T, T), diagonal=1).bool().to(x.device)\n",
    "\n",
    "        # Dummy Memory\n",
    "        memory = torch.zeros(x.size(0), 1, x.size(2)).to(x.device)\n",
    "\n",
    "        x = self.decoder(tgt=x, memory=memory, tgt_mask=causal_mask)\n",
    "        x = self.norm(x)\n",
    "        logits = self.output_proj(x)\n",
    "        return logits\n",
    "\n",
    "# Training und Evaluierung\n",
    "def evaluate(model, dataloader, vocab_size, device):\n",
    "    model.eval()  # Wechsel in den Evaluationsmodus\n",
    "    total_loss = 0\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    with torch.no_grad():  # Kein Gradientenberechnung\n",
    "        for batch in dataloader:\n",
    "            inputs = batch[:, :-1].to(device)\n",
    "            targets = batch[:, 1:].to(device)\n",
    "\n",
    "            logits = model(inputs)\n",
    "            logits = logits.view(-1, vocab_size)\n",
    "            targets = targets.view(-1)\n",
    "\n",
    "            loss = loss_fn(logits, targets)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    return avg_loss\n",
    "\n",
    "def train(model, train_dataloader, eval_dataloader, vocab_size, device, epochs=10, lr=1e-4):\n",
    "    wandb.init(project=\"nano-transformer\", config={\n",
    "        \"epochs\": epochs,\n",
    "        \"lr\": lr,\n",
    "        \"batch_size\": train_dataloader.batch_size,\n",
    "        \"seq_len\": train_dataloader.dataset.seq_len,\n",
    "        \"vocab_size\": vocab_size,\n",
    "        \"dataset\": \"tiny_shakespeare\",\n",
    "        \"d_ff\": d_ff,\n",
    "        \"max_len\": max_len,\n",
    "        \"num_layers\": num_layers\n",
    "    })\n",
    "\n",
    "    model = model.to(device)\n",
    "    optimizer =  optim.SGD(model.parameters(), lr=lr)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for batch in tqdm(train_dataloader, desc=f\"Epoch {epoch+1}\"):\n",
    "            inputs = batch[:, :-1].to(device)\n",
    "            targets = batch[:, 1:].to(device)\n",
    "\n",
    "            logits = model(inputs)\n",
    "            logits = logits.view(-1, vocab_size)\n",
    "            targets = targets.view(-1)\n",
    "\n",
    "            loss = loss_fn(logits, targets)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = total_loss / len(train_dataloader)\n",
    "        print(f\"Epoch {epoch+1}: Training Loss = {avg_train_loss:.4f}\")\n",
    "\n",
    "        # Evaluierung nach jedem Trainingsepochendurchgang\n",
    "        avg_eval_loss = evaluate(model, eval_dataloader, vocab_size, device)\n",
    "        print(f\"Epoch {epoch+1}: Evaluation Loss = {avg_eval_loss:.4f}\")\n",
    "\n",
    "        # Logs an WandB senden\n",
    "        wandb.log({\n",
    "            \"epoch\": epoch+1,\n",
    "            \"train_loss\": avg_train_loss,\n",
    "            \"eval_loss\": avg_eval_loss\n",
    "        })\n",
    "\n",
    "    # Modell speichern nach Training\n",
    "    torch.save(model.state_dict(), \"nano_transformer_trained.pth\")\n",
    "\n",
    "# Dataset vorbereiten\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "dataset = load_dataset(\"tiny_shakespeare\", split=\"train\", trust_remote_code=True)\n",
    "text = dataset['text'][0]\n",
    "tokens = tokenizer.encode(text, truncation=False)\n",
    "tokens = torch.tensor(tokens)\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, tokens, seq_len):\n",
    "        self.tokens = tokens\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tokens) - self.seq_len\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.tokens[idx:idx+self.seq_len+1]\n",
    "\n",
    "seq_len = 64\n",
    "batch_size = 32\n",
    "dataset = TextDataset(tokens, seq_len)\n",
    "train_dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Erstellen eines Evaluationsdatensatzes (z.B. indem wir die ersten 20% des Datensatzes verwenden)\n",
    "eval_dataset = TextDataset(tokens[dataset.seq_len:], seq_len)  # Hier könnten auch speziellere Evaluierungsdaten genutzt werden\n",
    "eval_dataloader = DataLoader(eval_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(\"DataLoader bereit!\")\n",
    "\n",
    "# Modell konfigurieren\n",
    "vocab_size = tokenizer.vocab_size\n",
    "d_model = 32\n",
    "n_heads = 4\n",
    "d_ff = 512  \n",
    "max_len = 128\n",
    "num_layers = 2\n",
    "\n",
    "model = NanoTransformer(vocab_size, d_model, n_heads, d_ff, max_len, num_layers)\n",
    "\n",
    "# Training starten\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "train(model, train_dataloader, eval_dataloader, vocab_size, device, epochs=10, lr=3e-3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "380d51bf",
   "metadata": {},
   "source": [
    "### batch_size = von 32 zu  64\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "515f9c2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (301966 > 1024). Running this sequence through the model will result in indexing errors\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataLoader bereit!\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">whole-morning-15</strong> at: <a href='https://wandb.ai/adel-haj-jumah-hochschule-hannover/nano-transformer/runs/u4m97u75' target=\"_blank\">https://wandb.ai/adel-haj-jumah-hochschule-hannover/nano-transformer/runs/u4m97u75</a><br> View project at: <a href='https://wandb.ai/adel-haj-jumah-hochschule-hannover/nano-transformer' target=\"_blank\">https://wandb.ai/adel-haj-jumah-hochschule-hannover/nano-transformer</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250424_213542-u4m97u75\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\Adel\\Desktop\\GenerativeAI-Project\\GenerativeAI-Project\\wandb\\run-20250424_213613-yk4jkqtp</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/adel-haj-jumah-hochschule-hannover/nano-transformer/runs/yk4jkqtp' target=\"_blank\">polished-water-16</a></strong> to <a href='https://wandb.ai/adel-haj-jumah-hochschule-hannover/nano-transformer' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/adel-haj-jumah-hochschule-hannover/nano-transformer' target=\"_blank\">https://wandb.ai/adel-haj-jumah-hochschule-hannover/nano-transformer</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/adel-haj-jumah-hochschule-hannover/nano-transformer/runs/yk4jkqtp' target=\"_blank\">https://wandb.ai/adel-haj-jumah-hochschule-hannover/nano-transformer/runs/yk4jkqtp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 4718/4718 [02:14<00:00, 35.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Training Loss = 3.9587\n",
      "Epoch 1: Evaluation Loss = 3.3660\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 4718/4718 [02:35<00:00, 30.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Training Loss = 3.0333\n",
      "Epoch 2: Evaluation Loss = 2.8101\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 4718/4718 [02:37<00:00, 29.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Training Loss = 2.7099\n",
      "Epoch 3: Evaluation Loss = 2.6113\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 4718/4718 [03:10<00:00, 24.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Training Loss = 2.5601\n",
      "Epoch 4: Evaluation Loss = 2.4975\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 4718/4718 [03:24<00:00, 23.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Training Loss = 2.4698\n",
      "Epoch 5: Evaluation Loss = 2.4229\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 4718/4718 [03:24<00:00, 23.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: Training Loss = 2.4080\n",
      "Epoch 6: Evaluation Loss = 2.3788\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 4718/4718 [03:24<00:00, 23.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: Training Loss = 2.3617\n",
      "Epoch 7: Evaluation Loss = 2.3388\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|██████████| 4718/4718 [02:45<00:00, 28.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: Training Loss = 2.3252\n",
      "Epoch 8: Evaluation Loss = 2.3018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 4718/4718 [02:40<00:00, 29.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: Training Loss = 2.2955\n",
      "Epoch 9: Evaluation Loss = 2.2779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|██████████| 4718/4718 [02:36<00:00, 30.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: Training Loss = 2.2699\n",
      "Epoch 10: Evaluation Loss = 2.2472\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "#  NanoTransformer mit PyTorch Decoder\n",
    "from torch.nn import TransformerDecoder, TransformerDecoderLayer\n",
    "\n",
    "class NanoTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, n_heads, d_ff, max_len, num_layers):\n",
    "        super().__init__()\n",
    "        self.embed = TokenAndPositionalEmbedding(vocab_size, d_model, max_len)\n",
    "\n",
    "        decoder_layer = TransformerDecoderLayer(d_model, n_heads, d_ff, batch_first=True)\n",
    "        self.decoder = TransformerDecoder(decoder_layer, num_layers)\n",
    "\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        self.output_proj = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embed(x)\n",
    "        T = x.size(1)\n",
    "\n",
    "        # Causal Mask erstellen\n",
    "        causal_mask = torch.triu(torch.ones(T, T), diagonal=1).bool().to(x.device)\n",
    "\n",
    "        # Dummy Memory\n",
    "        memory = torch.zeros(x.size(0), 1, x.size(2)).to(x.device)\n",
    "\n",
    "        x = self.decoder(tgt=x, memory=memory, tgt_mask=causal_mask)\n",
    "        x = self.norm(x)\n",
    "        logits = self.output_proj(x)\n",
    "        return logits\n",
    "\n",
    "# Training und Evaluierung\n",
    "def evaluate(model, dataloader, vocab_size, device):\n",
    "    model.eval()  # Wechsel in den Evaluationsmodus\n",
    "    total_loss = 0\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    with torch.no_grad():  # Kein Gradientenberechnung\n",
    "        for batch in dataloader:\n",
    "            inputs = batch[:, :-1].to(device)\n",
    "            targets = batch[:, 1:].to(device)\n",
    "\n",
    "            logits = model(inputs)\n",
    "            logits = logits.view(-1, vocab_size)\n",
    "            targets = targets.view(-1)\n",
    "\n",
    "            loss = loss_fn(logits, targets)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    return avg_loss\n",
    "\n",
    "def train(model, train_dataloader, eval_dataloader, vocab_size, device, epochs=10, lr=1e-4):\n",
    "    wandb.init(project=\"nano-transformer\", config={\n",
    "        \"epochs\": epochs,\n",
    "        \"lr\": lr,\n",
    "        \"batch_size\": train_dataloader.batch_size,\n",
    "        \"seq_len\": train_dataloader.dataset.seq_len,\n",
    "        \"vocab_size\": vocab_size,\n",
    "        \"dataset\": \"tiny_shakespeare\",\n",
    "        \"d_ff\": d_ff,\n",
    "        \"max_len\": max_len,\n",
    "        \"num_layers\": num_layers\n",
    "    })\n",
    "\n",
    "    model = model.to(device)\n",
    "    optimizer =  optim.Adam(model.parameters(), lr=lr)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for batch in tqdm(train_dataloader, desc=f\"Epoch {epoch+1}\"):\n",
    "            inputs = batch[:, :-1].to(device)\n",
    "            targets = batch[:, 1:].to(device)\n",
    "\n",
    "            logits = model(inputs)\n",
    "            logits = logits.view(-1, vocab_size)\n",
    "            targets = targets.view(-1)\n",
    "\n",
    "            loss = loss_fn(logits, targets)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = total_loss / len(train_dataloader)\n",
    "        print(f\"Epoch {epoch+1}: Training Loss = {avg_train_loss:.4f}\")\n",
    "\n",
    "        # Evaluierung nach jedem Trainingsepochendurchgang\n",
    "        avg_eval_loss = evaluate(model, eval_dataloader, vocab_size, device)\n",
    "        print(f\"Epoch {epoch+1}: Evaluation Loss = {avg_eval_loss:.4f}\")\n",
    "\n",
    "        # Logs an WandB senden\n",
    "        wandb.log({\n",
    "            \"epoch\": epoch+1,\n",
    "            \"train_loss\": avg_train_loss,\n",
    "            \"eval_loss\": avg_eval_loss\n",
    "        })\n",
    "\n",
    "    # Modell speichern nach Training\n",
    "    torch.save(model.state_dict(), \"nano_transformer_trained.pth\")\n",
    "\n",
    "# Dataset vorbereiten\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "dataset = load_dataset(\"tiny_shakespeare\", split=\"train\", trust_remote_code=True)\n",
    "text = dataset['text'][0]\n",
    "tokens = tokenizer.encode(text, truncation=False)\n",
    "tokens = torch.tensor(tokens)\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, tokens, seq_len):\n",
    "        self.tokens = tokens\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tokens) - self.seq_len\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.tokens[idx:idx+self.seq_len+1]\n",
    "\n",
    "seq_len = 64\n",
    "batch_size = 64\n",
    "dataset = TextDataset(tokens, seq_len)\n",
    "train_dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Erstellen eines Evaluationsdatensatzes (z.B. indem wir die ersten 20% des Datensatzes verwenden)\n",
    "eval_dataset = TextDataset(tokens[dataset.seq_len:], seq_len)  # Hier könnten auch speziellere Evaluierungsdaten genutzt werden\n",
    "eval_dataloader = DataLoader(eval_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(\"DataLoader bereit!\")\n",
    "\n",
    "# Modell konfigurieren\n",
    "vocab_size = tokenizer.vocab_size\n",
    "d_model = 32\n",
    "n_heads = 4\n",
    "d_ff = 512  \n",
    "max_len = 128\n",
    "num_layers = 2\n",
    "\n",
    "model = NanoTransformer(vocab_size, d_model, n_heads, d_ff, max_len, num_layers)\n",
    "\n",
    "# Training starten\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "train(model, train_dataloader, eval_dataloader, vocab_size, device, epochs=10, lr=3e-3)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
