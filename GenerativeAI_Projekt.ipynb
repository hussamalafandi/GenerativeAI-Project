{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c027491",
   "metadata": {},
   "source": [
    "# GenerativeAI \"Sprachmodell\" Projekt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb4417d7",
   "metadata": {},
   "source": [
    "## Verbing mit wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e5700284",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33madel-haj-jumah\u001b[0m (\u001b[33madel-haj-jumah-hochschule-hannover\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe187fce",
   "metadata": {},
   "source": [
    "1. Importieren die notwendigen Bibliotheken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "149a539f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from pathlib import Path\n",
    "import random\n",
    "from datasets import load_dataset\n",
    "import torch.optim as optim\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe1cf68",
   "metadata": {},
   "source": [
    "2. Token And Positional Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed6d9690",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TokenAndPositionalEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, max_len):\n",
    "        super().__init__()\n",
    "        # ID der Token in einen Vektorraum\n",
    "        self.token_embed = nn.Embedding(vocab_size, d_model)\n",
    "        # Positionale Einbettungen (lernen relative Positionen \"das ist learnable\")\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, 512, d_model)) # batch 1 , bis zu 512 token und vektor größe\n",
    "    def forward(self, x):\n",
    "        seq_len = x.size(1) # 1 ist Anzahl der Token\"Sequenzlänge\" (0 ist batch)\n",
    "        token_emb = self.token_embed(x)\n",
    "        pos_emb = self.pos_embedding[:, :seq_len, :]\n",
    "        return token_emb + pos_emb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d7e418",
   "metadata": {},
   "source": [
    "3. Masked Multi-Head Self-Attention\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ed2db8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MaskedSelfAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Masked (Causal) Self-Attention Layer\n",
    "    → Modell kann nur auf vergangene Tokens schauen\n",
    "    → Verwendet PyTorch nn.MultiheadAttention\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, n_heads):\n",
    "        super().__init__()\n",
    "        self.attn = nn.MultiheadAttention(\n",
    "            embed_dim=d_model, \n",
    "            num_heads=n_heads, \n",
    "            batch_first=True  # wichtig! Damit x.shape = (B, T, C) funktioniert\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        T = x.size(1)  # Sequence Length\n",
    "\n",
    "        # Causal Mask (obere Dreiecksmatrix)\n",
    "        mask = torch.triu(torch.ones(T, T), diagonal=1).bool().to(x.device)\n",
    "\n",
    "        # MultiheadAttention erwartet: (query, key, value, attn_mask)\n",
    "        out, _ = self.attn(x, x, x, attn_mask=mask)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7abb1c6",
   "metadata": {},
   "source": [
    "4. Add & Norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9cf72fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddNorm(nn.Module):\n",
    "    def __init__(self, d_model):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x, sublayer_output):\n",
    "        return self.norm(x + sublayer_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a2036d",
   "metadata": {},
   "source": [
    "5. Feedforward Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ebdfb3b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_ff, d_model)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db87d56",
   "metadata": {},
   "source": [
    "6. Alles zusammenfügen: Transformator-Decoderblock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "39bb6180",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, d_ff):\n",
    "        super().__init__()\n",
    "        self.attn = MaskedSelfAttention(d_model, n_heads)  # PyTorch MultiheadAttention inside\n",
    "        self.addnorm1 = AddNorm(d_model)\n",
    "        self.ff = FeedForward(d_model, d_ff)\n",
    "        self.addnorm2 = AddNorm(d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.addnorm1(x, self.attn(x))  # Attention + Residual + LayerNorm\n",
    "        x = self.addnorm2(x, self.ff(x))    # FeedForward + Residual + LayerNorm\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b91e5a",
   "metadata": {},
   "source": [
    "7. Zusammenbau des NanoTransformers (nur Decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "977967f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class NanoTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, n_heads, d_ff, max_len, num_layers):\n",
    "        super().__init__()\n",
    "        self.embed = TokenAndPositionalEmbedding(vocab_size, d_model, max_len)\n",
    "        self.blocks = nn.ModuleList([\n",
    "            TransformerBlock(d_model, n_heads, d_ff) for _ in range(num_layers)\n",
    "        ])\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        self.output_proj = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embed(x)\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        x = self.norm(x)\n",
    "        logits = self.output_proj(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e17f607c",
   "metadata": {},
   "source": [
    "8. Trainieren des Modells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4704706d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def train(model, dataloader, vocab_size, device, epochs=10, lr=1e-4):\n",
    "    # Wandb initialisieren, nur einmal zu Beginn des Trainings\n",
    "    wandb.init(project=\"nano-transformer\", config={\n",
    "        \"epochs\": epochs,\n",
    "        \"lr\": lr,\n",
    "        \"batch_size\": dataloader.batch_size,\n",
    "        \"seq_len\": dataloader.dataset.seq_len,\n",
    "        \"vocab_size\": vocab_size\n",
    "    })\n",
    "    \n",
    "    # Modell auf das richtige Gerät (GPU oder CPU) verschieben\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Optimizer und Loss-Funktion\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    model.train()  # Modell in Trainingsmodus versetzen\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for batch in tqdm(dataloader, desc=\"Training Progress\"):\n",
    "            # Eingabe- und Ziel-Tensoren\n",
    "            inputs = batch[:, :-1].to(device)\n",
    "            targets = batch[:, 1:].to(device)\n",
    "\n",
    "            # Vorwärtsdurchlauf\n",
    "            logits = model(inputs)\n",
    "            logits = logits.view(-1, vocab_size)  # Umformen für CrossEntropyLoss\n",
    "            targets = targets.view(-1)  # Umformen für CrossEntropyLoss\n",
    "\n",
    "            # Verlustberechnung\n",
    "            loss = loss_fn(logits, targets)\n",
    "\n",
    "            # Backpropagation und Optimierung\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Gesamten Verlust summieren\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        # Durchschnittlichen Verlust für die Epoche berechnen\n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        print(f\"Epoch {epoch+1}: Loss = {avg_loss:.4f}\")\n",
    "\n",
    "        # Verlust an Wandb senden\n",
    "        wandb.log({\"epoch\": epoch+1, \"loss\": avg_loss})\n",
    "    \n",
    "    # Modell speichern nach Training (optional)\n",
    "    torch.save(model.state_dict(), \"nano_transformer_trained.pth\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4700336",
   "metadata": {},
   "source": [
    "9. Dataset Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4293020e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (301966 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataLoader bereit!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "\n",
    "# 1. Tokenizer laden\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# 2. Tiny Shakespeare Dataset laden\n",
    "dataset = load_dataset(\"tiny_shakespeare\", split=\"train\", trust_remote_code=True)\n",
    "text = dataset['text'][0]\n",
    "\n",
    "# 3. Text in Token IDs umwandeln\n",
    "tokens = tokenizer.encode(text, truncation=False)\n",
    "tokens = torch.tensor(tokens)\n",
    "\n",
    "# 4. Dataset Klasse\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, tokens, seq_len):\n",
    "        self.tokens = tokens\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tokens) - self.seq_len\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.tokens[idx:idx+self.seq_len+1]\n",
    "\n",
    "# 5. DataLoader bauen\n",
    "seq_len = 64\n",
    "batch_size = 32\n",
    "dataset = TextDataset(tokens, seq_len)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "print(\"DataLoader bereit!\")\n",
    "\n",
    "# 6. Den Modell (Mini NanoTransformer)\n",
    "vocab_size = tokenizer.vocab_size\n",
    "d_model = 32\n",
    "n_heads = 1\n",
    "d_ff = 256\n",
    "max_len = 64\n",
    "num_layers = 1\n",
    "\n",
    "model = NanoTransformer(vocab_size, d_model, n_heads, d_ff, max_len, num_layers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e654952a",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "train(model, dataloader, vocab_size, device, epochs=10, lr=3e-4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dd26ac85",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (301966 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataLoader bereit!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "\n",
    "# 1. Tokenizer laden\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# 2. Tiny Shakespeare Dataset laden\n",
    "dataset = load_dataset(\"tiny_shakespeare\", split=\"train\", trust_remote_code=True)\n",
    "text = dataset['text'][0]\n",
    "\n",
    "# 3. Text in Token IDs umwandeln\n",
    "tokens = tokenizer.encode(text, truncation=False)\n",
    "tokens = torch.tensor(tokens)\n",
    "\n",
    "# 4. Dataset Klasse\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, tokens, seq_len):\n",
    "        self.tokens = tokens\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tokens) - self.seq_len\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.tokens[idx:idx+self.seq_len+1]\n",
    "\n",
    "# 5. DataLoader bauen\n",
    "seq_len = 64\n",
    "batch_size = 32\n",
    "dataset = TextDataset(tokens, seq_len)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "print(\"DataLoader bereit!\")\n",
    "\n",
    "\n",
    "d_model = 128\n",
    "n_heads = 4\n",
    "d_ff = 512\n",
    "num_layers = 2\n",
    "seq_len = 128 # 128 Token pro Sequenz\n",
    "max_len = 128 # 128 Token pro Sequenz\n",
    "\n",
    "\n",
    "model = NanoTransformer(vocab_size, d_model, n_heads, d_ff, max_len, num_layers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "194df713",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "train(model, dataloader, vocab_size, device, epochs=10, lr=3e-4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e4c2289",
   "metadata": {},
   "source": [
    "## MaskedSelfAttention, AddNorm und FeedForward ersetzen durch nn.TransformerDecoderLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ad38c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "#  NanoTransformer mit PyTorch Decoder\n",
    "from torch.nn import TransformerDecoder, TransformerDecoderLayer\n",
    "\n",
    "class NanoTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, n_heads, d_ff, max_len, num_layers):\n",
    "        super().__init__()\n",
    "        self.embed = TokenAndPositionalEmbedding(vocab_size, d_model, max_len)\n",
    "\n",
    "        decoder_layer = TransformerDecoderLayer(d_model, n_heads, d_ff, batch_first=True)\n",
    "        self.decoder = TransformerDecoder(decoder_layer, num_layers)\n",
    "\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        self.output_proj = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embed(x)\n",
    "        T = x.size(1)\n",
    "\n",
    "        # Causal Mask erstellen\n",
    "        causal_mask = torch.triu(torch.ones(T, T), diagonal=1).bool().to(x.device)\n",
    "\n",
    "        # Dummy Memory\n",
    "        memory = torch.zeros(x.size(0), 1, x.size(2)).to(x.device)\n",
    "\n",
    "        x = self.decoder(tgt=x, memory=memory, tgt_mask=causal_mask)\n",
    "        x = self.norm(x)\n",
    "        logits = self.output_proj(x)\n",
    "        return logits\n",
    "\n",
    "# Training und Evaluierung\n",
    "def evaluate(model, dataloader, vocab_size, device):\n",
    "    model.eval()  # Wechsel in den Evaluationsmodus\n",
    "    total_loss = 0\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    with torch.no_grad():  # Kein Gradientenberechnung\n",
    "        for batch in dataloader:\n",
    "            inputs = batch[:, :-1].to(device)\n",
    "            targets = batch[:, 1:].to(device)\n",
    "\n",
    "            logits = model(inputs)\n",
    "            logits = logits.view(-1, vocab_size)\n",
    "            targets = targets.view(-1)\n",
    "\n",
    "            loss = loss_fn(logits, targets)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    return avg_loss\n",
    "\n",
    "def train(model, train_dataloader, eval_dataloader, vocab_size, device, epochs=10, lr=1e-4):\n",
    "    wandb.init(project=\"nano-transformer\", config={\n",
    "        \"epochs\": epochs,\n",
    "        \"lr\": lr,\n",
    "        \"batch_size\": train_dataloader.batch_size,\n",
    "        \"seq_len\": train_dataloader.dataset.seq_len,\n",
    "        \"vocab_size\": vocab_size,\n",
    "        \"dataset\": \"tiny_shakespeare\",\n",
    "        \"d_ff\": d_ff,\n",
    "        \"max_len\": max_len,\n",
    "        \"num_layers\": num_layers\n",
    "    })\n",
    "\n",
    "    model = model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for batch in tqdm(train_dataloader, desc=f\"Epoch {epoch+1}\"):\n",
    "            inputs = batch[:, :-1].to(device)\n",
    "            targets = batch[:, 1:].to(device)\n",
    "\n",
    "            logits = model(inputs)\n",
    "            logits = logits.view(-1, vocab_size)\n",
    "            targets = targets.view(-1)\n",
    "\n",
    "            loss = loss_fn(logits, targets)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = total_loss / len(train_dataloader)\n",
    "        print(f\"Epoch {epoch+1}: Training Loss = {avg_train_loss:.4f}\")\n",
    "\n",
    "        # Evaluierung nach jedem Trainingsepochendurchgang\n",
    "        avg_eval_loss = evaluate(model, eval_dataloader, vocab_size, device)\n",
    "        print(f\"Epoch {epoch+1}: Evaluation Loss = {avg_eval_loss:.4f}\")\n",
    "\n",
    "        # Logs an WandB senden\n",
    "        wandb.log({\n",
    "            \"epoch\": epoch+1,\n",
    "            \"train_loss\": avg_train_loss,\n",
    "            \"eval_loss\": avg_eval_loss\n",
    "        })\n",
    "\n",
    "    # Modell speichern nach Training\n",
    "    torch.save(model.state_dict(), \"nano_transformer_trained.pth\")\n",
    "\n",
    "# Dataset vorbereiten\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "dataset = load_dataset(\"tiny_shakespeare\", split=\"train\", trust_remote_code=True)\n",
    "text = dataset['text'][0]\n",
    "tokens = tokenizer.encode(text, truncation=False)\n",
    "tokens = torch.tensor(tokens)\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, tokens, seq_len):\n",
    "        self.tokens = tokens\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tokens) - self.seq_len\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.tokens[idx:idx+self.seq_len+1]\n",
    "\n",
    "seq_len = 64\n",
    "batch_size = 32\n",
    "dataset = TextDataset(tokens, seq_len)\n",
    "train_dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Erstellen eines Evaluationsdatensatzes (z.B. indem wir die ersten 20% des Datensatzes verwenden)\n",
    "eval_dataset = TextDataset(tokens[dataset.seq_len:], seq_len)  # Hier könnten auch speziellere Evaluierungsdaten genutzt werden\n",
    "eval_dataloader = DataLoader(eval_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(\"DataLoader bereit!\")\n",
    "\n",
    "# Modell konfigurieren\n",
    "vocab_size = tokenizer.vocab_size\n",
    "d_model = 32\n",
    "n_heads = 1\n",
    "d_ff = 256\n",
    "max_len = 64\n",
    "num_layers = 1\n",
    "\n",
    "model = NanoTransformer(vocab_size, d_model, n_heads, d_ff, max_len, num_layers)\n",
    "\n",
    "# Training starten\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "train(model, train_dataloader, eval_dataloader, vocab_size, device, epochs=100, lr=3e-4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "368eb653",
   "metadata": {},
   "source": [
    "### lr statt 0.0003 zu 0.003"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4b46d989",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataLoader bereit!\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▂▃▄▅▅▆▇▇█</td></tr><tr><td>eval_loss</td><td>█▅▄▃▃▂▂▂▁▁▁</td></tr><tr><td>train_loss</td><td>█▄▃▃▂▂▂▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>11</td></tr><tr><td>eval_loss</td><td>3.12321</td></tr><tr><td>train_loss</td><td>3.14373</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">devout-silence-8</strong> at: <a href='https://wandb.ai/adel-haj-jumah-hochschule-hannover/nano-transformer/runs/7k65u329' target=\"_blank\">https://wandb.ai/adel-haj-jumah-hochschule-hannover/nano-transformer/runs/7k65u329</a><br> View project at: <a href='https://wandb.ai/adel-haj-jumah-hochschule-hannover/nano-transformer' target=\"_blank\">https://wandb.ai/adel-haj-jumah-hochschule-hannover/nano-transformer</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250424_170714-7k65u329\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\Adel\\Desktop\\GenerativeAI-Project\\GenerativeAI-Project\\wandb\\run-20250424_174145-fq7ea85h</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/adel-haj-jumah-hochschule-hannover/nano-transformer/runs/fq7ea85h' target=\"_blank\">winter-brook-9</a></strong> to <a href='https://wandb.ai/adel-haj-jumah-hochschule-hannover/nano-transformer' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/adel-haj-jumah-hochschule-hannover/nano-transformer' target=\"_blank\">https://wandb.ai/adel-haj-jumah-hochschule-hannover/nano-transformer</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/adel-haj-jumah-hochschule-hannover/nano-transformer/runs/fq7ea85h' target=\"_blank\">https://wandb.ai/adel-haj-jumah-hochschule-hannover/nano-transformer/runs/fq7ea85h</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 9435/9435 [02:01<00:00, 77.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Training Loss = 3.8809\n",
      "Epoch 1: Evaluation Loss = 3.4163\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 9435/9435 [01:57<00:00, 80.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Training Loss = 3.2139\n",
      "Epoch 2: Evaluation Loss = 3.0782\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 9435/9435 [01:54<00:00, 82.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Training Loss = 3.0033\n",
      "Epoch 3: Evaluation Loss = 2.9272\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 9435/9435 [01:54<00:00, 82.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Training Loss = 2.8861\n",
      "Epoch 4: Evaluation Loss = 2.8302\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 9435/9435 [01:56<00:00, 80.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Training Loss = 2.8056\n",
      "Epoch 5: Evaluation Loss = 2.7626\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 9435/9435 [01:50<00:00, 85.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: Training Loss = 2.7456\n",
      "Epoch 6: Evaluation Loss = 2.7118\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 9435/9435 [01:50<00:00, 85.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: Training Loss = 2.6989\n",
      "Epoch 7: Evaluation Loss = 2.6711\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|██████████| 9435/9435 [01:52<00:00, 84.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: Training Loss = 2.6617\n",
      "Epoch 8: Evaluation Loss = 2.6357\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 9435/9435 [01:49<00:00, 85.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: Training Loss = 2.6311\n",
      "Epoch 9: Evaluation Loss = 2.6102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|██████████| 9435/9435 [01:53<00:00, 82.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: Training Loss = 2.6050\n",
      "Epoch 10: Evaluation Loss = 2.5891\n"
     ]
    }
   ],
   "source": [
    "# Erstellen eines Evaluationsdatensatzes (z.B. indem wir die ersten 20% des Datensatzes verwenden)\n",
    "eval_dataset = TextDataset(tokens[dataset.seq_len:], seq_len)  # Hier könnten auch speziellere Evaluierungsdaten genutzt werden\n",
    "eval_dataloader = DataLoader(eval_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(\"DataLoader bereit!\")\n",
    "\n",
    "# Modell konfigurieren\n",
    "vocab_size = tokenizer.vocab_size\n",
    "d_model = 32\n",
    "n_heads = 1\n",
    "d_ff = 256\n",
    "max_len = 64\n",
    "num_layers = 1\n",
    "\n",
    "model = NanoTransformer(vocab_size, d_model, n_heads, d_ff, max_len, num_layers)\n",
    "\n",
    "# Training starten\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "train(model, train_dataloader, eval_dataloader, vocab_size, device, epochs=10, lr=3e-3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33785a79",
   "metadata": {},
   "source": [
    "### num_layers =von 1 zu  2 / n_heads = von 1 zu 4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9803a3cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataLoader bereit!\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>eval_loss</td><td>█▅▄▃▂▂▂▁▁▁</td></tr><tr><td>train_loss</td><td>█▄▃▃▂▂▂▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>10</td></tr><tr><td>eval_loss</td><td>2.58912</td></tr><tr><td>train_loss</td><td>2.60498</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">winter-brook-9</strong> at: <a href='https://wandb.ai/adel-haj-jumah-hochschule-hannover/nano-transformer/runs/fq7ea85h' target=\"_blank\">https://wandb.ai/adel-haj-jumah-hochschule-hannover/nano-transformer/runs/fq7ea85h</a><br> View project at: <a href='https://wandb.ai/adel-haj-jumah-hochschule-hannover/nano-transformer' target=\"_blank\">https://wandb.ai/adel-haj-jumah-hochschule-hannover/nano-transformer</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250424_174145-fq7ea85h\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\Adel\\Desktop\\GenerativeAI-Project\\GenerativeAI-Project\\wandb\\run-20250424_181244-69egtk7f</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/adel-haj-jumah-hochschule-hannover/nano-transformer/runs/69egtk7f' target=\"_blank\">radiant-bird-10</a></strong> to <a href='https://wandb.ai/adel-haj-jumah-hochschule-hannover/nano-transformer' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/adel-haj-jumah-hochschule-hannover/nano-transformer' target=\"_blank\">https://wandb.ai/adel-haj-jumah-hochschule-hannover/nano-transformer</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/adel-haj-jumah-hochschule-hannover/nano-transformer/runs/69egtk7f' target=\"_blank\">https://wandb.ai/adel-haj-jumah-hochschule-hannover/nano-transformer/runs/69egtk7f</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 9435/9435 [02:18<00:00, 68.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Training Loss = 3.8056\n",
      "Epoch 1: Evaluation Loss = 3.2602\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 9435/9435 [02:16<00:00, 69.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Training Loss = 2.9659\n",
      "Epoch 2: Evaluation Loss = 2.7749\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 9435/9435 [02:20<00:00, 67.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Training Loss = 2.6883\n",
      "Epoch 3: Evaluation Loss = 2.6001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 9435/9435 [02:17<00:00, 68.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Training Loss = 2.5610\n",
      "Epoch 4: Evaluation Loss = 2.5110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 9435/9435 [02:14<00:00, 70.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Training Loss = 2.4837\n",
      "Epoch 5: Evaluation Loss = 2.4507\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 9435/9435 [02:14<00:00, 69.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: Training Loss = 2.4301\n",
      "Epoch 6: Evaluation Loss = 2.3999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 9435/9435 [02:11<00:00, 71.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: Training Loss = 2.3896\n",
      "Epoch 7: Evaluation Loss = 2.3705\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|██████████| 9435/9435 [02:11<00:00, 71.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: Training Loss = 2.3576\n",
      "Epoch 8: Evaluation Loss = 2.3354\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 9435/9435 [02:12<00:00, 71.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: Training Loss = 2.3321\n",
      "Epoch 9: Evaluation Loss = 2.3148\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|██████████| 9435/9435 [02:21<00:00, 66.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: Training Loss = 2.3096\n",
      "Epoch 10: Evaluation Loss = 2.2918\n"
     ]
    }
   ],
   "source": [
    "# Erstellen eines Evaluationsdatensatzes (z.B. indem wir die ersten 20% des Datensatzes verwenden)\n",
    "eval_dataset = TextDataset(tokens[dataset.seq_len:], seq_len)  # Hier könnten auch speziellere Evaluierungsdaten genutzt werden\n",
    "eval_dataloader = DataLoader(eval_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(\"DataLoader bereit!\")\n",
    "\n",
    "# Modell konfigurieren\n",
    "vocab_size = tokenizer.vocab_size\n",
    "d_model = 32\n",
    "n_heads = 4\n",
    "d_ff = 256\n",
    "max_len = 64\n",
    "num_layers = 2\n",
    "\n",
    "model = NanoTransformer(vocab_size, d_model, n_heads, d_ff, max_len, num_layers)\n",
    "\n",
    "# Training starten\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "train(model, train_dataloader, eval_dataloader, vocab_size, device, epochs=10, lr=3e-3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da90c58",
   "metadata": {},
   "source": [
    "### max_len = von 64 zu 128 / d_ff = von 256 zu 512  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5bcdb2c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataLoader bereit!\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>eval_loss</td><td>█▄▃▃▂▂▂▁▁▁</td></tr><tr><td>train_loss</td><td>█▄▃▂▂▂▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>10</td></tr><tr><td>eval_loss</td><td>2.2918</td></tr><tr><td>train_loss</td><td>2.30964</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">radiant-bird-10</strong> at: <a href='https://wandb.ai/adel-haj-jumah-hochschule-hannover/nano-transformer/runs/69egtk7f' target=\"_blank\">https://wandb.ai/adel-haj-jumah-hochschule-hannover/nano-transformer/runs/69egtk7f</a><br> View project at: <a href='https://wandb.ai/adel-haj-jumah-hochschule-hannover/nano-transformer' target=\"_blank\">https://wandb.ai/adel-haj-jumah-hochschule-hannover/nano-transformer</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250424_181244-69egtk7f\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\Adel\\Desktop\\GenerativeAI-Project\\GenerativeAI-Project\\wandb\\run-20250424_184854-mni0nra1</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/adel-haj-jumah-hochschule-hannover/nano-transformer/runs/mni0nra1' target=\"_blank\">blooming-frost-11</a></strong> to <a href='https://wandb.ai/adel-haj-jumah-hochschule-hannover/nano-transformer' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/adel-haj-jumah-hochschule-hannover/nano-transformer' target=\"_blank\">https://wandb.ai/adel-haj-jumah-hochschule-hannover/nano-transformer</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/adel-haj-jumah-hochschule-hannover/nano-transformer/runs/mni0nra1' target=\"_blank\">https://wandb.ai/adel-haj-jumah-hochschule-hannover/nano-transformer/runs/mni0nra1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 9435/9435 [02:24<00:00, 65.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Training Loss = 3.8457\n",
      "Epoch 1: Evaluation Loss = 3.2873\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 9435/9435 [02:20<00:00, 67.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Training Loss = 2.9488\n",
      "Epoch 2: Evaluation Loss = 2.7384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 9435/9435 [02:20<00:00, 67.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Training Loss = 2.6451\n",
      "Epoch 3: Evaluation Loss = 2.5582\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 9435/9435 [02:20<00:00, 67.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Training Loss = 2.5092\n",
      "Epoch 4: Evaluation Loss = 2.4452\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 9435/9435 [02:19<00:00, 67.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Training Loss = 2.4271\n",
      "Epoch 5: Evaluation Loss = 2.3794\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 9435/9435 [02:19<00:00, 67.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: Training Loss = 2.3698\n",
      "Epoch 6: Evaluation Loss = 2.3415\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 9435/9435 [02:19<00:00, 67.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: Training Loss = 2.3280\n",
      "Epoch 7: Evaluation Loss = 2.2969\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|██████████| 9435/9435 [02:19<00:00, 67.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: Training Loss = 2.2949\n",
      "Epoch 8: Evaluation Loss = 2.2728\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 9435/9435 [02:19<00:00, 67.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: Training Loss = 2.2676\n",
      "Epoch 9: Evaluation Loss = 2.2426\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|██████████| 9435/9435 [02:17<00:00, 68.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: Training Loss = 2.2442\n",
      "Epoch 10: Evaluation Loss = 2.2160\n"
     ]
    }
   ],
   "source": [
    "# Erstellen eines Evaluationsdatensatzes (z.B. indem wir die ersten 20% des Datensatzes verwenden)\n",
    "eval_dataset = TextDataset(tokens[dataset.seq_len:], seq_len)  # Hier könnten auch speziellere Evaluierungsdaten genutzt werden\n",
    "eval_dataloader = DataLoader(eval_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(\"DataLoader bereit!\")\n",
    "\n",
    "# Modell konfigurieren\n",
    "vocab_size = tokenizer.vocab_size\n",
    "d_model = 32\n",
    "n_heads = 4\n",
    "d_ff = 512  \n",
    "max_len = 128\n",
    "num_layers = 2\n",
    "\n",
    "model = NanoTransformer(vocab_size, d_model, n_heads, d_ff, max_len, num_layers)\n",
    "\n",
    "# Training starten\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "train(model, train_dataloader, eval_dataloader, vocab_size, device, epochs=10, lr=3e-3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "278d4a56",
   "metadata": {},
   "source": [
    "### d_model = von 32 zu 128 / num_layers = von 2 zu 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "74d4a55c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>eval_loss</td><td>█▄▃▂▂▂▂▁▁▁</td></tr><tr><td>train_loss</td><td>█▄▃▂▂▂▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>10</td></tr><tr><td>eval_loss</td><td>2.216</td></tr><tr><td>train_loss</td><td>2.24425</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">blooming-frost-11</strong> at: <a href='https://wandb.ai/adel-haj-jumah-hochschule-hannover/nano-transformer/runs/mni0nra1' target=\"_blank\">https://wandb.ai/adel-haj-jumah-hochschule-hannover/nano-transformer/runs/mni0nra1</a><br> View project at: <a href='https://wandb.ai/adel-haj-jumah-hochschule-hannover/nano-transformer' target=\"_blank\">https://wandb.ai/adel-haj-jumah-hochschule-hannover/nano-transformer</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250424_184854-mni0nra1\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\Adel\\Desktop\\GenerativeAI-Project\\GenerativeAI-Project\\wandb\\run-20250424_192755-20um92u3</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/adel-haj-jumah-hochschule-hannover/nano-transformer/runs/20um92u3' target=\"_blank\">apricot-surf-12</a></strong> to <a href='https://wandb.ai/adel-haj-jumah-hochschule-hannover/nano-transformer' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/adel-haj-jumah-hochschule-hannover/nano-transformer' target=\"_blank\">https://wandb.ai/adel-haj-jumah-hochschule-hannover/nano-transformer</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/adel-haj-jumah-hochschule-hannover/nano-transformer/runs/20um92u3' target=\"_blank\">https://wandb.ai/adel-haj-jumah-hochschule-hannover/nano-transformer/runs/20um92u3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 9435/9435 [03:48<00:00, 41.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Training Loss = 5.9286\n",
      "Epoch 1: Evaluation Loss = 5.8530\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 9435/9435 [03:46<00:00, 41.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Training Loss = 5.4539\n",
      "Epoch 2: Evaluation Loss = 5.0933\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 9435/9435 [03:45<00:00, 41.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Training Loss = 5.1421\n",
      "Epoch 3: Evaluation Loss = 5.0835\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 9435/9435 [03:45<00:00, 41.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Training Loss = 5.0913\n",
      "Epoch 4: Evaluation Loss = 5.0308\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 9435/9435 [03:54<00:00, 40.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Training Loss = 4.7355\n",
      "Epoch 5: Evaluation Loss = 4.5584\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 9435/9435 [03:36<00:00, 43.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: Training Loss = 4.4312\n",
      "Epoch 6: Evaluation Loss = 4.2819\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 9435/9435 [03:45<00:00, 41.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: Training Loss = 4.2090\n",
      "Epoch 7: Evaluation Loss = 4.1360\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|██████████| 9435/9435 [03:34<00:00, 44.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: Training Loss = 4.0530\n",
      "Epoch 8: Evaluation Loss = 3.9643\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 9435/9435 [03:32<00:00, 44.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: Training Loss = 3.9459\n",
      "Epoch 9: Evaluation Loss = 3.8622\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|██████████| 9435/9435 [03:32<00:00, 44.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: Training Loss = 3.8531\n",
      "Epoch 10: Evaluation Loss = 3.8033\n"
     ]
    }
   ],
   "source": [
    "# Modell konfigurieren\n",
    "vocab_size = tokenizer.vocab_size\n",
    "d_model = 128\n",
    "n_heads = 4\n",
    "d_ff = 512  \n",
    "max_len = 128\n",
    "num_layers = 4\n",
    "\n",
    "model = NanoTransformer(vocab_size, d_model, n_heads, d_ff, max_len, num_layers)\n",
    "\n",
    "# Training starten\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "train(model, train_dataloader, eval_dataloader, vocab_size, device, epochs=10, lr=3e-3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b2612cf",
   "metadata": {},
   "source": [
    "### Optimierungsalgorithmus von  Adam zu SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "31efa2da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (301966 > 1024). Running this sequence through the model will result in indexing errors\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataLoader bereit!\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>eval_loss</td><td>█▆▅▄▃▃▂▂▁▁</td></tr><tr><td>train_loss</td><td>█▅▄▃▃▂▂▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>10</td></tr><tr><td>eval_loss</td><td>7.46348</td></tr><tr><td>train_loss</td><td>7.50356</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">zesty-wind-13</strong> at: <a href='https://wandb.ai/adel-haj-jumah-hochschule-hannover/nano-transformer/runs/vraq85ye' target=\"_blank\">https://wandb.ai/adel-haj-jumah-hochschule-hannover/nano-transformer/runs/vraq85ye</a><br> View project at: <a href='https://wandb.ai/adel-haj-jumah-hochschule-hannover/nano-transformer' target=\"_blank\">https://wandb.ai/adel-haj-jumah-hochschule-hannover/nano-transformer</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250424_202300-vraq85ye\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\Adel\\Desktop\\GenerativeAI-Project\\GenerativeAI-Project\\wandb\\run-20250424_205659-9dap3nes</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/adel-haj-jumah-hochschule-hannover/nano-transformer/runs/9dap3nes' target=\"_blank\">cosmic-water-14</a></strong> to <a href='https://wandb.ai/adel-haj-jumah-hochschule-hannover/nano-transformer' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/adel-haj-jumah-hochschule-hannover/nano-transformer' target=\"_blank\">https://wandb.ai/adel-haj-jumah-hochschule-hannover/nano-transformer</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/adel-haj-jumah-hochschule-hannover/nano-transformer/runs/9dap3nes' target=\"_blank\">https://wandb.ai/adel-haj-jumah-hochschule-hannover/nano-transformer/runs/9dap3nes</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 9435/9435 [02:15<00:00, 69.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Training Loss = 8.3323\n",
      "Epoch 1: Evaluation Loss = 7.5116\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 9435/9435 [02:13<00:00, 70.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Training Loss = 7.2151\n",
      "Epoch 2: Evaluation Loss = 6.9832\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 9435/9435 [02:12<00:00, 71.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Training Loss = 6.8284\n",
      "Epoch 3: Evaluation Loss = 6.6963\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 9435/9435 [02:14<00:00, 70.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Training Loss = 6.5963\n",
      "Epoch 4: Evaluation Loss = 6.5087\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 9435/9435 [02:13<00:00, 70.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Training Loss = 6.4402\n",
      "Epoch 5: Evaluation Loss = 6.3793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 9435/9435 [02:08<00:00, 73.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: Training Loss = 6.3292\n",
      "Epoch 6: Evaluation Loss = 6.2830\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 9435/9435 [02:10<00:00, 72.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: Training Loss = 6.2421\n",
      "Epoch 7: Evaluation Loss = 6.2033\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|██████████| 9435/9435 [02:15<00:00, 69.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: Training Loss = 6.1660\n",
      "Epoch 8: Evaluation Loss = 6.1289\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 9435/9435 [02:14<00:00, 69.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: Training Loss = 6.0917\n",
      "Epoch 9: Evaluation Loss = 6.0546\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|██████████| 9435/9435 [02:14<00:00, 70.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: Training Loss = 6.0164\n",
      "Epoch 10: Evaluation Loss = 5.9775\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "#  NanoTransformer mit PyTorch Decoder\n",
    "from torch.nn import TransformerDecoder, TransformerDecoderLayer\n",
    "\n",
    "class NanoTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, n_heads, d_ff, max_len, num_layers):\n",
    "        super().__init__()\n",
    "        self.embed = TokenAndPositionalEmbedding(vocab_size, d_model, max_len)\n",
    "\n",
    "        decoder_layer = TransformerDecoderLayer(d_model, n_heads, d_ff, batch_first=True)\n",
    "        self.decoder = TransformerDecoder(decoder_layer, num_layers)\n",
    "\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        self.output_proj = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embed(x)\n",
    "        T = x.size(1)\n",
    "\n",
    "        # Causal Mask erstellen\n",
    "        causal_mask = torch.triu(torch.ones(T, T), diagonal=1).bool().to(x.device)\n",
    "\n",
    "        # Dummy Memory\n",
    "        memory = torch.zeros(x.size(0), 1, x.size(2)).to(x.device)\n",
    "\n",
    "        x = self.decoder(tgt=x, memory=memory, tgt_mask=causal_mask)\n",
    "        x = self.norm(x)\n",
    "        logits = self.output_proj(x)\n",
    "        return logits\n",
    "\n",
    "# Training und Evaluierung\n",
    "def evaluate(model, dataloader, vocab_size, device):\n",
    "    model.eval()  # Wechsel in den Evaluationsmodus\n",
    "    total_loss = 0\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    with torch.no_grad():  # Kein Gradientenberechnung\n",
    "        for batch in dataloader:\n",
    "            inputs = batch[:, :-1].to(device)\n",
    "            targets = batch[:, 1:].to(device)\n",
    "\n",
    "            logits = model(inputs)\n",
    "            logits = logits.view(-1, vocab_size)\n",
    "            targets = targets.view(-1)\n",
    "\n",
    "            loss = loss_fn(logits, targets)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    return avg_loss\n",
    "\n",
    "def train(model, train_dataloader, eval_dataloader, vocab_size, device, epochs=10, lr=1e-4):\n",
    "    wandb.init(project=\"nano-transformer\", config={\n",
    "        \"epochs\": epochs,\n",
    "        \"lr\": lr,\n",
    "        \"batch_size\": train_dataloader.batch_size,\n",
    "        \"seq_len\": train_dataloader.dataset.seq_len,\n",
    "        \"vocab_size\": vocab_size,\n",
    "        \"dataset\": \"tiny_shakespeare\",\n",
    "        \"d_ff\": d_ff,\n",
    "        \"max_len\": max_len,\n",
    "        \"num_layers\": num_layers\n",
    "    })\n",
    "\n",
    "    model = model.to(device)\n",
    "    optimizer =  optim.SGD(model.parameters(), lr=lr)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for batch in tqdm(train_dataloader, desc=f\"Epoch {epoch+1}\"):\n",
    "            inputs = batch[:, :-1].to(device)\n",
    "            targets = batch[:, 1:].to(device)\n",
    "\n",
    "            logits = model(inputs)\n",
    "            logits = logits.view(-1, vocab_size)\n",
    "            targets = targets.view(-1)\n",
    "\n",
    "            loss = loss_fn(logits, targets)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = total_loss / len(train_dataloader)\n",
    "        print(f\"Epoch {epoch+1}: Training Loss = {avg_train_loss:.4f}\")\n",
    "\n",
    "        # Evaluierung nach jedem Trainingsepochendurchgang\n",
    "        avg_eval_loss = evaluate(model, eval_dataloader, vocab_size, device)\n",
    "        print(f\"Epoch {epoch+1}: Evaluation Loss = {avg_eval_loss:.4f}\")\n",
    "\n",
    "        # Logs an WandB senden\n",
    "        wandb.log({\n",
    "            \"epoch\": epoch+1,\n",
    "            \"train_loss\": avg_train_loss,\n",
    "            \"eval_loss\": avg_eval_loss\n",
    "        })\n",
    "\n",
    "    # Modell speichern nach Training\n",
    "    torch.save(model.state_dict(), \"nano_transformer_trained.pth\")\n",
    "\n",
    "# Dataset vorbereiten\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "dataset = load_dataset(\"tiny_shakespeare\", split=\"train\", trust_remote_code=True)\n",
    "text = dataset['text'][0]\n",
    "tokens = tokenizer.encode(text, truncation=False)\n",
    "tokens = torch.tensor(tokens)\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, tokens, seq_len):\n",
    "        self.tokens = tokens\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tokens) - self.seq_len\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.tokens[idx:idx+self.seq_len+1]\n",
    "\n",
    "seq_len = 64\n",
    "batch_size = 32\n",
    "dataset = TextDataset(tokens, seq_len)\n",
    "train_dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Erstellen eines Evaluationsdatensatzes (z.B. indem wir die ersten 20% des Datensatzes verwenden)\n",
    "eval_dataset = TextDataset(tokens[dataset.seq_len:], seq_len)  # Hier könnten auch speziellere Evaluierungsdaten genutzt werden\n",
    "eval_dataloader = DataLoader(eval_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(\"DataLoader bereit!\")\n",
    "\n",
    "# Modell konfigurieren\n",
    "vocab_size = tokenizer.vocab_size\n",
    "d_model = 32\n",
    "n_heads = 4\n",
    "d_ff = 512  \n",
    "max_len = 128\n",
    "num_layers = 2\n",
    "\n",
    "model = NanoTransformer(vocab_size, d_model, n_heads, d_ff, max_len, num_layers)\n",
    "\n",
    "# Training starten\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "train(model, train_dataloader, eval_dataloader, vocab_size, device, epochs=10, lr=3e-3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "380d51bf",
   "metadata": {},
   "source": [
    "### batch_size = von 32 zu  64\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "515f9c2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (301966 > 1024). Running this sequence through the model will result in indexing errors\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataLoader bereit!\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">whole-morning-15</strong> at: <a href='https://wandb.ai/adel-haj-jumah-hochschule-hannover/nano-transformer/runs/u4m97u75' target=\"_blank\">https://wandb.ai/adel-haj-jumah-hochschule-hannover/nano-transformer/runs/u4m97u75</a><br> View project at: <a href='https://wandb.ai/adel-haj-jumah-hochschule-hannover/nano-transformer' target=\"_blank\">https://wandb.ai/adel-haj-jumah-hochschule-hannover/nano-transformer</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250424_213542-u4m97u75\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\Adel\\Desktop\\GenerativeAI-Project\\GenerativeAI-Project\\wandb\\run-20250424_213613-yk4jkqtp</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/adel-haj-jumah-hochschule-hannover/nano-transformer/runs/yk4jkqtp' target=\"_blank\">polished-water-16</a></strong> to <a href='https://wandb.ai/adel-haj-jumah-hochschule-hannover/nano-transformer' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/adel-haj-jumah-hochschule-hannover/nano-transformer' target=\"_blank\">https://wandb.ai/adel-haj-jumah-hochschule-hannover/nano-transformer</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/adel-haj-jumah-hochschule-hannover/nano-transformer/runs/yk4jkqtp' target=\"_blank\">https://wandb.ai/adel-haj-jumah-hochschule-hannover/nano-transformer/runs/yk4jkqtp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 4718/4718 [02:14<00:00, 35.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Training Loss = 3.9587\n",
      "Epoch 1: Evaluation Loss = 3.3660\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 4718/4718 [02:35<00:00, 30.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Training Loss = 3.0333\n",
      "Epoch 2: Evaluation Loss = 2.8101\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 4718/4718 [02:37<00:00, 29.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Training Loss = 2.7099\n",
      "Epoch 3: Evaluation Loss = 2.6113\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 4718/4718 [03:10<00:00, 24.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Training Loss = 2.5601\n",
      "Epoch 4: Evaluation Loss = 2.4975\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 4718/4718 [03:24<00:00, 23.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Training Loss = 2.4698\n",
      "Epoch 5: Evaluation Loss = 2.4229\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 4718/4718 [03:24<00:00, 23.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: Training Loss = 2.4080\n",
      "Epoch 6: Evaluation Loss = 2.3788\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 4718/4718 [03:24<00:00, 23.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: Training Loss = 2.3617\n",
      "Epoch 7: Evaluation Loss = 2.3388\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|██████████| 4718/4718 [02:45<00:00, 28.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: Training Loss = 2.3252\n",
      "Epoch 8: Evaluation Loss = 2.3018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 4718/4718 [02:40<00:00, 29.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: Training Loss = 2.2955\n",
      "Epoch 9: Evaluation Loss = 2.2779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|██████████| 4718/4718 [02:36<00:00, 30.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: Training Loss = 2.2699\n",
      "Epoch 10: Evaluation Loss = 2.2472\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "#  NanoTransformer mit PyTorch Decoder\n",
    "from torch.nn import TransformerDecoder, TransformerDecoderLayer\n",
    "\n",
    "class NanoTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, n_heads, d_ff, max_len, num_layers):\n",
    "        super().__init__()\n",
    "        self.embed = TokenAndPositionalEmbedding(vocab_size, d_model, max_len)\n",
    "\n",
    "        decoder_layer = TransformerDecoderLayer(d_model, n_heads, d_ff, batch_first=True)\n",
    "        self.decoder = TransformerDecoder(decoder_layer, num_layers)\n",
    "\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        self.output_proj = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embed(x)\n",
    "        T = x.size(1)\n",
    "\n",
    "        # Causal Mask erstellen\n",
    "        causal_mask = torch.triu(torch.ones(T, T), diagonal=1).bool().to(x.device)\n",
    "\n",
    "        # Dummy Memory\n",
    "        memory = torch.zeros(x.size(0), 1, x.size(2)).to(x.device)\n",
    "\n",
    "        x = self.decoder(tgt=x, memory=memory, tgt_mask=causal_mask)\n",
    "        x = self.norm(x)\n",
    "        logits = self.output_proj(x)\n",
    "        return logits\n",
    "\n",
    "# Training und Evaluierung\n",
    "def evaluate(model, dataloader, vocab_size, device):\n",
    "    model.eval()  # Wechsel in den Evaluationsmodus\n",
    "    total_loss = 0\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    with torch.no_grad():  # Kein Gradientenberechnung\n",
    "        for batch in dataloader:\n",
    "            inputs = batch[:, :-1].to(device)\n",
    "            targets = batch[:, 1:].to(device)\n",
    "\n",
    "            logits = model(inputs)\n",
    "            logits = logits.view(-1, vocab_size)\n",
    "            targets = targets.view(-1)\n",
    "\n",
    "            loss = loss_fn(logits, targets)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    return avg_loss\n",
    "\n",
    "def train(model, train_dataloader, eval_dataloader, vocab_size, device, epochs=10, lr=1e-4):\n",
    "    wandb.init(project=\"nano-transformer\", config={\n",
    "        \"epochs\": epochs,\n",
    "        \"lr\": lr,\n",
    "        \"batch_size\": train_dataloader.batch_size,\n",
    "        \"seq_len\": train_dataloader.dataset.seq_len,\n",
    "        \"vocab_size\": vocab_size,\n",
    "        \"dataset\": \"tiny_shakespeare\",\n",
    "        \"d_ff\": d_ff,\n",
    "        \"max_len\": max_len,\n",
    "        \"num_layers\": num_layers\n",
    "    })\n",
    "\n",
    "    model = model.to(device)\n",
    "    optimizer =  optim.Adam(model.parameters(), lr=lr)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for batch in tqdm(train_dataloader, desc=f\"Epoch {epoch+1}\"):\n",
    "            inputs = batch[:, :-1].to(device)\n",
    "            targets = batch[:, 1:].to(device)\n",
    "\n",
    "            logits = model(inputs)\n",
    "            logits = logits.view(-1, vocab_size)\n",
    "            targets = targets.view(-1)\n",
    "\n",
    "            loss = loss_fn(logits, targets)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = total_loss / len(train_dataloader)\n",
    "        print(f\"Epoch {epoch+1}: Training Loss = {avg_train_loss:.4f}\")\n",
    "\n",
    "        # Evaluierung nach jedem Trainingsepochendurchgang\n",
    "        avg_eval_loss = evaluate(model, eval_dataloader, vocab_size, device)\n",
    "        print(f\"Epoch {epoch+1}: Evaluation Loss = {avg_eval_loss:.4f}\")\n",
    "\n",
    "        # Logs an WandB senden\n",
    "        wandb.log({\n",
    "            \"epoch\": epoch+1,\n",
    "            \"train_loss\": avg_train_loss,\n",
    "            \"eval_loss\": avg_eval_loss\n",
    "        })\n",
    "\n",
    "    # Modell speichern nach Training\n",
    "    torch.save(model.state_dict(), \"nano_transformer_trained.pth\")\n",
    "\n",
    "# Dataset vorbereiten\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "dataset = load_dataset(\"tiny_shakespeare\", split=\"train\", trust_remote_code=True)\n",
    "text = dataset['text'][0]\n",
    "tokens = tokenizer.encode(text, truncation=False)\n",
    "tokens = torch.tensor(tokens)\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, tokens, seq_len):\n",
    "        self.tokens = tokens\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tokens) - self.seq_len\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.tokens[idx:idx+self.seq_len+1]\n",
    "\n",
    "seq_len = 64\n",
    "batch_size = 64\n",
    "dataset = TextDataset(tokens, seq_len)\n",
    "train_dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Erstellen eines Evaluationsdatensatzes (z.B. indem wir die ersten 20% des Datensatzes verwenden)\n",
    "eval_dataset = TextDataset(tokens[dataset.seq_len:], seq_len)  # Hier könnten auch speziellere Evaluierungsdaten genutzt werden\n",
    "eval_dataloader = DataLoader(eval_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(\"DataLoader bereit!\")\n",
    "\n",
    "# Modell konfigurieren\n",
    "vocab_size = tokenizer.vocab_size\n",
    "d_model = 32\n",
    "n_heads = 4\n",
    "d_ff = 512  \n",
    "max_len = 128\n",
    "num_layers = 2\n",
    "\n",
    "model = NanoTransformer(vocab_size, d_model, n_heads, d_ff, max_len, num_layers)\n",
    "\n",
    "# Training starten\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "train(model, train_dataloader, eval_dataloader, vocab_size, device, epochs=10, lr=3e-3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae18fabc",
   "metadata": {},
   "source": [
    "### Dropout hinzufügen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "67624b77",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (301966 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataLoader bereit!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\Adel\\Desktop\\GenerativeAI-Project\\GenerativeAI-Project\\wandb\\run-20250426_202716-x6lofhz5</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/adel-haj-jumah-hochschule-hannover/nano-transformer/runs/x6lofhz5' target=\"_blank\">upbeat-snow-17</a></strong> to <a href='https://wandb.ai/adel-haj-jumah-hochschule-hannover/nano-transformer' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/adel-haj-jumah-hochschule-hannover/nano-transformer' target=\"_blank\">https://wandb.ai/adel-haj-jumah-hochschule-hannover/nano-transformer</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/adel-haj-jumah-hochschule-hannover/nano-transformer/runs/x6lofhz5' target=\"_blank\">https://wandb.ai/adel-haj-jumah-hochschule-hannover/nano-transformer/runs/x6lofhz5</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 9435/9435 [02:16<00:00, 69.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Training Loss = 3.8361\n",
      "Epoch 1: Evaluation Loss = 3.2683\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 9435/9435 [02:11<00:00, 71.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Training Loss = 2.9342\n",
      "Epoch 2: Evaluation Loss = 2.7216\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 9435/9435 [02:12<00:00, 70.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Training Loss = 2.6330\n",
      "Epoch 3: Evaluation Loss = 2.5407\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 9435/9435 [02:12<00:00, 71.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Training Loss = 2.5005\n",
      "Epoch 4: Evaluation Loss = 2.4423\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 9435/9435 [02:13<00:00, 70.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Training Loss = 2.4212\n",
      "Epoch 5: Evaluation Loss = 2.3806\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 9435/9435 [02:12<00:00, 71.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: Training Loss = 2.3654\n",
      "Epoch 6: Evaluation Loss = 2.3293\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 9435/9435 [02:11<00:00, 71.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: Training Loss = 2.3239\n",
      "Epoch 7: Evaluation Loss = 2.2975\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|██████████| 9435/9435 [02:19<00:00, 67.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: Training Loss = 2.2906\n",
      "Epoch 8: Evaluation Loss = 2.2623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 9435/9435 [02:12<00:00, 71.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: Training Loss = 2.2639\n",
      "Epoch 9: Evaluation Loss = 2.2466\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|██████████| 9435/9435 [02:07<00:00, 73.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: Training Loss = 2.2412\n",
      "Epoch 10: Evaluation Loss = 2.2234\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "#  NanoTransformer mit PyTorch Decoder\n",
    "from torch.nn import TransformerDecoder, TransformerDecoderLayer\n",
    "\n",
    "class NanoTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, n_heads, d_ff, max_len, num_layers, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.embed = TokenAndPositionalEmbedding(vocab_size, d_model, max_len)\n",
    "\n",
    "        decoder_layer = TransformerDecoderLayer(d_model, n_heads, d_ff, batch_first=True)\n",
    "        self.decoder = TransformerDecoder(decoder_layer, num_layers)\n",
    "\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        self.output_proj = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embed(x)\n",
    "        T = x.size(1)\n",
    "\n",
    "        # Causal Mask erstellen\n",
    "        causal_mask = torch.triu(torch.ones(T, T), diagonal=1).bool().to(x.device)\n",
    "\n",
    "        # Dummy Memory\n",
    "        memory = torch.zeros(x.size(0), 1, x.size(2)).to(x.device)\n",
    "\n",
    "        x = self.decoder(tgt=x, memory=memory, tgt_mask=causal_mask)\n",
    "        x = self.norm(x)\n",
    "        logits = self.output_proj(x)\n",
    "        return logits\n",
    "\n",
    "# Training und Evaluierung\n",
    "def evaluate(model, dataloader, vocab_size, device):\n",
    "    model.eval()  # Wechsel in den Evaluationsmodus\n",
    "    total_loss = 0\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    with torch.no_grad():  # Kein Gradientenberechnung\n",
    "        for batch in dataloader:\n",
    "            inputs = batch[:, :-1].to(device)\n",
    "            targets = batch[:, 1:].to(device)\n",
    "\n",
    "            logits = model(inputs)\n",
    "            logits = logits.view(-1, vocab_size)\n",
    "            targets = targets.view(-1)\n",
    "\n",
    "            loss = loss_fn(logits, targets)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    return avg_loss\n",
    "\n",
    "def train(model, train_dataloader, eval_dataloader, vocab_size, device, epochs=10, lr=1e-3):\n",
    "    wandb.init(project=\"nano-transformer\", config={\n",
    "        \"epochs\": epochs,\n",
    "        \"lr\": lr,\n",
    "        \"batch_size\": train_dataloader.batch_size,\n",
    "        \"seq_len\": train_dataloader.dataset.seq_len,\n",
    "        \"vocab_size\": vocab_size,\n",
    "        \"dataset\": \"tiny_shakespeare\",\n",
    "        \"d_ff\": d_ff,\n",
    "        \"max_len\": max_len,\n",
    "        \"num_layers\": num_layers\n",
    "    })\n",
    "\n",
    "    model = model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for batch in tqdm(train_dataloader, desc=f\"Epoch {epoch+1}\"):\n",
    "            inputs = batch[:, :-1].to(device)\n",
    "            targets = batch[:, 1:].to(device)\n",
    "\n",
    "            logits = model(inputs)\n",
    "            logits = logits.view(-1, vocab_size)\n",
    "            targets = targets.view(-1)\n",
    "\n",
    "            loss = loss_fn(logits, targets)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = total_loss / len(train_dataloader)\n",
    "        print(f\"Epoch {epoch+1}: Training Loss = {avg_train_loss:.4f}\")\n",
    "\n",
    "        # Evaluierung nach jedem Trainingsepochendurchgang\n",
    "        avg_eval_loss = evaluate(model, eval_dataloader, vocab_size, device)\n",
    "        print(f\"Epoch {epoch+1}: Evaluation Loss = {avg_eval_loss:.4f}\")\n",
    "\n",
    "        # Logs an WandB senden\n",
    "        wandb.log({\n",
    "            \"epoch\": epoch+1,\n",
    "            \"train_loss\": avg_train_loss,\n",
    "            \"eval_loss\": avg_eval_loss\n",
    "        })\n",
    "\n",
    "    # Modell speichern nach Training\n",
    "    torch.save(model.state_dict(), \"nano_transformer_trained.pth\")\n",
    "\n",
    "# Dataset vorbereiten\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "dataset = load_dataset(\"tiny_shakespeare\", split=\"train\", trust_remote_code=True)\n",
    "text = dataset['text'][0]\n",
    "tokens = tokenizer.encode(text, truncation=False)\n",
    "tokens = torch.tensor(tokens)\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, tokens, seq_len):\n",
    "        self.tokens = tokens\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tokens) - self.seq_len\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.tokens[idx:idx+self.seq_len+1]\n",
    "\n",
    "seq_len = 64\n",
    "batch_size = 32\n",
    "dataset = TextDataset(tokens, seq_len)\n",
    "train_dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Erstellen eines Evaluationsdatensatzes (z.B. indem wir die ersten 20% des Datensatzes verwenden)\n",
    "eval_dataset = TextDataset(tokens[dataset.seq_len:], seq_len)  # Hier könnten auch speziellere Evaluierungsdaten genutzt werden\n",
    "eval_dataloader = DataLoader(eval_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(\"DataLoader bereit!\")\n",
    "\n",
    "# Modell konfigurieren\n",
    "vocab_size = tokenizer.vocab_size\n",
    "d_model = 32\n",
    "n_heads = 4\n",
    "d_ff = 512  \n",
    "max_len = 128\n",
    "num_layers = 2\n",
    "\n",
    "model = NanoTransformer(vocab_size, d_model, n_heads, d_ff, max_len, num_layers, dropout=0.2)\n",
    "\n",
    "# Training starten\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "train(model, train_dataloader, eval_dataloader, vocab_size, device, epochs=10, lr=3e-3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e83e692",
   "metadata": {},
   "source": [
    "### Text Generation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a3de684e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, start_token, max_len=50, temperature=1.0):\n",
    "    model.eval()\n",
    "    input_ids = start_token.to(device)\n",
    "\n",
    "    for _ in range(max_len):\n",
    "        logits = model(input_ids)\n",
    "        next_token_logits = logits[:, -1, :] / temperature\n",
    "        probs = torch.softmax(next_token_logits, dim=-1)\n",
    "        next_token = torch.multinomial(probs, num_samples=1)\n",
    "        input_ids = torch.cat([input_ids, next_token], dim=1)\n",
    "\n",
    "    return input_ids.squeeze().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c26e504",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_text = \"The future of AI\"\n",
    "input_ids = tokenizer.encode(start_text, return_tensors=\"pt\").to(device)\n",
    "print(\"Input IDs:\", input_ids.shape)\n",
    "\n",
    "output_ids = generate(model, input_ids, max_len=50, temperature=1.0)\n",
    "output_text = tokenizer.decode(output_ids)\n",
    "print(output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ab8563",
   "metadata": {},
   "source": [
    "# fertiges Modell statt mein eigenes Transformer-Modell\n",
    "1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0a358484",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "import wandb  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c10e862a",
   "metadata": {},
   "source": [
    "2. Tokenizer und Modell laden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9440470",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Modell und Tokenizer laden\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"sshleifer/tiny-gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"sshleifer/tiny-gpt2\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a7f115",
   "metadata": {},
   "source": [
    "3. Dataset vorbereiten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff32801",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tiny Shakespeare laden\n",
    "dataset = load_dataset(\"tiny_shakespeare\", split=\"train\")\n",
    "\n",
    "# Den Text extrahieren\n",
    "text = dataset['text'][0]  # Nur der Text\n",
    "\n",
    "# Tokenisieren\n",
    "tokens = tokenizer.encode(text, truncation=False)\n",
    "tokens = torch.tensor(tokens)\n",
    "\n",
    "# Dataset Klasse definieren\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, tokens, seq_len):\n",
    "        self.tokens = tokens\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tokens) - self.seq_len\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.tokens[idx:idx+self.seq_len+1]\n",
    "\n",
    "# Parameter\n",
    "seq_len = 64\n",
    "batch_size = 32\n",
    "\n",
    "# Dataset und DataLoader erstellen\n",
    "train_dataset = TextDataset(tokens, seq_len)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Für Evaluation einfach ein weiteres Dataset erstellen\n",
    "# Erstellen eines Evaluationsdatensatzes (z.B. indem wir die ersten 20% des Datensatzes verwenden)  \n",
    "eval_dataset = TextDataset(tokens, seq_len)\n",
    "eval_dataloader = DataLoader(eval_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(\"DataLoader bereit!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "297cca4b",
   "metadata": {},
   "source": [
    "4. neues train und evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3fa8cdaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, dataloader, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            inputs = batch[:, :-1].to(device)\n",
    "            targets = batch[:, 1:].to(device)\n",
    "\n",
    "            outputs = model(input_ids=inputs, labels=targets)\n",
    "            loss = outputs.loss\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    return avg_loss\n",
    "\n",
    "\n",
    "def train(model, train_dataloader, eval_dataloader, device, epochs=5, lr=1e-3):\n",
    "    wandb.init(mode=\"offline\", project=\"gpt2-finetune\", config={\n",
    "        \"epochs\": epochs,\n",
    "        \"lr\": lr,\n",
    "        \"batch_size\": train_dataloader.batch_size,\n",
    "        \"seq_len\": train_dataloader.dataset.seq_len,\n",
    "        \"vocab_size\": model.config.vocab_size,\n",
    "        \"dataset\": \"tiny_shakespeare\",\n",
    "        \"model_name\": \"sshleifer/tiny-gpt2\"\n",
    "    })\n",
    "\n",
    "    model = model.to(device)\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr)\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for batch in tqdm(train_dataloader, desc=f\"Epoch {epoch+1}\"):\n",
    "            inputs = batch[:, :-1].to(device)\n",
    "            targets = batch[:, 1:].to(device)\n",
    "\n",
    "            outputs = model(input_ids=inputs, labels=targets)\n",
    "            loss = outputs.loss\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = total_loss / len(train_dataloader)\n",
    "        print(f\"Epoch {epoch+1}: Training Loss = {avg_train_loss:.4f}\")\n",
    "\n",
    "        avg_eval_loss = evaluate(model, eval_dataloader, device)\n",
    "        print(f\"Epoch {epoch+1}: Evaluation Loss = {avg_eval_loss:.4f}\")\n",
    "\n",
    "        wandb.log({\n",
    "            \"epoch\": epoch+1,\n",
    "            \"train_loss\": avg_train_loss,\n",
    "            \"eval_loss\": avg_eval_loss\n",
    "        })\n",
    "\n",
    "    torch.save(model.state_dict(), \"fine_tuned_gpt2.pth\")\n",
    "    print(\"Modell gespeichert!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b6093e",
   "metadata": {},
   "source": [
    "5. Ganzes Setup zusammengefasst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d3bacaf6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "You can sync this run to the cloud by running:<br><code>wandb sync c:\\Users\\Adel\\Desktop\\GenerativeAI-Project\\GenerativeAI-Project\\wandb\\offline-run-20250427_124544-d155uxyv<code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\offline-run-20250427_124544-d155uxyv\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "W&B syncing is set to <code>`offline`<code> in this directory.  <br>Run <code>`wandb online`<code> or set <code>WANDB_MODE=online<code> to enable cloud syncing."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 9435/9435 [05:10<00:00, 30.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Training Loss = 6.3228\n",
      "Epoch 1: Evaluation Loss = 6.3212\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 9435/9435 [05:38<00:00, 27.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Training Loss = 6.3217\n",
      "Epoch 2: Evaluation Loss = 6.3212\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 9435/9435 [06:30<00:00, 24.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Training Loss = 6.3215\n",
      "Epoch 3: Evaluation Loss = 6.3213\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 9435/9435 [04:31<00:00, 34.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Training Loss = 6.3214\n",
      "Epoch 4: Evaluation Loss = 6.3209\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 9435/9435 [03:34<00:00, 44.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Training Loss = 6.3213\n",
      "Epoch 5: Evaluation Loss = 6.3208\n",
      "Modell gespeichert!\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▃▅▆█</td></tr><tr><td>eval_loss</td><td>▇▆█▃▁</td></tr><tr><td>train_loss</td><td>█▃▂▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>5</td></tr><tr><td>eval_loss</td><td>6.32083</td></tr><tr><td>train_loss</td><td>6.32134</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "You can sync this run to the cloud by running:<br><code>wandb sync c:\\Users\\Adel\\Desktop\\GenerativeAI-Project\\GenerativeAI-Project\\wandb\\offline-run-20250427_124628-sk7gugsu<code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\offline-run-20250427_124628-sk7gugsu\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "train(\n",
    "    model, \n",
    "    train_dataloader, \n",
    "    eval_dataloader, \n",
    "    device, \n",
    "    epochs=5, \n",
    "    lr=4e-3  # ein bisschen niedrigerer Lernrate für finetuning\n",
    ")\n",
    "wandb.finish()  # Beende die WandB-Sitzung\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b8d092a",
   "metadata": {},
   "source": [
    "6. Text Generation nach dem Finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "57e66b79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time,\n",
      ":\n",
      ".\n",
      "\n",
      ";\n",
      ":\n",
      ",\n",
      "\n",
      "\n",
      "\n",
      "\n",
      ";:\n",
      " the:\n",
      ",\n",
      "\n",
      "\n",
      "\n",
      " to\n",
      "\n",
      "\n",
      ",\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " the\n",
      ";.\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Text-Input\n",
    "prompt = \"Once upon a time\"\n",
    "\n",
    "# Tokenisieren\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)  # <-- GANZ WICHTIG!!\n",
    "\n",
    "# Text generieren\n",
    "outputs = model.generate(\n",
    "    inputs[\"input_ids\"],\n",
    "    attention_mask=inputs[\"attention_mask\"],\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    "    max_length=50,\n",
    "    temperature=0.7,     # Etwas weniger chaotisch\n",
    "    top_k=20,            # Nur Top-20 Token zur Auswahl\n",
    "    top_p=0.9,           # Weniger random\n",
    "    do_sample=True,\n",
    "    num_return_sequences=1\n",
    ")\n",
    "\n",
    "\n",
    "# Ausgabe decodieren\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7ab78e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, AutoTokenizer\n",
    "\n",
    "# Statt:\n",
    "# model = GPT2LMHeadModel.from_pretrained(\"sshleifer/tiny-gpt2\")\n",
    "# Nutze das echte GPT2:\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "33c62e4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time, the first person to enter a room with a weapon or weapon of some kind, was an individual who had a great deal of experience in combat, or was a member of a guild, or a member of a guild's guild\n"
     ]
    }
   ],
   "source": [
    "# Text-Input\n",
    "prompt = \"Once upon a time\"\n",
    "\n",
    "# Tokenisieren\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)  # <-- GANZ WICHTIG!!\n",
    "\n",
    "# Text generieren\n",
    "outputs = model.generate(\n",
    "    inputs[\"input_ids\"],\n",
    "    attention_mask=inputs[\"attention_mask\"],\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    "    max_length=50,\n",
    "    temperature=0.7,     # Etwas weniger chaotisch\n",
    "    top_k=20,            # Nur Top-20 Token zur Auswahl\n",
    "    top_p=0.9,           # Weniger random\n",
    "    do_sample=True,\n",
    "    num_return_sequences=1\n",
    ")\n",
    "\n",
    "\n",
    "# Ausgabe decodieren\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
