{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c027491",
   "metadata": {},
   "source": [
    "# GenerativeAI \"Sprachmodell\" Projekt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb4417d7",
   "metadata": {},
   "source": [
    "## Verbing mit wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e5700284",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33madel-haj-jumah\u001b[0m (\u001b[33madel-haj-jumah-hochschule-hannover\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe187fce",
   "metadata": {},
   "source": [
    "1. Importieren die notwendigen Bibliotheken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149a539f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from pathlib import Path\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe1cf68",
   "metadata": {},
   "source": [
    "2. Token And Positional Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed6d9690",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TokenAndPositionalEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, max_len):\n",
    "        super().__init__()\n",
    "        # ID der Token in einen Vektorraum\n",
    "        self.token_embed = nn.Embedding(vocab_size, d_model)\n",
    "        # Positionale Einbettungen (lernen relative Positionen \"das ist learnable\")\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, 512, d_model)) # batch 1 , bis zu 512 token und vektor größe\n",
    "    def forward(self, x):\n",
    "        seq_len = x.size(1) # 1 ist Anzahl der Token\"Sequenzlänge\" (0 ist batch)\n",
    "        token_emb = self.token_embed(x)\n",
    "        pos_emb = self.pos_embedding[:, :seq_len, :]\n",
    "        return token_emb + pos_emb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d7e418",
   "metadata": {},
   "source": [
    "3. Masked Multi-Head Self-Attention\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ed2db8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MaskedSelfAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Masked (Causal) Self-Attention Layer\n",
    "    → Modell kann nur auf vergangene Tokens schauen\n",
    "    → Verwendet PyTorch nn.MultiheadAttention\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, n_heads):\n",
    "        super().__init__()\n",
    "        self.attn = nn.MultiheadAttention(\n",
    "            embed_dim=d_model, \n",
    "            num_heads=n_heads, \n",
    "            batch_first=True  # wichtig! Damit x.shape = (B, T, C) funktioniert\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        T = x.size(1)  # Sequence Length\n",
    "\n",
    "        # Causal Mask (obere Dreiecksmatrix)\n",
    "        mask = torch.triu(torch.ones(T, T), diagonal=1).bool().to(x.device)\n",
    "\n",
    "        # MultiheadAttention erwartet: (query, key, value, attn_mask)\n",
    "        out, _ = self.attn(x, x, x, attn_mask=mask)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7abb1c6",
   "metadata": {},
   "source": [
    "4. Add & Norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9cf72fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddNorm(nn.Module):\n",
    "    def __init__(self, d_model):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x, sublayer_output):\n",
    "        return self.norm(x + sublayer_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a2036d",
   "metadata": {},
   "source": [
    "5. Feedforward Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ebdfb3b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_ff, d_model)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db87d56",
   "metadata": {},
   "source": [
    "6. Alles zusammenfügen: Transformator-Decoderblock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "39bb6180",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, d_ff):\n",
    "        super().__init__()\n",
    "        self.attn = MaskedSelfAttention(d_model, n_heads)  # PyTorch MultiheadAttention inside\n",
    "        self.addnorm1 = AddNorm(d_model)\n",
    "        self.ff = FeedForward(d_model, d_ff)\n",
    "        self.addnorm2 = AddNorm(d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.addnorm1(x, self.attn(x))  # Attention + Residual + LayerNorm\n",
    "        x = self.addnorm2(x, self.ff(x))    # FeedForward + Residual + LayerNorm\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b91e5a",
   "metadata": {},
   "source": [
    "7. Zusammenbau des NanoTransformers (nur Decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "977967f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class NanoTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, n_heads, d_ff, max_len, num_layers):\n",
    "        super().__init__()\n",
    "        self.embed = TokenAndPositionalEmbedding(vocab_size, d_model, max_len)\n",
    "        self.blocks = nn.ModuleList([\n",
    "            TransformerBlock(d_model, n_heads, d_ff) for _ in range(num_layers)\n",
    "        ])\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        self.output_proj = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embed(x)\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        x = self.norm(x)\n",
    "        logits = self.output_proj(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e17f607c",
   "metadata": {},
   "source": [
    "8. Trainieren des Modells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4704706d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def train(model, dataloader, vocab_size, device, epochs=50, lr=1e-4):\n",
    "    # Wandb initialisieren, nur einmal zu Beginn des Trainings\n",
    "    wandb.init(project=\"nano-transformer\", config={\n",
    "        \"epochs\": epochs,\n",
    "        \"lr\": lr,\n",
    "        \"batch_size\": dataloader.batch_size,\n",
    "        \"seq_len\": dataloader.dataset.seq_len,\n",
    "        \"vocab_size\": vocab_size\n",
    "    })\n",
    "    \n",
    "    # Modell auf das richtige Gerät (GPU oder CPU) verschieben\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Optimizer und Loss-Funktion\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    model.train()  # Modell in Trainingsmodus versetzen\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for batch in tqdm(dataloader, desc=\"Training Progress\"):\n",
    "            # Eingabe- und Ziel-Tensoren\n",
    "            inputs = batch[:, :-1].to(device)\n",
    "            targets = batch[:, 1:].to(device)\n",
    "\n",
    "            # Vorwärtsdurchlauf\n",
    "            logits = model(inputs)\n",
    "            logits = logits.view(-1, vocab_size)  # Umformen für CrossEntropyLoss\n",
    "            targets = targets.view(-1)  # Umformen für CrossEntropyLoss\n",
    "\n",
    "            # Verlustberechnung\n",
    "            loss = loss_fn(logits, targets)\n",
    "\n",
    "            # Backpropagation und Optimierung\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Gesamten Verlust summieren\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        # Durchschnittlichen Verlust für die Epoche berechnen\n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        print(f\"Epoch {epoch+1}: Loss = {avg_loss:.4f}\")\n",
    "\n",
    "        # Verlust an Wandb senden\n",
    "        wandb.log({\"epoch\": epoch+1, \"loss\": avg_loss})\n",
    "    \n",
    "    # Modell speichern nach Training (optional)\n",
    "    torch.save(model.state_dict(), \"nano_transformer_trained.pth\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4700336",
   "metadata": {},
   "source": [
    "9. Dataset Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4293020e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (301966 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataLoader bereit!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "\n",
    "# 1. Tokenizer laden\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# 2. Tiny Shakespeare Dataset laden\n",
    "dataset = load_dataset(\"tiny_shakespeare\", split=\"train\", trust_remote_code=True)\n",
    "text = dataset['text'][0]\n",
    "\n",
    "# 3. Text in Token IDs umwandeln\n",
    "tokens = tokenizer.encode(text, truncation=False)\n",
    "tokens = torch.tensor(tokens)\n",
    "\n",
    "# 4. Dataset Klasse\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, tokens, seq_len):\n",
    "        self.tokens = tokens\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tokens) - self.seq_len\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.tokens[idx:idx+self.seq_len+1]\n",
    "\n",
    "# 5. DataLoader bauen\n",
    "seq_len = 64\n",
    "batch_size = 32\n",
    "dataset = TextDataset(tokens, seq_len)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "print(\"DataLoader bereit!\")\n",
    "\n",
    "# 6. Dein Modell (Mini NanoTransformer)\n",
    "vocab_size = tokenizer.vocab_size\n",
    "d_model = 32\n",
    "n_heads = 1\n",
    "d_ff = 256\n",
    "max_len = 64\n",
    "num_layers = 1\n",
    "\n",
    "model = NanoTransformer(vocab_size, d_model, n_heads, d_ff, max_len, num_layers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e654952a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\Adel\\Desktop\\GenerativeAI-Project\\GenerativeAI-Project\\wandb\\run-20250423_214033-ya02qpfu</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/adel-haj-jumah-hochschule-hannover/nano-transformer/runs/ya02qpfu' target=\"_blank\">polar-water-1</a></strong> to <a href='https://wandb.ai/adel-haj-jumah-hochschule-hannover/nano-transformer' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/adel-haj-jumah-hochschule-hannover/nano-transformer' target=\"_blank\">https://wandb.ai/adel-haj-jumah-hochschule-hannover/nano-transformer</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/adel-haj-jumah-hochschule-hannover/nano-transformer/runs/ya02qpfu' target=\"_blank\">https://wandb.ai/adel-haj-jumah-hochschule-hannover/nano-transformer/runs/ya02qpfu</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 100%|██████████| 9435/9435 [02:02<00:00, 76.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss = 4.9525\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  70%|██████▉   | 6566/9435 [01:25<00:37, 76.53it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m device = torch.device(\u001b[33m\"\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch.cuda.is_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvocab_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m3e-4\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 44\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(model, dataloader, vocab_size, device, epochs, lr)\u001b[39m\n\u001b[32m     41\u001b[39m     optimizer.step()\n\u001b[32m     43\u001b[39m     \u001b[38;5;66;03m# Gesamten Verlust summieren\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m44\u001b[39m     total_loss += \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     46\u001b[39m \u001b[38;5;66;03m# Durchschnittlichen Verlust für die Epoche berechnen\u001b[39;00m\n\u001b[32m     47\u001b[39m avg_loss = total_loss / \u001b[38;5;28mlen\u001b[39m(dataloader)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "train(model, dataloader, vocab_size, device, epochs=100, lr=3e-4)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
