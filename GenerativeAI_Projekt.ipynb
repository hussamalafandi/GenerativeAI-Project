{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c027491",
   "metadata": {},
   "source": [
    "# GenerativeAI \"Sprachmodell\" Projekt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb4417d7",
   "metadata": {},
   "source": [
    "## Verbing mit wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e5700284",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33madel-haj-jumah\u001b[0m (\u001b[33madel-haj-jumah-hochschule-hannover\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "017a1dc1",
   "metadata": {},
   "source": [
    "1. import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "63c66021",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"wikitext\", \"wikitext-103-v1\", split=\"train[:5%]\")\n",
    "text = \"\\n\".join(ds['text'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe187fce",
   "metadata": {},
   "source": [
    "2. Importieren die notwendigen Bibliotheken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "149a539f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer\n",
    "import wandb\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from pathlib import Path\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe1cf68",
   "metadata": {},
   "source": [
    "3. Token And Positional Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed6d9690",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TokenAndPositionalEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, max_len):\n",
    "        super().__init__()\n",
    "        # ID der Token in einen Vektorraum\n",
    "        self.token_embed = nn.Embedding(vocab_size, d_model)\n",
    "        # Positionale Einbettungen (lernen relative Positionen \"das ist learnable\")\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, 512, d_model)) # batch 1 , bis zu 512 token und vektor größe\n",
    "    def forward(self, x):\n",
    "        seq_len = x.size(1) # 1 ist Anzahl der Token\"Sequenzlänge\" (0 ist batch)\n",
    "        token_emb = self.token_embed(x)\n",
    "        pos_emb = self.pos_embedding[:, :seq_len, :]\n",
    "        return token_emb + pos_emb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d7e418",
   "metadata": {},
   "source": [
    "4. Masked Multi-Head Self-Attention\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed2db8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MaskedSelfAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Masked (Causal) Self-Attention Layer\n",
    "    → Modell kann nur auf vergangene Tokens schauen\n",
    "    → Verwendet PyTorch nn.MultiheadAttention\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, n_heads):\n",
    "        super().__init__()\n",
    "        self.attn = nn.MultiheadAttention(\n",
    "            embed_dim=d_model, \n",
    "            num_heads=n_heads, \n",
    "            batch_first=True  # wichtig! Damit x.shape = (B, T, C) funktioniert\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        T = x.size(1)  # Sequence Length\n",
    "\n",
    "        # Causal Mask (obere Dreiecksmatrix)\n",
    "        mask = torch.triu(torch.ones(T, T), diagonal=1).bool().to(x.device)\n",
    "\n",
    "        # MultiheadAttention erwartet: (query, key, value, attn_mask)\n",
    "        out, _ = self.attn(x, x, x, attn_mask=mask)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7abb1c6",
   "metadata": {},
   "source": [
    "5. Add & Norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9cf72fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddNorm(nn.Module):\n",
    "    def __init__(self, d_model):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x, sublayer_output):\n",
    "        return self.norm(x + sublayer_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a2036d",
   "metadata": {},
   "source": [
    "6. Feedforward Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ebdfb3b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_ff, d_model)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db87d56",
   "metadata": {},
   "source": [
    "7. Alles zusammenfügen: Transformator-Decoderblock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "39bb6180",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, d_ff):\n",
    "        super().__init__()\n",
    "        self.attn = MaskedSelfAttention(d_model, n_heads)  # PyTorch MultiheadAttention inside\n",
    "        self.addnorm1 = AddNorm(d_model)\n",
    "        self.ff = FeedForward(d_model, d_ff)\n",
    "        self.addnorm2 = AddNorm(d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.addnorm1(x, self.attn(x))  # Attention + Residual + LayerNorm\n",
    "        x = self.addnorm2(x, self.ff(x))    # FeedForward + Residual + LayerNorm\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b91e5a",
   "metadata": {},
   "source": [
    "8. Zusammenbau des NanoTransformers (nur Decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "977967f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class NanoTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, n_heads, d_ff, max_len, num_layers):\n",
    "        super().__init__()\n",
    "        self.embed = TokenAndPositionalEmbedding(vocab_size, d_model, max_len)\n",
    "        self.blocks = nn.ModuleList([\n",
    "            TransformerBlock(d_model, n_heads, d_ff) for _ in range(num_layers)\n",
    "        ])\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        self.output_proj = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embed(x)\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        x = self.norm(x)\n",
    "        logits = self.output_proj(x)\n",
    "        return logits"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
