{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NataHsH/GenerativeAI-Project/blob/master/LanguageModel_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IVXWf9frZf_u"
      },
      "outputs": [],
      "source": [
        "# Install required libraries\n",
        "# ==========================\n",
        "!pip install transformers datasets wandb accelerate\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install huggingface_hub\n"
      ],
      "metadata": {
        "id": "q4U9AYXy7SF2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Login to Hugging Face and Weights & Biases\n",
        "# ==========================\n",
        "from huggingface_hub import notebook_login\n",
        "import wandb\n",
        "\n",
        "# Login to Hugging Face\n",
        "notebook_login()\n",
        "\n",
        "# Login to Weights & Biases\n",
        "wandb.login()"
      ],
      "metadata": {
        "id": "KH8hXJEKZxwC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "# ==========================\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import PreTrainedTokenizerFast\n",
        "from datasets import load_dataset\n",
        "import wandb\n",
        "from huggingface_hub import HfApi, create_repo, upload_folder\n"
      ],
      "metadata": {
        "id": "nVVUkBrUZ2ZN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define hyperparameters\n",
        "# ==========================\n",
        "batch_size = 64\n",
        "block_size = 128\n",
        "embedding_dim = 256\n",
        "num_heads = 4\n",
        "num_layers = 4\n",
        "dropout = 0.1\n",
        "learning_rate = 3e-4\n",
        "num_epochs = 3\n",
        "\n",
        "model_name = \"tiny_shakespeare_transformer\"\n",
        "repository_id = \"NataliaH/tiny_shakespeare_transformer\""
      ],
      "metadata": {
        "id": "JCbnI3VIaC-S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load dataset\n",
        "# ==========================\n",
        "dataset = load_dataset(\"tiny_shakespeare\", split=\"train\")\n",
        "text = dataset['text']"
      ],
      "metadata": {
        "id": "B1vi_2OoauDp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenizer\n",
        "# ==========================\n",
        "# Create and train a simple character-level tokenizer\n",
        "tokenizer = PreTrainedTokenizerFast.from_pretrained(\"gpt2\")\n",
        "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
        "\n",
        "# Encode the dataset\n",
        "inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=block_size)\n",
        "input_ids = inputs[\"input_ids\"]\n"
      ],
      "metadata": {
        "id": "bdsiAdPGZ-QS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare the dataset for training\n",
        "# ==========================\n",
        "class ShakespeareDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, input_ids, block_size):\n",
        "        self.input_ids = input_ids\n",
        "        self.block_size = block_size\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.input_ids.size(1) // self.block_size\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        start_idx = idx * self.block_size\n",
        "        end_idx = start_idx + self.block_size\n",
        "        x = self.input_ids[:, start_idx:end_idx]\n",
        "        y = self.input_ids[:, start_idx+1:end_idx+1]\n",
        "        return x.squeeze(0), y.squeeze(0)\n",
        "\n",
        "dataset = ShakespeareDataset(input_ids, block_size)\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
      ],
      "metadata": {
        "id": "digXF1Yea61-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare the dataset\n",
        "def group_texts(tokenized_text, block_size):\n",
        "    total_length = len(tokenized_text)\n",
        "    total_length = (total_length // block_size) * block_size  # truncate to a multiple of block_size\n",
        "    input_ids = tokenized_text[:total_length]\n",
        "\n",
        "    input_ids = input_ids.view(-1, block_size)\n",
        "    target_ids = input_ids.clone()\n",
        "    return input_ids, target_ids\n",
        "\n",
        "# Tokenize the entire dataset\n",
        "tokenized_text = tokenizer(text, return_tensors='pt', add_special_tokens=False).input_ids.squeeze(0)\n",
        "\n",
        "# Group into blocks\n",
        "block_size = 128\n",
        "x, y = group_texts(tokenized_text, block_size)\n"
      ],
      "metadata": {
        "id": "CsuM5lLZcV8R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "import torch\n",
        "import json\n",
        "\n",
        "# Load the model state dict\n",
        "model_save_path = \"./tiny_shakespeare_transformer\"\n",
        "tokenizer_save_path = model_save_path\n",
        "\n",
        "# Define the model architecture\n",
        "class TransformerModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, num_heads, num_layers, block_size, dropout):\n",
        "        super().__init__()\n",
        "        self.token_embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        decoder_layer = nn.TransformerDecoderLayer(\n",
        "            d_model=embedding_dim,\n",
        "            nhead=num_heads,\n",
        "            dim_feedforward=embedding_dim * 4,\n",
        "            dropout=dropout,\n",
        "            activation='gelu'\n",
        "        )\n",
        "        self.transformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_layers)\n",
        "        self.fc_out = nn.Linear(embedding_dim, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        embedded = self.token_embedding(x)\n",
        "        memory = torch.zeros_like(embedded)\n",
        "        out = self.transformer_decoder(embedded, memory)\n",
        "        logits = self.fc_out(out)\n",
        "        return logits\n",
        "\n",
        "# Load the model state dict\n",
        "model = TransformerModel(\n",
        "    vocab_size=5000,  # Example vocab size, adjust based on your model\n",
        "    embedding_dim=256,  # Example, adjust as needed\n",
        "    num_heads=8,  # Example\n",
        "    num_layers=6,  # Example\n",
        "    block_size=128,  # Example\n",
        "    dropout=0.1  # Example\n",
        ")\n",
        "\n",
        "# Load model weights and move to the correct device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.load_state_dict(torch.load(f\"{model_save_path}/model.pth\"))\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "# Load tokenizer\n",
        "with open(f\"{tokenizer_save_path}/tokenizer.json\", \"r\") as f:\n",
        "    vocab = json.load(f)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")  # Or adjust to your model type\n",
        "tokenizer.add_tokens(list(vocab.keys()))  # Adding custom vocab\n",
        "\n",
        "# Ensure tokenizer vocab size is not larger than model vocab size\n",
        "tokenizer_vocab_size = len(tokenizer)\n",
        "model_vocab_size = model.token_embedding.num_embeddings\n",
        "if tokenizer_vocab_size > model_vocab_size:\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"gpt2\", use_fast=False)\n",
        "    tokenizer.add_tokens(list(vocab.keys())[:model_vocab_size])\n",
        "\n",
        "# Add pad token if necessary\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n"
      ],
      "metadata": {
        "id": "p5gcU7bUcSzd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define optimizer and loss function\n",
        "# ==========================\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Initialize wandb\n",
        "wandb.init(project=\"LanguageModel_Project\", name=\"tiny_shakespeare_transformer\")\n"
      ],
      "metadata": {
        "id": "dDSTPRXYa6hX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "x_train, x_val, y_train, y_val = train_test_split(x, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Move the data to the same device as the model\n",
        "x_train = x_train.to(device)\n",
        "y_train = y_train.to(device)\n",
        "x_val = x_val.to(device)\n",
        "y_val = y_val.to(device)\n",
        "\n",
        "# Training loop\n",
        "# ==========================\n",
        "num_epochs = 5\n",
        "model.train()\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    start_time = time.time()  # Track the time taken for the epoch\n",
        "    total_train_loss = 0\n",
        "\n",
        "    # Loop through the dataset manually in batches\n",
        "    for i in range(0, x_train.size(0), batch_size):\n",
        "        xb = x_train[i:i+batch_size].to(device)\n",
        "        yb = y_train[i:i+batch_size].to(device)\n",
        "\n",
        "        # Skip the last batch if it's smaller than batch_size\n",
        "        if xb.size(0) != batch_size or yb.size(0) != batch_size:\n",
        "            continue\n",
        "\n",
        "        optimizer.zero_grad()               # Reset gradients\n",
        "        logits = model(xb)                   # Forward pass\n",
        "        loss = criterion(logits.view(-1, logits.size(-1)), yb.view(-1))  # Compute loss\n",
        "        loss.backward()                      # Backward pass\n",
        "        optimizer.step()                     # Update weights\n",
        "\n",
        "        total_train_loss += loss.item()\n",
        "\n",
        "    # Validation step\n",
        "    model.eval()  # Switch the model to evaluation mode\n",
        "    total_val_loss = 0\n",
        "    with torch.no_grad():  # No need to compute gradients during validation\n",
        "        for i in range(0, x_val.size(0), batch_size):\n",
        "            xb_val = x_val[i:i+batch_size].to(device)\n",
        "            yb_val = y_val[i:i+batch_size].to(device)\n",
        "\n",
        "            # Skip the last batch if it's smaller than batch_size\n",
        "            if xb_val.size(0) != batch_size or yb_val.size(0) != batch_size:\n",
        "                continue\n",
        "\n",
        "            logits_val = model(xb_val)  # Forward pass\n",
        "            val_loss = criterion(logits_val.view(-1, logits_val.size(-1)), yb_val.view(-1))  # Compute loss\n",
        "            total_val_loss += val_loss.item()\n",
        "\n",
        "    # Compute average losses for the epoch\n",
        "    num_batches_train = x_train.size(0) // batch_size\n",
        "    avg_train_loss = total_train_loss / num_batches_train if num_batches_train > 0 else 0\n",
        "\n",
        "    num_batches_val = x_val.size(0) // batch_size\n",
        "    avg_val_loss = total_val_loss / num_batches_val if num_batches_val > 0 else 0\n",
        "\n",
        "    # Get the current learning rate\n",
        "    current_lr = optimizer.param_groups[0]['lr']\n",
        "\n",
        "    # Calculate the time taken for the epoch\n",
        "    epoch_time = time.time() - start_time\n",
        "\n",
        "    # Log all metrics\n",
        "    wandb.log({\n",
        "        \"train_loss\": avg_train_loss,\n",
        "        \"val_loss\": avg_val_loss,\n",
        "        \"learning_rate\": current_lr,\n",
        "        \"epoch_time\": epoch_time\n",
        "    }, step=epoch)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}, LR: {current_lr:.6f}, Time: {epoch_time:.2f}s\")\n"
      ],
      "metadata": {
        "id": "LoJb_aneeB0_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import json\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Define the save path\n",
        "model_save_path = \"./tiny_shakespeare_transformer\"\n",
        "tokenizer_save_path = model_save_path\n",
        "\n",
        "# Create directories if they don't exist\n",
        "os.makedirs(model_save_path, exist_ok=True)\n",
        "\n",
        "# Save model state dict\n",
        "torch.save(model.state_dict(), f\"{model_save_path}/model.pth\")\n",
        "\n",
        "# Save tokenizer (updated to handle custom tokenizer correctly)\n",
        "# Assuming you already added custom tokens and made necessary adjustments to the tokenizer\n",
        "tokenizer.save_pretrained(tokenizer_save_path)\n",
        "\n",
        "# If you want to save the vocabulary separately, you can also do it\n",
        "with open(f\"{tokenizer_save_path}/tokenizer.json\", \"w\") as f:\n",
        "    json.dump(tokenizer.get_vocab(), f)\n"
      ],
      "metadata": {
        "id": "3lV_7C33gH6t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Push model to Hugging Face Hub\n",
        "# ==========================\n",
        "api = HfApi()\n",
        "create_repo(repo_id=repository_id, exist_ok=True)\n",
        "upload_folder(folder_path=model_save_path, repo_id=repository_id)"
      ],
      "metadata": {
        "id": "PamsXQMabaxr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import textwrap\n",
        "\n",
        "# Define the path to save model and README\n",
        "model_save_path = \"./tiny_shakespeare_transformer\"\n",
        "\n",
        "# Create Model Card\n",
        "# ==========================\n",
        "model_card = textwrap.dedent(f\"\"\"\\\n",
        "    ---\n",
        "    license: mit\n",
        "    tags:\n",
        "    - text-generation\n",
        "    - transformer\n",
        "    - tiny-shakespeare\n",
        "    - decoder-only\n",
        "    model-index:\n",
        "    - name: tiny_shakespeare_transformer\n",
        "      results: []\n",
        "    ---\n",
        "\n",
        "    # tiny_shakespeare_transformer\n",
        "\n",
        "    A small Transformer Decoder model trained from scratch on the Tiny Shakespeare dataset.\n",
        "\n",
        "    ## Training details\n",
        "    - Dataset: Tiny Shakespeare\n",
        "    - Epochs: {num_epochs}\n",
        "    - Learning Rate: {learning_rate}\n",
        "    - Batch Size: {batch_size}\n",
        "    - Block Size: {block_size}\n",
        "    - Optimizer: AdamW\n",
        "    - Loss Function: CrossEntropyLoss\n",
        "    - Dropout Rate: {dropout}\n",
        "    - Embedding Dimension: {embedding_dim}\n",
        "    - Number of Layers: {num_layers}\n",
        "    - Number of Attention Heads: {num_heads}\n",
        "\n",
        "    ## Usage\n",
        "    To use this model, simply load it using the following code:\n",
        "\n",
        "    ```python\n",
        "    from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "    # Load the model and tokenizer\n",
        "    model = AutoModelForCausalLM.from_pretrained(\"{repository_id}\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"{repository_id}\")\n",
        "\n",
        "    # Encode input text\n",
        "    inputs = tokenizer(\"Once upon a time\", return_tensors=\"pt\")\n",
        "    outputs = model.generate(**inputs)\n",
        "    print(tokenizer.decode(outputs[0]))\n",
        "    ```\n",
        "\n",
        "    ## Model Architecture\n",
        "    This model is a Transformer Decoder-based architecture, optimized for text generation.\n",
        "    It was trained on the Tiny Shakespeare dataset to generate Shakespeare-like text.\n",
        "\n",
        "    ## Training Process\n",
        "    - Training was performed for {num_epochs} epochs.\n",
        "    - The model uses AdamW optimizer with a learning rate of {learning_rate}.\n",
        "    - Dropout rate during training was set to {dropout} to reduce overfitting.\n",
        "\n",
        "    ## License\n",
        "    This model is released under the MIT License.\n",
        "\"\"\")\n",
        "\n",
        "# Save the model card to README.md\n",
        "os.makedirs(model_save_path, exist_ok=True)\n",
        "\n",
        "with open(f\"{model_save_path}/README.md\", \"w\") as f:\n",
        "    f.write(model_card)\n",
        "\n",
        "# Print the contents of the README to verify\n",
        "!cat {model_save_path}/README.md\n"
      ],
      "metadata": {
        "id": "Ge4lIRt7-DRZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import upload_folder\n",
        "\n",
        "# Параметры\n",
        "repo_id = \"NataliaH/tiny_shakespeare_transformer\"  # Название репозитория, который уже существует\n",
        "model_save_path = \"./tiny_shakespeare_transformer\"\n",
        "\n",
        "# Загрузить модель в репозиторий\n",
        "upload_folder(\n",
        "    folder_path=model_save_path,\n",
        "    repo_id=repo_id,\n",
        "    commit_message=\"Initial commit with model and tokenizer\",\n",
        ")\n"
      ],
      "metadata": {
        "id": "VTuoBKWWoaUB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Function for text generation manually\n",
        "def generate_text_manually(model, tokenizer, prompt, max_length=300):\n",
        "    # Tokenize the input prompt\n",
        "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n",
        "\n",
        "    generated_ids = input_ids\n",
        "\n",
        "    # Text generation loop\n",
        "    for _ in range(max_length):\n",
        "        # Pass the generated ids through the model\n",
        "        output = model(generated_ids)  # Model output is a tensor of logits\n",
        "        logits = output[:, -1, :]  # Only take the last token's logits (for the last token in the sequence)\n",
        "\n",
        "        # Get the most probable next token\n",
        "        next_token_id = torch.argmax(logits, dim=-1).unsqueeze(-1)  # Get the token with the highest probability\n",
        "\n",
        "        # Append the new token to the generated tokens\n",
        "        generated_ids = torch.cat((generated_ids, next_token_id), dim=-1)\n",
        "\n",
        "        # Stop if the end of sequence token is generated\n",
        "        if next_token_id.item() == tokenizer.eos_token_id:\n",
        "            break\n",
        "\n",
        "    # Decode the generated tokens back to text, skip special tokens and clean the output\n",
        "    generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
        "    generated_text = generated_text.replace('Ġ', ' ')  # Replace Ġ with spaces\n",
        "    return generated_text\n",
        "\n",
        "# Example usage\n",
        "prompt = \"Once upon a time\"\n",
        "generated_text = generate_text_manually(model, tokenizer, prompt, max_length=300)\n",
        "\n",
        "\n",
        "print(generated_text)\n"
      ],
      "metadata": {
        "id": "zk2pEZYqkljZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}