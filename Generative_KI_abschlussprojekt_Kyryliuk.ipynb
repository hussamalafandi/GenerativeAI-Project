{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "465400cc",
   "metadata": {},
   "source": [
    "eine TransformerEncoderLayer-basiete Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {
    "id": "26271595"
   },
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "#eine TransformerEncoderLayer-basiete Model\n",
    "class GPTDecoderOnlyModel(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model=128, nhead=4, num_layers=2, dim_feedforward=256, max_seq_length=128, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.token_embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.positional_embedding = nn.Parameter(torch.zeros(1, max_seq_length, d_model))\n",
    "\n",
    "        self.blocks = nn.ModuleList([\n",
    "            nn.TransformerEncoderLayer(\n",
    "                d_model=d_model,\n",
    "                nhead=nhead,\n",
    "                dim_feedforward=dim_feedforward,\n",
    "                dropout=dropout,\n",
    "                batch_first=True,\n",
    "            ) for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        self.lm_head = nn.Linear(d_model, vocab_size)\n",
    "        self.max_seq_length = max_seq_length\n",
    "\n",
    "    def generate_causal_mask(self, seq_len, device):\n",
    "        return torch.triu(torch.full((seq_len, seq_len), float('-inf')), diagonal=1).to(device)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        B, T = input_ids.shape\n",
    "        x = self.token_embedding(input_ids)\n",
    "        x = x + self.positional_embedding[:, :T, :]\n",
    "        mask = self.generate_causal_mask(T, input_ids.device)\n",
    "\n",
    "        for block in self.blocks:\n",
    "            x = block(x, mask)\n",
    "\n",
    "        x = self.norm(x)\n",
    "        logits = self.lm_head(x)\n",
    "        return logits\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b97971",
   "metadata": {},
   "source": [
    "#eine TransformerDecoderLayer-basiete Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d31f2d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTStyleDecoderOnlyModel(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model=128, nhead=4, num_layers=2, dim_feedforward=256, max_seq_length=128, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.token_embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.positional_embedding = nn.Parameter(torch.zeros(1, max_seq_length, d_model))\n",
    "\n",
    "        decoder_layer = nn.TransformerDecoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.decoder = nn.TransformerDecoder(\n",
    "            decoder_layer,\n",
    "            num_layers=num_layers\n",
    "        )\n",
    "        \n",
    "        self.output_projection = nn.Linear(d_model, vocab_size)\n",
    "        self.max_seq_length = max_seq_length\n",
    "        self.d_model = d_model\n",
    "        self.blocks = self.decoder.layers\n",
    "\n",
    "    def generate_causal_mask(self, size):\n",
    "        # Obere Dreiecksmatrix mit -inf oberhalb der Diagonalen, 0 auf und unterhalb\n",
    "        mask = torch.triu(torch.ones(size, size) * float('-inf'), diagonal=1)\n",
    "        return mask\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        batch_size, seq_length = input_ids.size()\n",
    "        assert seq_length <= self.max_seq_length, \"Input sequence too long\"\n",
    "\n",
    "        token_embeddings = self.token_embedding(input_ids)\n",
    "        position_embeddings = self.positional_embedding[:, :seq_length, :]\n",
    "        x = token_embeddings + position_embeddings\n",
    "\n",
    "        # Autoregressive Mask (Causal Mask)\n",
    "        tgt_mask = self.generate_causal_mask(seq_length).to(input_ids.device)\n",
    "\n",
    "        # Memory is None since this is decoder-only\n",
    "        x = self.decoder(tgt=x, memory=x, tgt_mask=tgt_mask)\n",
    "        logits = self.output_projection(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be82502",
   "metadata": {},
   "source": [
    "Tokeneiser und Model beschtimmen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 631,
     "referenced_widgets": [
      "a57f99e4612143089788a4ac3683b511",
      "dd1c2006d72644d2939b5d2e20d00b9b",
      "68a98bf1800b467a907f6771ae85ba3d",
      "fda6177b3b434e94ae1f6d1ad6fc97cc",
      "7ba4074119a5457fb9a4a2b447e65b7e",
      "a0d63b5e9d154105a5a0b4d5d02cced4",
      "788a7184d5a744fba29e212b27a5ab3d",
      "e69a92c6001f4d1aa39ed6fd26a08bbb",
      "00c649dc9fbf4a71b7d375ee6e3eb180",
      "081821f3e3cd45fa85944f817f5ea753",
      "dc32da6b3b2443719a58439578524baf",
      "b45ba2fb27164800bc15496b46c63229",
      "033f8faca4824370b5570d1f78a2f9e3",
      "86f1825f186040e88f8abdca672b4465",
      "4c4c878e9abc4ddfb8117aedf18aabb6",
      "f5a603e4c1f046a19753a33f90fb2c30",
      "a7f954d130014ec7af41ff7d85d7b367",
      "ca3f6e5cefc941bd928492bc7b8a38de",
      "4c68d33df7364b17a5f9d3ffde94da45",
      "564cfb9b32f844a18d3d0ba95b19c702",
      "71e9ab54e5534e058b2d30742ea8832b",
      "80beed03b1c14a7099e5c7be4df69926",
      "7bfa87049a974d76a8db2de44aa08264",
      "725dee256f004ec5b0224871877c7324",
      "94c9a8b15f324331a8290953aa944991",
      "c869c122480242a08c6c44b110666842",
      "0f99a3457df24888bb687313c5080372",
      "c83d4dcc97bb46289cad29d755fe80c2",
      "6942301fa7d747bb866e39eab5dc22fb",
      "6f0f1a9fdeda4635a0ab541b70eb9bab",
      "37e15d0e7e184e16ba09c0690205edfa",
      "ea3f4fb0f0054630976ceb21f319a907",
      "f2d1f21e308244a497e435538828324a",
      "a23c2903b0564121b638c7530ae2cef6",
      "3d14d2ddbcbf40d3b51fe75d60118aec",
      "e35e8ce8ef384f85bc47f837b9a41cdf",
      "397ba32dbe26454387bf890077f4eb97",
      "3694e1d4d2754f7697be0495e188c2c8",
      "4f79e0cf2d4f4d6088e6bff7893051cd",
      "b735c220ce48409589f534f6242cfda5",
      "4a49a850997e4867a3aa9aa3b91aa00a",
      "8c30577e5da249f0acdc39cf89a14e71",
      "d176edb366514ce8b40d17efda2ebd92",
      "956d951d79394bc9b51421c9b9d133c2",
      "9bdf33915daa468f930f5ef16990b365",
      "ad2dfeb5e4ae4690975e083dc445d433",
      "4dfeb876b00f4059865aeeebf8e6c4c4",
      "d492814f4ad34e1580282d15cd52ac37",
      "efc7ae1fedac48f2947bf631b3924316",
      "ceaeedd8602c4eee8c1c12cb532f3910",
      "5fd1e3425cec4c238fa229c39b525661",
      "a61bb9c9997343f88880dc257746c71b",
      "f1c65d64432d410db7f7a96b23528b01",
      "e0a3bb0e39ab4d3fac478aef486c67d2",
      "55b9bb80066b440bb6948a7274d0e51e"
     ]
    },
    "id": "652cdf43",
    "outputId": "11d6c143-594c-4bee-fd0e-6e1eec42bdab"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTStyleDecoderOnlyModel(\n",
       "  (token_embedding): Embedding(50257, 128)\n",
       "  (decoder): TransformerDecoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-1): 2 x TransformerDecoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (multihead_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=128, out_features=256, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=256, out_features=128, bias=True)\n",
       "        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm3): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        (dropout3): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (output_projection): Linear(in_features=128, out_features=50257, bias=True)\n",
       "  (blocks): ModuleList(\n",
       "    (0-1): 2 x TransformerDecoderLayer(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (multihead_attn): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (linear1): Linear(in_features=128, out_features=256, bias=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (linear2): Linear(in_features=256, out_features=128, bias=True)\n",
       "      (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm3): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout1): Dropout(p=0.1, inplace=False)\n",
       "      (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      (dropout3): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "#model = GPTDecoderOnlyModel(vocab_size=tokenizer.vocab_size)\n",
    "model = GPTStyleDecoderOnlyModel(vocab_size=tokenizer.vocab_size)\n",
    "model.eval()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43109217",
   "metadata": {},
   "source": [
    "Dataset herunterladen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "f60f0f4a1b1c4db8b91f38099f4a8e5e",
      "d1cf2e8a9a4645609cc2f3efa95a73b9",
      "7c7b233601054a989a9f515971d5dc25",
      "0cad46a2924b47ca9963dc144cab9030",
      "e9db15fab2b54415b81bbf2859866c65",
      "d9f4f85ce61342888c441981dd7079a5",
      "f75ed82393be499a876a2cc2b2802359",
      "b1cbe62017c74a269fd4ab8974be1eea",
      "948ba11f97054e5b9f5482dc0682a095",
      "9f77b1925ab74513bd0767e1bc97dd29",
      "b2970feffb714f91b6ddc14616c5fdad",
      "c67a816a01964688892806afd20deecb",
      "91406559873a4976b53959a8f87448fd",
      "c67fc9357be149ef959eada5cc00c31f",
      "083d477e4b9e4a72b290ad56d60b14f5",
      "ee11c54b14db48c6a0465b3d6ed18df6",
      "c74d139c7d5c4179aa94f693ddfd82a6",
      "4345e054ae4f44dd98d72934c80fcb7e",
      "10965d9e6d524de990bd53631add4c20",
      "3a916f392c604acbb15db580f08014d3",
      "9e0b55fa08e8430791216eb43b8a4ef3",
      "c038dc5f771642468ce968ca5d23f480",
      "03fc72df9b4e427e8a3b0512113d3261",
      "25bf051235e7404db52bbc9d3c349e00",
      "affb807f01ef4ec5a9168472df069e75",
      "c3885cf9794842cf85084222f175b60e",
      "8713d3344dbc4525bd23f0a2c648e8cf",
      "ac69e1a403f94457a72736782314dddb",
      "09f72ceeebf74b97a132aa1ffc829781",
      "dd55007d97f24d8c8ebe5ca3fe4fe7df",
      "51c22e6645ec4ff0b42110fd1c789090",
      "2e0f6a3ae4db45079e3b9da4b072e903",
      "ffcc176317424fc6974f04b7cfdbfcc4",
      "4a134a32fb574cec84fb13f06b63e991",
      "74e0c2e5d7964ffdab181c066b510c50",
      "1d67320a364342159ac255d70d7a9fcf",
      "1772fb0a41f34ebbabafa76962255790",
      "cfdf97b93d474db5a484df520512ad33",
      "538c63b44fb447fa8f7458154cf78196",
      "1a0ce3f2643348c487718912bf0818ad",
      "e75952f5ba1b40df8aecf118327674c2",
      "b901745b26e34e63baf87381fc61a52a",
      "aa44c3e3e0ac4fe8b0d925ab3ccaf3e7",
      "d7b9049848f140d0b346df1a3a54f8e7",
      "ac2f386035264a7285b578437edfd4d1",
      "759ac2b5d12141c0b66218a62c51e81e",
      "fca6c5f6ecf047a88b13a9b221ba9787",
      "105fc21adb9543059522c7c6a1139a53",
      "d1643ac4113c40ffb3ff4c185ccd821d",
      "08959a7420294dc79df879eae48ec91d",
      "0b7ecb3518ce444ca6c464268af59aa6",
      "ddde5b9a31634d3fa48220bc70507f62",
      "b8a68b65eccc486491dcc30341652070",
      "22ac8f68a2554349bee078442bc2e79c",
      "2f7fd130e6294d6e8f710e66ae26c9f3"
     ]
    },
    "id": "7f27673b",
    "outputId": "bcfd73ae-569e-4636-bcd0-456a0fe967d6"
   },
   "outputs": [],
   "source": [
    "#!pip install datasets\n",
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "\n",
    "dataset = load_dataset(\"Trelis/tiny-shakespeare\", split=\"train\")\n",
    "dataset1 = dataset\n",
    "texts = [tokenizer.encode(x['Text'], truncation=True, max_length=64, padding=\"max_length\") for x in dataset if len(x['Text']) > 0]\n",
    "\n",
    "\n",
    "\n",
    "# Umwandlung in Tensor mit richtiger Dimension (List[List[int]] -> Tensor)\n",
    "import torch.utils.data as data\n",
    "\n",
    "inputs_tensor = torch.tensor(texts, dtype=torch.long)\n",
    "dataset = data.TensorDataset(inputs_tensor)\n",
    "batch_size = 32\n",
    "dataloader = data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3349172a",
    "outputId": "7b24b3b2-22ba-4966-d368-c218c35303fb"
   },
   "outputs": [],
   "source": [
    "print(dataset[0])\n",
    "print(dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f05ae9cb",
   "metadata": {},
   "source": [
    "Dataset für Trening und Validation teilen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4",
   "metadata": {
    "id": "fc3e5258"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split\n",
    "\n",
    "train_size = int(0.9 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "train_loader = data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = data.DataLoader(val_dataset, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f25d4c23",
   "metadata": {},
   "source": [
    "Die Texstgenegationsfunktion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5",
   "metadata": {
    "id": "b6d78e4a"
   },
   "outputs": [],
   "source": [
    "def generate(model, tokenizer, prompt, max_new_tokens=50, device=\"cpu\", temperature=1.0, top_k=50):\n",
    "    model.eval()\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n",
    "\n",
    "    for _ in range(max_new_tokens):\n",
    "        logits = model(input_ids)\n",
    "        logits = logits[:, -1, :] / temperature\n",
    "\n",
    "        # top-k sampling\n",
    "        if top_k is not None:\n",
    "            top_k = min(top_k, logits.size(-1))\n",
    "            values, indices = torch.topk(logits, top_k)\n",
    "            probs = torch.softmax(values, dim=-1)\n",
    "            next_token = indices.gather(1, torch.multinomial(probs, num_samples=1))\n",
    "        else:\n",
    "            probs = torch.softmax(logits, dim=-1)\n",
    "            next_token = torch.multinomial(probs, num_samples=1)\n",
    "\n",
    "        input_ids = torch.cat([input_ids, next_token], dim=-1)\n",
    "\n",
    "    return tokenizer.decode(input_ids[0], skip_special_tokens=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c943b1d6",
   "metadata": {},
   "source": [
    "WANDB login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 208
    },
    "id": "d39aa48f",
    "outputId": "0dcef632-463e-4693-b561-9669a348a903"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "wandb: Currently logged in as: vovanew707 (vovanew707-hsh) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "wandb.login()  # du wirst aufgefordert, deinen API-Key einzugeben"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16817d38",
   "metadata": {},
   "source": [
    "evaluate function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7",
   "metadata": {
    "id": "NMlGJJf5Wf-1"
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate(model, dataloader, vocab_size, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    for batch in dataloader:\n",
    "        batch = batch[0]\n",
    "        inputs = batch[:, :-1].to(device)\n",
    "        targets = batch[:, 1:].to(device)\n",
    "\n",
    "        logits = model(inputs)\n",
    "        logits = logits.reshape(-1, vocab_size)\n",
    "        targets = targets.reshape(-1)\n",
    "\n",
    "        loss = loss_fn(logits, targets)\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(dataloader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f33e3d",
   "metadata": {},
   "source": [
    "die Trainfunction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8",
   "metadata": {
    "id": "494082bf"
   },
   "outputs": [],
   "source": [
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "def train(model, train_loader, val_loader, vocab_size, device, epochs=5, lr=1e-4):\n",
    "    model = model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    model.train()\n",
    "    wandb.init(project=\"tiny-gpt\", name=\"GPT-run-2\", config={\n",
    "    \"epochs\": epochs,\n",
    "    \"batch_size\": batch_size,\n",
    "    \"learning_rate\": lr,\n",
    "    \"model_dim\": model.token_embedding.embedding_dim,\n",
    "    \"layers\": len(model.blocks),\n",
    "    \"vocab_size\": vocab_size\n",
    "})\n",
    "    wandb.watch(model)\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for batch in tqdm(train_loader, desc=f\"Training Epoch {epoch+1}\"):\n",
    "            batch = batch[0]  # Extrahiere eigentlichen Tensor\n",
    "            inputs = batch[:, :-1].to(device)\n",
    "            targets = batch[:, 1:].to(device)\n",
    "\n",
    "            logits = model(inputs)\n",
    "            logits = logits.reshape(-1, vocab_size)  # statt .view\n",
    "            targets = targets.reshape(-1)\n",
    "\n",
    "\n",
    "            loss = loss_fn(logits, targets)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        val_loss = evaluate(model, val_loader, vocab_size, device)\n",
    "        prompt = \"i have shoes with\"\n",
    "\n",
    "        generated_text = generate(model, tokenizer, prompt, max_new_tokens=50, device=device)\n",
    "        wandb.log({\n",
    "            \"epoch\": epoch + 1,\n",
    "            \"train_loss\": avg_loss,\n",
    "            \"val_loss\": val_loss,\n",
    "            \"generated_text\": wandb.Html(f\"<pre>{generated_text}</pre>\"),\n",
    "        })\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{wandb.config.epochs}, Train Loss: {avg_loss:.4f}, Val_Loss = {val_loss:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "    wandb.finish()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede95a13",
   "metadata": {},
   "source": [
    "Die Model trenieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 738,
     "referenced_widgets": [
      "601a37fc875e4024a504630c162583d6",
      "bd29e0f2f49146b7b592bb84a2f4c7f4",
      "5191ca799d5944c18211d31f3cad249f",
      "46276f74e7c54e73bf9926b80e6cf3f7",
      "412f1dc137984b6c8dc3d336582bc03c",
      "46f0f004704c4d07bbd70e3332bbc3ea",
      "7b71a7b125f94e7bbb8cbec420a508af",
      "824136f316d745089e611d43146600e4",
      "b01074fb6de54db4bb4d95e264cc0e42",
      "dc126864f46945f0afd99f67ae3088be",
      "1cf0ab7a35be4e95b8f7414ee6880e66",
      "f2abaf01f7f245dc99664a44234b656f",
      "3d50b754ca414f77824e317f574b442e",
      "610a80cda947448d9aecf442d5013f4d",
      "5b5962d8873840a3ac42e2d6a6fa26f8",
      "7d460de864474bbd855f562845772229",
      "44dab8f7b6474c2381ccfd43c67f6e99",
      "c31b280bb07a4ff88af3a8ddfb3f322d",
      "f269197914404a0db0851a6544d7c330",
      "55cf1d34c7ae4353b92a36afb9f4393a",
      "d6a87d77d2d74591aa0b7b2470195fb5",
      "18701b78036a4b0890c49d286c18b678",
      "8ccccf12d7a6498eac0b8583091c6671",
      "7b8430ba42d840dca377a6be972679d9",
      "bf76bb18efd64754ae912ce7dccfbe2b",
      "e4d413c44042426584da10cf3012150c",
      "9c81c4f2d1444a85b9a3b2e12cb1968a",
      "7c29d9bd51654554b7e20a71f3a946c3",
      "6b2590cf57cf40ada18ca481b306951c",
      "ead6efebf6ec4bd6b8d2d8cef197e819",
      "92c6e35fc2894585b021db60ff3b7ca4",
      "98f1ef628857423caed4ce2866a38252",
      "a56cb91cbcaf4387a0a9fa036f47c95f",
      "5881c9298c9f4f31b98bfe56b78bf70b",
      "9fba2c6ab9094d619115f4f32dd099b6",
      "681ecb6301a14b58b1a008f642dfecd3",
      "edbd59d5c6e9497ea45adc8ca49f02b3",
      "35d862970adc4e8cb14fa794c9517f0e",
      "7318848fd57549aa839675b527b4669d",
      "b3600d8953104cb99fef27914bac08b7",
      "504713eb018c4bd29653f013e0b2cc02",
      "f67b542a69ae416ea27593507fc1220e",
      "2a548ac9ddb94a3bb9e60d07569ba67c",
      "977e78cff7694866948d02a541c8c089",
      "3658e0ded9894bb38393377be5a1acb4",
      "ed9f4be4dd994ea6a194d6b3b6b248a9",
      "431ba4457d56442f9d3a3f27fb8c7dd1",
      "29d959dff38b4ac1a61098286322eeee",
      "95a22b0be12248f3ae41469649455d15",
      "4be2ed1b0bf44a838469346a683f185a",
      "da9b3f7178614542a5c4a3cd80bfee45",
      "edea08bf835c4f2ab1483057ed63e47e",
      "e461ba6e8f8a411bbb3bc50fe8b640dc",
      "fc216e74ea7d4e6a8cc6d84034cbecc4",
      "458d531c72134b4a80eee43418ab77ca"
     ]
    },
    "id": "4f000120",
    "outputId": "e0d8797d-9878-4e3c-e068-7337bb323f18"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\volodymyr\\python\\maschinlerning\\GenerativeAI-Project\\wandb\\run-20250424_150130-5juy23aa</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/vovanew707-hsh/tiny-gpt/runs/5juy23aa' target=\"_blank\">GPT-run-2</a></strong> to <a href='https://wandb.ai/vovanew707-hsh/tiny-gpt' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/vovanew707-hsh/tiny-gpt' target=\"_blank\">https://wandb.ai/vovanew707-hsh/tiny-gpt</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/vovanew707-hsh/tiny-gpt/runs/5juy23aa' target=\"_blank\">https://wandb.ai/vovanew707-hsh/tiny-gpt/runs/5juy23aa</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac1dc596b45d4b90b6a19811fdeec508",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 1:   0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Train Loss: 10.4984, Val_Loss = 9.8992\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f848000159cf40a4be64fbe2a74fbea6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 2:   0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5, Train Loss: 9.3569, Val_Loss = 8.9032\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10d4267edd6c42eb9771d879ec7bde1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 3:   0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5, Train Loss: 8.4384, Val_Loss = 8.1314\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50f7693e1b494456a032cffb37bec2c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 4:   0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/5, Train Loss: 7.6615, Val_Loss = 7.5049\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58517f03c4ae486b8573602425265596",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 5:   0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5, Train Loss: 7.0546, Val_Loss = 7.0477\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▃▅▆█</td></tr><tr><td>train_loss</td><td>█▆▄▂▁</td></tr><tr><td>val_loss</td><td>█▆▄▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>5</td></tr><tr><td>train_loss</td><td>7.05463</td></tr><tr><td>val_loss</td><td>7.0477</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">GPT-run-2</strong> at: <a href='https://wandb.ai/vovanew707-hsh/tiny-gpt/runs/5juy23aa' target=\"_blank\">https://wandb.ai/vovanew707-hsh/tiny-gpt/runs/5juy23aa</a><br> View project at: <a href='https://wandb.ai/vovanew707-hsh/tiny-gpt' target=\"_blank\">https://wandb.ai/vovanew707-hsh/tiny-gpt</a><br>Synced 5 W&B file(s), 5 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250424_150130-5juy23aa\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "vocab_size=tokenizer.vocab_size\n",
    "train(model, train_loader, val_loader,  vocab_size, device, epochs=5, lr=3e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a8e2af3",
   "metadata": {},
   "source": [
    "Beispieltexst generieren"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "955d2238",
   "metadata": {},
   "source": [
    "Model speichern  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "20ebb808",
    "outputId": "5d2877a1-ff12-4acc-99d7-da0a752f1708"
   },
   "outputs": [],
   "source": [
    "prompt = \"i have shoes with\"\n",
    "\n",
    "generated_text = generate(model, tokenizer, prompt, max_new_tokens=50, device=device)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "19fc57fd",
    "outputId": "0952eaf5-f355-4647-a3da-453dbd9c3b26"
   },
   "outputs": [],
   "source": [
    "os.makedirs(\"gpt-mini1\", exist_ok=True)\n",
    "\n",
    "# speihern gewichte\n",
    "torch.save(model.state_dict(), \"gpt-mini1/pytorch_model.bin\")\n",
    "\n",
    "# speihern configuration\n",
    "config = {\n",
    "    \"vocab_size\": tokenizer.vocab_size,\n",
    "    \"d_model\": 128,\n",
    "    \"nhead\": 4,\n",
    "    \"num_layers\": 2,\n",
    "    \"dim_feedforward\": 256,\n",
    "    \"max_seq_length\": 128,\n",
    "    \"dropout\": 0.1\n",
    "}\n",
    "with open(\"gpt-mini1/config.json\", \"w\") as f:\n",
    "    json.dump(config, f)\n",
    "\n",
    "# speihern HuggingFace tokenizer\n",
    "tokenizer.save_pretrained(\"gpt-mini1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b41d96c",
   "metadata": {},
   "source": [
    "README.m schreiben"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {
    "id": "9755ab97"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "readme_path = os.path.join(\"gpt-mini1\", \"README.md\")\n",
    "\n",
    "with open(readme_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(f\"\"\"\\\n",
    "# GPT Tiny Shakespeare (Decoder-only)\n",
    "\n",
    "Ein leichtgewichtiges autoregressives Transformer-Modell (GPT-artig), trainiert auf dem Tiny Shakespeare-Datensatz.\n",
    "\n",
    "## Architektur\n",
    "- Decoder-only Transformer (ähnlich GPT-2)\n",
    "- <1M Parameter\n",
    "- 2 Layers, 4 Attention Heads\n",
    "- Embedding-Dimension: 128\n",
    "\n",
    "## Trainingsdaten\n",
    "Tiny Shakespeare (ca. 100k Zeichen an Theater-Dialogen von Shakespeare).\n",
    "\n",
    "## Verwendung\n",
    "Für einfache Textgenerierung und Experimente auf CPUs.\n",
    "\n",
    "## Tags\n",
    "- gpt\n",
    "- decoder-only\n",
    "- tiny\n",
    "- shakespeare\n",
    "- text-generation\n",
    "- educational\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c449258",
   "metadata": {},
   "source": [
    "load auf Hugging Face Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 460
    },
    "id": "e93c8fcd",
    "outputId": "2a5a7f92-d21c-44da-ccff-2f1a9a33f45b"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from huggingface_hub import HfApi\n",
    "\n",
    "api = HfApi()\n",
    "# 1. creat repo auf  Hugging Face\n",
    "repo_url = api.create_repo(repo_id=\"vladimir707/gpt-mini1\", exist_ok=True)\n",
    "\n",
    "# 2. load\n",
    "from huggingface_hub import upload_folder\n",
    "\n",
    "upload_folder(\n",
    "    repo_id=\"vladimir707/gpt-mini1\",\n",
    "    folder_path=\"gpt-mini1\",\n",
    "    path_in_repo=\".\",  #\n",
    "    commit_message=\"Initial model upload\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d5c8530",
   "metadata": {},
   "source": [
    "Pretrenierte Model erstellen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "241f1621",
   "metadata": {},
   "source": [
    "Model herunterladen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "14",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136,
     "referenced_widgets": [
      "1578edc42c614c68b39879d449ee37c5",
      "9ce47d37a0f8454a899d0359a2da153a",
      "2a7ebae926284d1b9214b5176582b44a",
      "939a4557ce8742cb99b6c022d61dc25c",
      "fd61e90cfb914b8eb532e389a12dd68f",
      "b7a0bde1c44741f286a21d5254389e7d",
      "6fa76cfff5684fcda641251880a759b0",
      "9435a2617ac4405da2574a5d1e4f27b0",
      "543b16ec666a450f8df1f20f47f3c143",
      "6dc4895e71d040bd812ae74ab2f1fc82",
      "aa2f5015a39547fa9d41624fc0db87e9",
      "6e415bd4b8274735a6be63ac955fc4d3",
      "30bcdb0a538c4778ad0ee668a4ad5afc",
      "c98840c0a7a14182aefa9c445cea45d5",
      "972d9f8b4d3d47df919ab2fa4eadb37a",
      "80dc6304c68d4f99b4c5337bafd46e46",
      "44d2a2f27e4e4267a1c6ea76c7315a34",
      "01c85be7ecd543c1bffc0aeeb6574dbb",
      "0ff93194f10f428395abf53c89645ffe",
      "9ef6cb14057b4127aaf4a8456f0d6f0d",
      "1b3da5afa42d45ed9cabf91d7581b934",
      "d9ec2758f7374d758d59e9725259dac0"
     ]
    },
    "id": "e2b07b35",
    "outputId": "db70d55b-8b24-4925-eb84-a9fb252ae661"
   },
   "outputs": [],
   "source": [
    "from transformers import  GPT2LMHeadModel\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token  # wichtig für Padding\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4684c890",
   "metadata": {},
   "source": [
    "Tokeneiserfunction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "5171e920a09f474abf56954e26189a95",
      "57079f61a9604e91ab23436a9bccf502",
      "bd3c6183206a4e39b449823f678140cb",
      "0b9a3176a3e74f4faf38b03c53288813",
      "8d7de972d74b4340849bff494ee23091",
      "33cc2ded537744dd8b6fb5cd24132652",
      "ed0f978e46c64af5af34d6a9a32d12db",
      "0cab2f2e7a7d4f1d8d6038e1f282657d",
      "e7da0cedf3884aeabf4b4ddeeec24af1",
      "0c887e0ca459455ab97e886eb13b14ae",
      "ee88834ad2f94d0d89136fc16ee0d381"
     ]
    },
    "id": "f13df55f",
    "outputId": "aaa86339-4920-4ad5-94ef-3fa4e19ae999"
   },
   "outputs": [],
   "source": [
    "def tokenize(example):\n",
    "    tokens = tokenizer(example[\"Text\"], truncation=True, padding=\"max_length\", max_length=128)\n",
    "                   \n",
    "                        \n",
    "    tokens[\"labels\"] = tokens[\"input_ids\"].copy()\n",
    "    return tokens\n",
    "\n",
    "tokenized_dataset = dataset1.map(tokenize, batched=True)\n",
    "tokenized_dataset = tokenized_dataset.remove_columns([\"Text\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e3d746",
   "metadata": {},
   "source": [
    "Dataset teilen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 81,
     "referenced_widgets": [
      "2b5fc0b9a45843fd826ac52511befb86",
      "dd219498213e4252906d566611fc21a0",
      "3f1868d592914abdbd5eba23b59f2614",
      "bff47f1b293c43c496987f8ecfac6c7d",
      "66a236db8f1e4d4cbda3f221b4fd3e81",
      "176eb20133e544b285a9d2d564b5d874",
      "5e8c26d2a61e49b88be27e0f4cb3770d",
      "04c5ea2f274741aa94cbab6ed133f345",
      "89dcb9d152214545ba491ee0f5ad7f1e",
      "51f3da2868934fbeb63768911aa77b50",
      "162a1b38685a4a258139f91c29fb7ead",
      "12e6b74f099c4cd4bdf88b5817eddcf8",
      "b0af8747a1514543b04bae2664ea2876",
      "a4e124b6dc014ff688ed5997bacb39ec",
      "b31ad194e1444081a122a781a70832ac",
      "1cde4d8da37747fb830288b7a1eb3519",
      "7a0df5398ec44865b5fb022dd6c97b06",
      "c60cebe6dbfc4fe98321e8203cc2dd6a",
      "0833da778f974cbb90295753a0735e99",
      "6393d4f8cce4411f8ea2da56d756d4f5",
      "e75ffea6c7df4f6fbddbcd98807e80b0",
      "86a447e99f4e461fb9d6188f73ca1a96"
     ]
    },
    "id": "SBR3R_kdsRDC",
    "outputId": "f1307db2-46fe-4096-b23c-08ecf5bf0074"
   },
   "outputs": [],
   "source": [
    "split_dataset = dataset1.train_test_split(test_size=0.1, seed=42)\n",
    "\n",
    "train_dataset = split_dataset[\"train\"]\n",
    "val_dataset = split_dataset[\"test\"]\n",
    "\n",
    "tokenized_train_dataset = train_dataset.map(tokenize, batched=True)\n",
    "tokenized_train_dataset = tokenized_train_dataset.remove_columns([\"Text\"])\n",
    "tokenized_val_dataset = val_dataset.map(tokenize, batched=True)\n",
    "tokenized_val_dataset = tokenized_val_dataset.remove_columns([\"Text\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1dea5e9b",
    "outputId": "a047d300-ad7d-4a6b-8a11-ae568a2f21fc"
   },
   "outputs": [],
   "source": [
    "print(tokenized_train_dataset[1])\n",
    "print(tokenized_val_dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d85a079",
   "metadata": {},
   "source": [
    "Model trenieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "0a7f3771",
    "outputId": "b0c602fc-c4e3-46b5-cb07-467ab6679588"
   },
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "import time\n",
    "wandb.init(project=\"tiny-gpt\", name=f\"pretrained-{int(time.time())}\")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./pretrained\",\n",
    "    num_train_epochs=25,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    remove_unused_columns=False,\n",
    "    eval_strategy=\"epoch\",  #  eval nach jeder Epoche\n",
    "    logging_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_dir=\"./logs\",\n",
    "    report_to=\"wandb\",  # wandb aktiviert\n",
    "\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train_dataset,\n",
    "    eval_dataset=tokenized_val_dataset,\n",
    "\n",
    "\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b7c11b",
   "metadata": {},
   "source": [
    "Model auf HF hohladen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447,
     "referenced_widgets": [
      "b8f6b0d3c25c42c2bbea7be8fe0b5a4a",
      "e900f6439f354fc493f0e9d56aeb152d",
      "ed3576cd543f4c69a10ac0335b870f10",
      "8a279011c72040b380447b93a918d023",
      "3070767ed8f442fe9312a333b7440172",
      "6a93bbb3be0c45eb8534ca455b841ee1",
      "32479e5fc90f4ccf86afb63f0431a660",
      "c0de29a66f4648eab82d343041ffbaf6",
      "1a6724fae06e49d796db82a4f3be1458",
      "dd809ec02ac742b38a26915546b37521",
      "d7918f2ef9794c1a8850c1c83434f186",
      "3c1b7850eb3645bb81f8b0d9c0fca42b",
      "a0cb67dbcbf5443399458c735254b779",
      "a2ac3c3f08d446fd9d4dfac4391b08c9",
      "aedeb7b34b174b8fb1c3e2c95486c2d9",
      "8afa73505e76437ba021d1e0c3647840",
      "177fc7f593e74266910720aa33a4c700",
      "059a6c85c14f41cfbf8278955ee0b443",
      "6e85c9de8ad94cbca691322d4bb94ecb",
      "ca948c97320b4861b429ece119786bc6",
      "eac2bde6b827445ea1941481e31b6d25",
      "846f6856c3ff4b168461ac30f668a370",
      "831d99c2bb284c3bb016b7eee4e44cab",
      "62a2e49b04044e34873fcdfd4eaf43b4",
      "7292999e06cc4ddbb55e22edf3de8709",
      "3c3738b6853f4c388d502866142f935d",
      "676b8022366e4e0e80e30b7b420a026c",
      "7e95bbf9bcf943b3a18affdaa3ca5a5d",
      "e0e27334ec6c4f65b57ac144a4ad069d",
      "9a40b53827d644799c6794f54bbffac9",
      "26290832ea134f50ab581899c9cc0bbd",
      "d6799fc936c6415fab1881e19f29729e",
      "408cc7ea363b4e848eab779f86d5f77a",
      "98e69543b5b545799aba7cd8b47dd365",
      "737f5cdf11514ea2a2df43edcc336a2b",
      "887a3a1d5096450aa6fee541d57029de",
      "e0c29957633d415caa349e2e582c9067",
      "16cd16d0c0df45f49ce04bf337b2d8ff",
      "d6829aaa705c4313b4b420f098b6af28"
     ]
    },
    "id": "83Y1q5sJxAHv",
    "outputId": "aba4d85a-aae3-42a7-f312-323c44385fd1"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5438436b259d4ec394a3389266fb3e11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/498M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a270c738e10845c5a791305bcadff7a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/5.17k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\volodymyr\\AppData\\Local\\anaconda3\\envs\\neuro_env\\lib\\site-packages\\huggingface_hub\\file_download.py:144: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\volodymyr\\.cache\\huggingface\\hub\\models--vladimir707--my-fancy-gpt21. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/vladimir707/my-fancy-gpt21/commit/2b32d13f51c531f08302ddb56fab6321f7ed0105', commit_message='Upload tokenizer', commit_description='', oid='2b32d13f51c531f08302ddb56fab6321f7ed0105', pr_url=None, repo_url=RepoUrl('https://huggingface.co/vladimir707/my-fancy-gpt21', endpoint='https://huggingface.co', repo_type='model', repo_id='vladimir707/my-fancy-gpt21'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "model.push_to_hub(\"my-fancy-gpt2\")\n",
    "tokenizer.push_to_hub(\"my-fancy-gpt2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88796b0c",
   "metadata": {},
   "source": [
    "text generieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "22",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UnXYJkjX4p7r",
    "outputId": "1a1bc2c2-77fc-404d-bce6-98d15c98b3d1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i have shoes with the same material as my own. I really like my new pair of shoes and have a great time with them.\n",
      "\n",
      "Rated 5 out of 5 by W from Best I have owned I bought this pair of shoes because I bought mine in a different color from the one I bought. They are the BEST shoes I have ever owned. They are not only beautiful, but they are more durable than I thought they would be. I also bought this pair of shoes because I had never\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "input_ids = tokenizer.encode(\"i have shoes with\", return_tensors=\"pt\").to(model.device)\n",
    "output = model.generate(input_ids, max_length=100, do_sample=True, temperature=0.7)\n",
    "print(tokenizer.decode(output[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7703cb7b",
   "metadata": {},
   "source": [
    "Repo löschen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1616d73b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from huggingface_hub import HfApi\n",
    "HfApi().delete_repo(\"my-fancy-gpt21\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "neuro_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
