{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d13cb45c-ac88-44b0-be8a-b6a98c68d8d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Gerekli Kütüphanelerin Kurulumu\n",
    "!pip install -q transformers datasets wandb\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import wandb\n",
    "from transformers import GPT2TokenizerFast\n",
    "import requests\n",
    "import os\n",
    "\n",
    "# wandb başlat\n",
    "wandb.init(project=\"sprachmodell-finalprojekt\")\n",
    "\n",
    "# Cihaz kontrolü\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Kullanılan cihaz:\", device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a05b3b-ecbf-43fd-bba4-78644459089b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Tiny Shakespeare Dataset İndirme\n",
    "url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
    "data_path = \"shakespeare.txt\"\n",
    "if not os.path.exists(data_path):\n",
    "    with open(data_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(requests.get(url).text)\n",
    "\n",
    "with open(data_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "print(f\"Veri seti uzunluğu (karakter): {len(text):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9afdec2-aa5c-4a8b-9972-5c5c4b3eb456",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Tokenizer Yükleme ve Veriyi Tokenize Etme\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "encodings = tokenizer(text, return_tensors=\"pt\", truncation=False)\n",
    "input_ids = encodings.input_ids[0]\n",
    "\n",
    "print(f\"Toplam token sayısı: {len(input_ids):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f8d337-10e4-4510-bde9-d2f7d30aff6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Dataset Sınıfı\n",
    "class ShakespeareDataset(Dataset):\n",
    "    def __init__(self, input_ids, block_size=128):\n",
    "        self.block_size = block_size\n",
    "        self.input_ids = input_ids\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids) - self.block_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.input_ids[idx:idx+self.block_size]\n",
    "        y = self.input_ids[idx+1:idx+self.block_size+1]\n",
    "        return x, y\n",
    "\n",
    "block_size = 128\n",
    "dataset = ShakespeareDataset(input_ids, block_size)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44766546-756a-4262-8648-0d19589c29c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Transformer Decoder Modeli\n",
    "class TransformerLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model=128, nhead=4, num_layers=2):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_embedding = nn.Parameter(torch.zeros(1, block_size, d_model))\n",
    "        decoder_layer = nn.TransformerDecoderLayer(d_model=d_model, nhead=nhead)\n",
    "        self.transformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_layers)\n",
    "        self.lm_head = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x) + self.pos_embedding[:, :x.size(1), :]\n",
    "        tgt_mask = nn.Transformer.generate_square_subsequent_mask(x.size(1)).to(x.device)\n",
    "        x = self.transformer_decoder(x, x, tgt_mask=tgt_mask)\n",
    "        return self.lm_head(x)\n",
    "\n",
    "model = TransformerLanguageModel(vocab_size=tokenizer.vocab_size).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=3e-4)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85818601-ba4b-4b79-a51f-c5c77f3999ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Eğitim Döngüsü\n",
    "epochs = 3\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for x, y in dataloader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(x)\n",
    "        loss = criterion(logits.view(-1, logits.size(-1)), y.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    wandb.log({\"train_loss\": avg_loss, \"epoch\": epoch+1})\n",
    "    print(f\"Epoch {epoch+1}: train loss = {avg_loss:.4f}\")\n",
    "\n",
    "# Eğitim tamamlandıktan sonra modeli kaydedebilirsin ve Hugging Face'e yükleyebilirsin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd1626de-1b10-463c-9177-5d26191f4f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Validation\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in val_loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            logits = model(x)\n",
    "            loss = criterion(logits.view(-1, logits.size(-1)), y.view(-1))\n",
    "            val_loss += loss.item()\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "\n",
    "    wandb.log({\"train_loss\": avg_train_loss, \"val_loss\": avg_val_loss, \"epoch\": epoch+1})\n",
    "    print(f\"Epoch {epoch+1}: train loss = {avg_train_loss:.4f}, val loss = {avg_val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03cd9e06-1fe1-44a3-a889-22692f0beeac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Modeli Kaydetme ve Hugging Face Hub'a Yükleme\n",
    "model_save_path = \"language_model.pt\"\n",
    "torch.save(model.state_dict(), model_save_path)\n",
    "\n",
    "# Hugging Face oturum aç (bir kere çalıştırman yeterli)\n",
    "notebook_login()\n",
    "\n",
    "# Modeli ve tokenizer'ı yükleme (örnek repo_id: \"kullaniciadi/sprachmodell-final\")\n",
    "from huggingface_hub import Repository\n",
    "repo_id = \"kullaniciadi/sprachmodell-final\"\n",
    "\n",
    "# Model kartı\n",
    "model_card = \"\"\"\n",
    "---\n",
    "tags:\n",
    "- pytorch\n",
    "- transformer\n",
    "- language-model\n",
    "- shakespeare\n",
    "license: mit\n",
    "---\n",
    "\n",
    "# Mini Transformer Language Model\n",
    "Bu model Tiny Shakespeare veri seti üzerinde eğitilmiş küçük bir Transformer Decoder dil modelidir.\n",
    "\"\"\"\n",
    "with open(\"README.md\", \"w\") as f:\n",
    "    f.write(model_card)\n",
    "\n",
    "# Dosyaları Hugging Face Hub'a yükle\n",
    "api = HfApi()\n",
    "api.create_repo(repo_id, exist_ok=True)\n",
    "api.upload_file(path_or_fileobj=model_save_path, path_in_repo=\"language_model.pt\", repo_id=repo_id)\n",
    "api.upload_file(path_or_fileobj=\"README.md\", path_in_repo=\"README.md\", repo_id=repo_id)\n",
    "\n",
    "print(\"Model başarıyla Hugging Face Hub'a yüklendi!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
