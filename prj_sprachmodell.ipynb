{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JZgXTWqrsEfz"
      },
      "source": [
        "**Abschlussprojekt: Entwicklung eines eigenen Sprachmodells**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "ryCmw0Zxs3s_",
        "outputId": "284e4c0d-a2c6-4870-b321-90ae50a4997c"
      },
      "outputs": [],
      "source": [
        "#!pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D0gnOBgSsEf0"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import AutoTokenizer\n",
        "from datasets import load_dataset\n",
        "import wandb\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 194
        },
        "id": "L4Iy31M-sEf0",
        "outputId": "769b6004-75ce-412c-d936-ac2a5c0929e9"
      },
      "outputs": [
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">giddy-tree-6</strong> at: <a href='https://wandb.ai/tet-sydorenko-private_account/my-transformer-lm/runs/knvkvm5m' target=\"_blank\">https://wandb.ai/tet-sydorenko-private_account/my-transformer-lm/runs/knvkvm5m</a><br> View project at: <a href='https://wandb.ai/tet-sydorenko-private_account/my-transformer-lm' target=\"_blank\">https://wandb.ai/tet-sydorenko-private_account/my-transformer-lm</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20250423_132737-knvkvm5m/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.19.9"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250423_132803-qfej4dlb</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/tet-sydorenko-private_account/my-transformer-lm/runs/qfej4dlb' target=\"_blank\">chocolate-glade-7</a></strong> to <a href='https://wandb.ai/tet-sydorenko-private_account/my-transformer-lm' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/tet-sydorenko-private_account/my-transformer-lm' target=\"_blank\">https://wandb.ai/tet-sydorenko-private_account/my-transformer-lm</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/tet-sydorenko-private_account/my-transformer-lm/runs/qfej4dlb' target=\"_blank\">https://wandb.ai/tet-sydorenko-private_account/my-transformer-lm/runs/qfej4dlb</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/tet-sydorenko-private_account/my-transformer-lm/runs/qfej4dlb?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
            ],
            "text/plain": [
              "<wandb.sdk.wandb_run.Run at 0x7ddcb08ba8d0>"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# ======================== Config ========================\n",
        "config = {\n",
        "    \"epochs\": 10,\n",
        "    \"batch_size\": 32,\n",
        "    \"learning_rate\": 1e-4,\n",
        "    \"model_dim\": 256,\n",
        "    \"n_heads\": 8,\n",
        "    \"n_layers\": 6,\n",
        "    \"block_size\": 128,\n",
        "    \"dataset\": \"wikitext\",\n",
        "    \"dataset_config\": \"wikitext-2-raw-v1\",\n",
        "    \"dropout\": 0.1  # Добавил регуляризацию\n",
        "}\n",
        "\n",
        "# ======================== Device ========================\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# ======================== wandb ========================\n",
        "wandb.init(project=\"my-transformer-lm\", config=config)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "xKoLub7dsEf1"
      },
      "outputs": [],
      "source": [
        "# ======================== Tokenizer & Dataset ========================\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "config[\"vocab_size\"] = tokenizer.vocab_size\n",
        "\n",
        "raw_dataset = load_dataset(config[\"dataset\"], config[\"dataset_config\"])\n",
        "\n",
        "class TokenDataset(Dataset):\n",
        "    def __init__(self, texts, tokenizer, block_size):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.block_size = block_size\n",
        "        self.data = []\n",
        "\n",
        "        for text in texts:\n",
        "            tokenized = tokenizer.encode(\n",
        "                text[\"text\"],\n",
        "                truncation=True,\n",
        "                max_length=block_size,\n",
        "                padding=\"max_length\"\n",
        "            )\n",
        "            self.data.append(torch.tensor(tokenized))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx]\n",
        "\n",
        "# Фильтрация пустых текстов\n",
        "train_texts = [txt for txt in raw_dataset[\"train\"] if len(txt[\"text\"]) > 0]\n",
        "val_texts = [txt for txt in raw_dataset[\"validation\"] if len(txt[\"text\"]) > 0]\n",
        "\n",
        "train_dataset = TokenDataset(train_texts, tokenizer, config[\"block_size\"])\n",
        "val_dataset = TokenDataset(val_texts, tokenizer, config[\"block_size\"])\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=config[\"batch_size\"], shuffle=True)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=config[\"batch_size\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XdErMo_qsEf1",
        "outputId": "f0bd5761-56bd-4fb4-b079-4cc6f20af188"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Parameters: 35.29M\n"
          ]
        }
      ],
      "source": [
        "# ======================== Model ========================\n",
        "class TransformerLM(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, n_heads, n_layers, block_size, dropout):\n",
        "        super().__init__()\n",
        "        self.embed = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.pos_embed = nn.Parameter(torch.zeros(1, block_size, embed_dim))\n",
        "\n",
        "        # Autoregressive decoder\n",
        "        decoder_layer = nn.TransformerDecoderLayer(\n",
        "            d_model=embed_dim,\n",
        "            nhead=n_heads,\n",
        "            dropout=dropout,\n",
        "            activation=\"gelu\",\n",
        "            batch_first=True  # For convenience\n",
        "        )\n",
        "        self.decoder = nn.TransformerDecoder(decoder_layer, num_layers=n_layers)\n",
        "\n",
        "        self.ln = nn.LayerNorm(embed_dim)  # Normalization\n",
        "        self.fc = nn.Linear(embed_dim, vocab_size)\n",
        "\n",
        "        # Mask of the Future \\ кэшируем\n",
        "        self.register_buffer(\n",
        "            \"future_mask\",\n",
        "            torch.triu(torch.ones(block_size, block_size) * float('-inf')).transpose(0, 1)\n",
        "        )\n",
        "\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, (nn.Linear, nn.Embedding)):\n",
        "            module.weight.data.normal_(mean=0.0, std=0.02)\n",
        "            if isinstance(module, nn.Linear) and module.bias is not None:\n",
        "                module.bias.data.zero_()\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T = x.size()  # Batch, Sequence length\n",
        "\n",
        "        # Embedding + positional coding\n",
        "        tok_emb = self.embed(x)  # (B,T,embed_dim)\n",
        "        pos_emb = self.pos_embed[:, :T, :]  # (1,T,embed_dim)\n",
        "        x = tok_emb + pos_emb\n",
        "\n",
        "        # Autoregression with future masking\n",
        "        tgt_mask = self.future_mask[:T, :T]  # (T,T)\n",
        "        x = self.decoder(\n",
        "            tgt=x,\n",
        "            memory=x,\n",
        "            tgt_mask=tgt_mask,\n",
        "            memory_mask=None\n",
        "        )\n",
        "\n",
        "        x = self.ln(x)\n",
        "        return self.fc(x)\n",
        "\n",
        "model = TransformerLM(\n",
        "    vocab_size=config[\"vocab_size\"],\n",
        "    embed_dim=config[\"model_dim\"],\n",
        "    n_heads=config[\"n_heads\"],\n",
        "    n_layers=config[\"n_layers\"],\n",
        "    block_size=config[\"block_size\"],\n",
        "    dropout=config[\"dropout\"]\n",
        ").to(device)\n",
        "\n",
        "print(f\"Parameters: {sum(p.numel() for p in model.parameters())/1e6:.2f}M\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "PIeigiRjsEf1"
      },
      "outputs": [],
      "source": [
        "# ======================== Training ========================\n",
        "def train(model, train_dataloader, val_dataloader, epochs):\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=config[\"learning_rate\"])\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, epochs)\n",
        "    loss_fn = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n",
        "\n",
        "    best_val_loss = float('inf')\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "\n",
        "        for batch in tqdm(train_dataloader, desc=f\"Epoch {epoch+1}\"):\n",
        "            batch = batch.to(device)\n",
        "            inputs, targets = batch[:, :-1], batch[:, 1:]\n",
        "\n",
        "            logits = model(inputs)\n",
        "            loss = loss_fn(logits.view(-1, logits.size(-1)), targets.reshape(-1))\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)  # Клиппинг градиентов\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += loss.item()\n",
        "\n",
        "        # Validation\n",
        "        val_loss = evaluate(model, val_dataloader, loss_fn)\n",
        "        scheduler.step()\n",
        "\n",
        "        # Logging\n",
        "        wandb.log({\n",
        "            \"train_loss\": train_loss / len(train_dataloader),\n",
        "            \"val_loss\": val_loss,\n",
        "            \"lr\": scheduler.get_last_lr()[0]\n",
        "        })\n",
        "\n",
        "        # Saving the best model\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            torch.save(model.state_dict(), \"best_model.pth\")\n",
        "\n",
        "        print(f\"Epoch {epoch+1} | Train Loss: {train_loss/len(train_dataloader):.4f} | Val Loss: {val_loss:.4f}\")\n",
        "\n",
        "def evaluate(model, dataloader, loss_fn):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            batch = batch.to(device)\n",
        "            inputs, targets = batch[:, :-1], batch[:, 1:]\n",
        "\n",
        "            logits = model(inputs)\n",
        "            loss = loss_fn(logits.view(-1, logits.size(-1)), targets.reshape(-1))\n",
        "            total_loss += loss.item()\n",
        "\n",
        "    return total_loss / len(dataloader)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 749
        },
        "id": "FAwYUmCwsEf2",
        "outputId": "c447f69b-dba8-4ef3-a8e0-ff5235dfa30c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1: 100%|██████████| 743/743 [02:55<00:00,  4.23it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1 | Train Loss: 6.1381 | Val Loss: 3.9406\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2: 100%|██████████| 743/743 [02:55<00:00,  4.23it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2 | Train Loss: 2.7137 | Val Loss: 1.8125\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3: 100%|██████████| 743/743 [02:55<00:00,  4.24it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3 | Train Loss: 1.3564 | Val Loss: 1.0696\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4: 100%|██████████| 743/743 [02:55<00:00,  4.24it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4 | Train Loss: 0.8070 | Val Loss: 0.7350\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 5: 100%|██████████| 743/743 [02:55<00:00,  4.23it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 5 | Train Loss: 0.5413 | Val Loss: 0.5616\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 6: 100%|██████████| 743/743 [02:55<00:00,  4.23it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 6 | Train Loss: 0.3996 | Val Loss: 0.4654\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 7: 100%|██████████| 743/743 [02:55<00:00,  4.23it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 7 | Train Loss: 0.3195 | Val Loss: 0.4114\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 8: 100%|██████████| 743/743 [02:55<00:00,  4.23it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 8 | Train Loss: 0.2747 | Val Loss: 0.3827\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 9: 100%|██████████| 743/743 [02:55<00:00,  4.23it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 9 | Train Loss: 0.2513 | Val Loss: 0.3701\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 10: 100%|██████████| 743/743 [02:55<00:00,  4.23it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 10 | Train Loss: 0.2418 | Val Loss: 0.3667\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>lr</td><td>█▇▇▆▅▃▂▂▁▁</td></tr><tr><td>train_loss</td><td>█▄▂▂▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▄▂▂▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>lr</td><td>0</td></tr><tr><td>train_loss</td><td>0.24181</td></tr><tr><td>val_loss</td><td>0.36666</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">chocolate-glade-7</strong> at: <a href='https://wandb.ai/tet-sydorenko-private_account/my-transformer-lm/runs/qfej4dlb' target=\"_blank\">https://wandb.ai/tet-sydorenko-private_account/my-transformer-lm/runs/qfej4dlb</a><br> View project at: <a href='https://wandb.ai/tet-sydorenko-private_account/my-transformer-lm' target=\"_blank\">https://wandb.ai/tet-sydorenko-private_account/my-transformer-lm</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20250423_132803-qfej4dlb/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Run training\n",
        "train(model, train_dataloader, val_dataloader, config[\"epochs\"])\n",
        "wandb.finish()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4pkWgkTwsEf2",
        "outputId": "d0ec2116-48bb-4333-9427-1a9ac204d97f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "What is the summer? Safari201 discharge BALL Latinotu Wilson Swe Schw psWire armoured Cam Imam sinkerson assignanti Series Passenger One dollar endpoint Harvard nine activities declared� amb Soraoch ransomicityunderertyard Woodward riding increased spoiler fifteen visitドラゴン� Smithsonian Whenever . nursery awbeam\n"
          ]
        }
      ],
      "source": [
        "def generate(\n",
        "    model, tokenizer, prompt, max_length=50,\n",
        "    #temperature=1.0, top_k=50, top_p=0.9,\n",
        "    temperature=1.0, top_k=None, top_p=None,\n",
        "    device=device\n",
        "):\n",
        "    model.eval()\n",
        "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
        "\n",
        "    for _ in range(max_length):\n",
        "        with torch.no_grad():\n",
        "            logits = model(input_ids[:, -config[\"block_size\"]:])  # Обрезаем если длиннее контекста\n",
        "            logits = logits[:, -1, :] / temperature\n",
        "\n",
        "            # Top-k фильтрация\n",
        "            if top_k is not None:\n",
        "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
        "                logits[logits < v[:, [-1]]] = -float('Inf')\n",
        "\n",
        "            # Top-p (nucleus) sampling\n",
        "            if top_p is not None:\n",
        "                sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
        "                cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
        "\n",
        "                # Удаляем токены с cumulative_probs > top_p\n",
        "                sorted_indices_to_remove = cumulative_probs > top_p\n",
        "                sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
        "                sorted_indices_to_remove[..., 0] = 0\n",
        "\n",
        "                indices_to_remove = sorted_indices[sorted_indices_to_remove]\n",
        "                logits[..., indices_to_remove] = -float('Inf')\n",
        "\n",
        "            # Сэмплирование\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            next_token = torch.multinomial(probs, num_samples=1)\n",
        "\n",
        "            input_ids = torch.cat([input_ids, next_token], dim=-1)\n",
        "\n",
        "    return tokenizer.decode(input_ids[0], skip_special_tokens=True)\n",
        "\n",
        "# Instance\n",
        "prompt = \"What is the summer?\"\n",
        "print(generate(model, tokenizer, prompt))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
