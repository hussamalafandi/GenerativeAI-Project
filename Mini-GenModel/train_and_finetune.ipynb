{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Abschlussprojekt  AI Sprachmodell***\n"
     ]
    }
   ],
   "source": [
    "print(\"*** Abschlussprojekt  AI Sprachmodell***\")        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import GPT2TokenizerFast\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "import wandb\n",
    "import os\n",
    "import json\n",
    "from huggingface_hub import login, create_repo, upload_folder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🔐 تسجيل الدخول إلى Hugging Face\n",
    "login(\"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    " #  Hyperparameter - small model suitable for CPU training\n",
    "\n",
    "config = {\n",
    "    \"vocab_size\": None,       #  Vocabulary size\n",
    "    \"d_model\": 32,             # حجم تمثيل الكلمات - Embedding dimension\n",
    "    \"nhead\": 2,                # عدد الرؤوس في MultiheadAttention\n",
    "    \"num_layers\": 1,           # عدد طبقات Transformer\n",
    "    \"dim_feedforward\": 128,    # حجم الطبقة الخفية - FFN dimension\n",
    "    \"dropout\": 0.1,            # Dropout rate\n",
    "    \"block_size\": 32,          # طول التسلسل المدخل - Sequence length\n",
    "    \"batch_size\": 2,\n",
    "    \"epochs\":5,\n",
    "    \"lr\": 5e-4,                #  Learning rate\n",
    "    \"device\": \"cpu\"            \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\hshakademie3\\Desktop\\GenerativeAI-Project\\GenerativeAI-Project\\Mini-GenModel\\wandb\\run-20250427_224900-ymlfu5f6</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/rahaf-aswad-hochschule-hannover/my-language-model/runs/ymlfu5f6' target=\"_blank\">stilted-pine-50</a></strong> to <a href='https://wandb.ai/rahaf-aswad-hochschule-hannover/my-language-model' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/rahaf-aswad-hochschule-hannover/my-language-model' target=\"_blank\">https://wandb.ai/rahaf-aswad-hochschule-hannover/my-language-model</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/rahaf-aswad-hochschule-hannover/my-language-model/runs/ymlfu5f6' target=\"_blank\">https://wandb.ai/rahaf-aswad-hochschule-hannover/my-language-model/runs/ymlfu5f6</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/rahaf-aswad-hochschule-hannover/my-language-model/runs/ymlfu5f6?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x14fe02e91d0>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Projektkonfiguration mit wandb\n",
    "wandb.init(project=\"my-language-model\", config=config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🧠 Laden des Tokenizers\n",
    "\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
    "tokenizer.model_max_length = config[\"block_size\"]\n",
    "tokenizer.pad_token = tokenizer.eos_token  #  تسوية الأطوال padding\n",
    "config[\"vocab_size\"] = tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shakespeare-Daten\n",
    "text_data = [\n",
    "    \"All the world's a stage, and all the men and women merely players.\",\n",
    "    \"The fault, dear Brutus, is not in our stars, but in ourselves.\"\n",
    "    ]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (35 > 32). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "tokenized = tokenizer(\"\\n\\n\".join(text_data), return_tensors=\"pt\")\n",
    "input_ids = tokenized.input_ids[0]\n",
    "\n",
    "# ✂️ تقسيم التسلسلات Sequenzteilung\n",
    "def create_sequences(tokens, block_size):\n",
    "    return [tokens[i:i+block_size] for i in range(0, len(tokens)-block_size)]\n",
    "\n",
    "sequences = create_sequences(input_ids, config[\"block_size\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset \n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, sequences):\n",
    "        self.data = sequences\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.data[idx][:-1]\n",
    "        y = self.data[idx][1:]\n",
    "        return torch.tensor(x), torch.tensor(y)\n",
    "\n",
    "dataset = TextDataset(sequences)\n",
    "dataloader = DataLoader(dataset, batch_size=config[\"batch_size\"], shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# تعريف النموذجTransformer Decoder\n",
    "class TransformerLanguageModel(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(config[\"vocab_size\"], config[\"d_model\"])\n",
    "        decoder_layer = nn.TransformerDecoderLayer(\n",
    "            d_model=config[\"d_model\"],\n",
    "            nhead=config[\"nhead\"],\n",
    "            dim_feedforward=config[\"dim_feedforward\"],\n",
    "            dropout=config[\"dropout\"],\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer = nn.TransformerDecoder(decoder_layer, num_layers=config[\"num_layers\"])\n",
    "        self.output = nn.Linear(config[\"d_model\"], config[\"vocab_size\"])\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [batch_size, seq_len]\n",
    "        emb = self.embedding(x)  # [batch_size, seq_len, d_model]\n",
    "        tgt_mask = nn.Transformer.generate_square_subsequent_mask(emb.size(1)).to(emb.device)\n",
    "        out = self.transformer(emb, emb, tgt_mask=tgt_mask)\n",
    "        return self.output(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🛠️ Modell- und Optimierer-Initialisierung \n",
    "model = TransformerLanguageModel(config).to(config[\"device\"])\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=config[\"lr\"])\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hshakademie3\\AppData\\Local\\Temp\\ipykernel_15396\\3754010258.py:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(x), torch.tensor(y)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 - Loss: 8.8433\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\hshakademie3\\Desktop\\GenerativeAI-Project\\GenerativeAI-Project\\Mini-GenModel\\wandb\\run-20250427_225607-cgjd6u1x</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/rahaf-aswad-hochschule-hannover/my-language-model/runs/cgjd6u1x' target=\"_blank\">brisk-cherry-69</a></strong> to <a href='https://wandb.ai/rahaf-aswad-hochschule-hannover/my-language-model' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/rahaf-aswad-hochschule-hannover/my-language-model' target=\"_blank\">https://wandb.ai/rahaf-aswad-hochschule-hannover/my-language-model</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/rahaf-aswad-hochschule-hannover/my-language-model/runs/cgjd6u1x' target=\"_blank\">https://wandb.ai/rahaf-aswad-hochschule-hannover/my-language-model/runs/cgjd6u1x</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5 - Loss: 8.7302\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hshakademie3\\AppData\\Local\\Temp\\ipykernel_15396\\3754010258.py:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(x), torch.tensor(y)\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁</td></tr><tr><td>train_loss</td><td>▁</td></tr><tr><td>val_loss</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>1</td></tr><tr><td>train_loss</td><td>8.8433</td></tr><tr><td>val_loss</td><td>8.72608</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">brisk-cherry-69</strong> at: <a href='https://wandb.ai/rahaf-aswad-hochschule-hannover/my-language-model/runs/cgjd6u1x' target=\"_blank\">https://wandb.ai/rahaf-aswad-hochschule-hannover/my-language-model/runs/cgjd6u1x</a><br> View project at: <a href='https://wandb.ai/rahaf-aswad-hochschule-hannover/my-language-model' target=\"_blank\">https://wandb.ai/rahaf-aswad-hochschule-hannover/my-language-model</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250427_225607-cgjd6u1x\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\hshakademie3\\Desktop\\GenerativeAI-Project\\GenerativeAI-Project\\Mini-GenModel\\wandb\\run-20250427_225608-rp3wqbqj</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/rahaf-aswad-hochschule-hannover/my-language-model/runs/rp3wqbqj' target=\"_blank\">dazzling-shape-70</a></strong> to <a href='https://wandb.ai/rahaf-aswad-hochschule-hannover/my-language-model' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/rahaf-aswad-hochschule-hannover/my-language-model' target=\"_blank\">https://wandb.ai/rahaf-aswad-hochschule-hannover/my-language-model</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/rahaf-aswad-hochschule-hannover/my-language-model/runs/rp3wqbqj' target=\"_blank\">https://wandb.ai/rahaf-aswad-hochschule-hannover/my-language-model/runs/rp3wqbqj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5 - Loss: 8.6742\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hshakademie3\\AppData\\Local\\Temp\\ipykernel_15396\\3754010258.py:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(x), torch.tensor(y)\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁</td></tr><tr><td>train_loss</td><td>▁</td></tr><tr><td>val_loss</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>2</td></tr><tr><td>train_loss</td><td>8.73024</td></tr><tr><td>val_loss</td><td>8.62052</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">dazzling-shape-70</strong> at: <a href='https://wandb.ai/rahaf-aswad-hochschule-hannover/my-language-model/runs/rp3wqbqj' target=\"_blank\">https://wandb.ai/rahaf-aswad-hochschule-hannover/my-language-model/runs/rp3wqbqj</a><br> View project at: <a href='https://wandb.ai/rahaf-aswad-hochschule-hannover/my-language-model' target=\"_blank\">https://wandb.ai/rahaf-aswad-hochschule-hannover/my-language-model</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250427_225608-rp3wqbqj\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\hshakademie3\\Desktop\\GenerativeAI-Project\\GenerativeAI-Project\\Mini-GenModel\\wandb\\run-20250427_225611-vxxy7eim</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/rahaf-aswad-hochschule-hannover/my-language-model/runs/vxxy7eim' target=\"_blank\">good-elevator-71</a></strong> to <a href='https://wandb.ai/rahaf-aswad-hochschule-hannover/my-language-model' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/rahaf-aswad-hochschule-hannover/my-language-model' target=\"_blank\">https://wandb.ai/rahaf-aswad-hochschule-hannover/my-language-model</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/rahaf-aswad-hochschule-hannover/my-language-model/runs/vxxy7eim' target=\"_blank\">https://wandb.ai/rahaf-aswad-hochschule-hannover/my-language-model/runs/vxxy7eim</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/5 - Loss: 8.5756\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hshakademie3\\AppData\\Local\\Temp\\ipykernel_15396\\3754010258.py:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(x), torch.tensor(y)\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁</td></tr><tr><td>train_loss</td><td>▁</td></tr><tr><td>val_loss</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>3</td></tr><tr><td>train_loss</td><td>8.67421</td></tr><tr><td>val_loss</td><td>8.5209</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">good-elevator-71</strong> at: <a href='https://wandb.ai/rahaf-aswad-hochschule-hannover/my-language-model/runs/vxxy7eim' target=\"_blank\">https://wandb.ai/rahaf-aswad-hochschule-hannover/my-language-model/runs/vxxy7eim</a><br> View project at: <a href='https://wandb.ai/rahaf-aswad-hochschule-hannover/my-language-model' target=\"_blank\">https://wandb.ai/rahaf-aswad-hochschule-hannover/my-language-model</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250427_225611-vxxy7eim\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\hshakademie3\\Desktop\\GenerativeAI-Project\\GenerativeAI-Project\\Mini-GenModel\\wandb\\run-20250427_225613-4zv3sans</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/rahaf-aswad-hochschule-hannover/my-language-model/runs/4zv3sans' target=\"_blank\">olive-disco-72</a></strong> to <a href='https://wandb.ai/rahaf-aswad-hochschule-hannover/my-language-model' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/rahaf-aswad-hochschule-hannover/my-language-model' target=\"_blank\">https://wandb.ai/rahaf-aswad-hochschule-hannover/my-language-model</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/rahaf-aswad-hochschule-hannover/my-language-model/runs/4zv3sans' target=\"_blank\">https://wandb.ai/rahaf-aswad-hochschule-hannover/my-language-model/runs/4zv3sans</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5 - Loss: 8.4837\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hshakademie3\\AppData\\Local\\Temp\\ipykernel_15396\\3754010258.py:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(x), torch.tensor(y)\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁</td></tr><tr><td>train_loss</td><td>▁</td></tr><tr><td>val_loss</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>4</td></tr><tr><td>train_loss</td><td>8.57564</td></tr><tr><td>val_loss</td><td>8.44951</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">olive-disco-72</strong> at: <a href='https://wandb.ai/rahaf-aswad-hochschule-hannover/my-language-model/runs/4zv3sans' target=\"_blank\">https://wandb.ai/rahaf-aswad-hochschule-hannover/my-language-model/runs/4zv3sans</a><br> View project at: <a href='https://wandb.ai/rahaf-aswad-hochschule-hannover/my-language-model' target=\"_blank\">https://wandb.ai/rahaf-aswad-hochschule-hannover/my-language-model</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250427_225613-4zv3sans\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\hshakademie3\\Desktop\\GenerativeAI-Project\\GenerativeAI-Project\\Mini-GenModel\\wandb\\run-20250427_225615-nxtcjcq5</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/rahaf-aswad-hochschule-hannover/my-language-model/runs/nxtcjcq5' target=\"_blank\">swift-dream-73</a></strong> to <a href='https://wandb.ai/rahaf-aswad-hochschule-hannover/my-language-model' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/rahaf-aswad-hochschule-hannover/my-language-model' target=\"_blank\">https://wandb.ai/rahaf-aswad-hochschule-hannover/my-language-model</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/rahaf-aswad-hochschule-hannover/my-language-model/runs/nxtcjcq5' target=\"_blank\">https://wandb.ai/rahaf-aswad-hochschule-hannover/my-language-model/runs/nxtcjcq5</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hshakademie3\\AppData\\Local\\Temp\\ipykernel_15396\\3754010258.py:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(x), torch.tensor(y)\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁</td></tr><tr><td>train_loss</td><td>▁</td></tr><tr><td>val_loss</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>5</td></tr><tr><td>train_loss</td><td>8.48365</td></tr><tr><td>val_loss</td><td>8.35677</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">swift-dream-73</strong> at: <a href='https://wandb.ai/rahaf-aswad-hochschule-hannover/my-language-model/runs/nxtcjcq5' target=\"_blank\">https://wandb.ai/rahaf-aswad-hochschule-hannover/my-language-model/runs/nxtcjcq5</a><br> View project at: <a href='https://wandb.ai/rahaf-aswad-hochschule-hannover/my-language-model' target=\"_blank\">https://wandb.ai/rahaf-aswad-hochschule-hannover/my-language-model</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250427_225615-nxtcjcq5\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "#🔁 Training\n",
    "for epoch in range(config[\"epochs\"]):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for x, y in dataloader:\n",
    "        x, y = x.to(config[\"device\"]), y.to(config[\"device\"])\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(x)\n",
    "        loss = loss_fn(logits.view(-1, config[\"vocab_size\"]), y.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    print(f\"Epoch {epoch+1}/{config['epochs']} - Loss: {avg_loss:.4f}\")\n",
    "    wandb.init(project=\"my-language-model\", config=config)\n",
    "    wandb.log({\"train_loss\": avg_loss, \"epoch\": epoch+1})\n",
    "    \n",
    "    # 🔍 Performance Evaluation  تقييم الأداء\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_loss = 0\n",
    "        for x, y in dataloader:\n",
    "            x, y = x.to(config[\"device\"]), y.to(config[\"device\"])\n",
    "            logits = model(x)\n",
    "            loss = loss_fn(logits.view(-1, config[\"vocab_size\"]), y.view(-1))\n",
    "            val_loss += loss.item()\n",
    "        val_loss /= len(dataloader)\n",
    "        wandb.log({\"val_loss\": val_loss, \"epoch\": epoch+1})\n",
    "        \n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n"
     ]
    }
   ],
   "source": [
    "# 🧠Generate text \n",
    "\n",
    "def generate_text(model, tokenizer, prompt, max_length=50):\n",
    "    model.eval()\n",
    "    tokens = tokenizer.encode(prompt, return_tensors=\"pt\").to(config[\"device\"])\n",
    "    generated = tokens\n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_length):\n",
    "            if generated.size(1) > config[\"block_size\"]:\n",
    "                generated = generated[:, -config[\"block_size\"]:]\n",
    "            output = model(generated)\n",
    "            next_token_logits = output[:, -1, :]\n",
    "            next_token = torch.argmax(next_token_logits, dim=-1).unsqueeze(0)\n",
    "            generated = torch.cat((generated, next_token), dim=1)\n",
    "    result = tokenizer.decode(generated[0], skip_special_tokens=True)\n",
    "    return result\n",
    "print(generate_text(model, tokenizer, \"The king said\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No files have been modified since last commit. Skipping to prevent empty commit.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/rahaf-aswad/Mini-GenModel/commit/cfe89674e64786fae3a36e210ec66e1ec8d17051', commit_message='Upload folder using huggingface_hub', commit_description='', oid='cfe89674e64786fae3a36e210ec66e1ec8d17051', pr_url=None, repo_url=RepoUrl('https://huggingface.co/rahaf-aswad/Mini-GenModel', endpoint='https://huggingface.co', repo_type='model', repo_id='rahaf-aswad/Mini-GenModel'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# 💾 Save-Upload \n",
    "repo_name = \"Mini-GenModel\"\n",
    "os.makedirs(repo_name, exist_ok=True)\n",
    "torch.save(model.state_dict(), f\"{repo_name}/pytorch_model.bin\")\n",
    "tokenizer.save_pretrained(repo_name)\n",
    "with open(f\"{repo_name}/config.json\", \"w\") as f:\n",
    "    json.dump(config, f)\n",
    "\n",
    "upload_folder(folder_path=repo_name, repo_id=f\"rahaf-aswad/{repo_name}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
