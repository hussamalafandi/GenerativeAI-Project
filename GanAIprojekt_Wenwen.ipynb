{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7vH4oiL4uF_t",
        "outputId": "ae6a32e0-3a4d-44ee-f9c8-f559c596d31b"
      },
      "outputs": [],
      "source": [
        "# Importing libraries and settings\n",
        "import torch  # PyTorch is a Python library for Machine Learning and Deep Learning.\n",
        "import torch.nn as nn   # torch.nn: Building blocks for neural networks\n",
        "import torch.nn.functional as F   \n",
        "import torch.optim as optim   # torch.optim: Optimizer (for example, to reduce Loss)\n",
        "\n",
        "from transformers import AutoTokenizer\n",
        "# transformers transformers is an open source AI library developed by Firma Hugging Face.\n",
        "# This library is one of the most commonly used tools for NLP (Natural Language Processing)\n",
        "# AutoTokenizer is a tool that automatically loads a \"tokenizer\".\n",
        "# Turn human language into a “list of numbers” so that the AI model can understand.\n",
        "\n",
        "from datasets import load_dataset\n",
        "# From the datasets library, import a function called load_dataset. \n",
        "# This function is used to Help you quickly download and load open source datasets for training or testing AI models.\n",
        "\n",
        "from torch.optim import Adam\n",
        "\n",
        "import wandb \n",
        "# wandb (Weights & Biases) is a tool for recording, tracking, and visualizing the machine learning training process.\n",
        "\n",
        "import os \n",
        "# os= Control and Read operating system: File operations, Path processing, Environment variable management.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N2lUzQNY60qr",
        "outputId": "66ac9553-0ee8-4507-c860-f39f754afa9f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (301966 > 1024). Running this sequence through the model will result in indexing errors\n"
          ]
        }
      ],
      "source": [
        "# Set some global variables\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "BATCH_SIZE = 32    # The number of training samples per batch.\n",
        "SEQ_LEN = 64       # The length of each token, which <= 1024 in GPT2.\n",
        "EPOCHS = 5\n",
        "\n",
        "# Using GPT2 tokenizer and Tiny Shakespeare text\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "tokenizer.pad_token = tokenizer.eos_token  # GPT2 does not have pad_token, use eos_token instead\n",
        "\n",
        "# load Tiny Shakespeare Dataset\n",
        "dataset = load_dataset(\"tiny_shakespeare\", trust_remote_code=True ) # allow these codes (custom loading logic) to run\n",
        "text = dataset[\"train\"][0][\"text\"]  # Get the original text string\n",
        "\n",
        "# ==== encode into \"token\" ====\n",
        "tokens = tokenizer.encode(text, return_tensors=\"pt\").squeeze()\n",
        "tokens = tokens[:1024]  # Limit the length, GPT2 only accepts a maximum of 1024 tokens\n",
        "\n",
        "# Split these tokens into fixed-length input-output pairs as training samples\n",
        "# \"make_chunks\" is a function to cut the tokens (integer form) of a whole paragraph- \n",
        "# -into small training samples (chunks), for language model input (reading) and target (prediction)\n",
        "def make_chunks(tokens, seq_len):\n",
        "    inputs = []\n",
        "    targets = []\n",
        "    for i in range(0, len(tokens) - seq_len):\n",
        "        inputs.append(tokens[i:i+seq_len])  \n",
        "        # Input: the first N words seen by the model\n",
        "        targets.append(tokens[i+1:i+seq_len+1])  \n",
        "        # Output (target): The next word that the model is going to predict\n",
        "    return torch.stack(inputs), torch.stack(targets) \n",
        "    # Convert these sliced ​​fragments into tensor format for model training.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "input_ids, labels = make_chunks(tokens, SEQ_LEN)\n",
        "# Cut a whole token sequence (tokens) into multiple short input sequences (input_ids) and target output (labels). \n",
        "# to train the language model, to let the model learn to predict the following words from the previous ones\n",
        "\n",
        "# establish DataLoader: make the prepared training data into a form that can be fed to the model in batches\n",
        "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
        "# TensorDataset: Pack two Tensors (input_ids, labels) into a dataset\n",
        "# DataLoader: Loading data in batches\n",
        "# random_split: randomly divide the data into two parts (training and verification)\n",
        "\n",
        "dataset = TensorDataset(input_ids, labels) \n",
        "# Create the dataset. Each pair of (input, label) is a training sample\n",
        "\n",
        "train_size = int(0.9 * len(dataset))  \n",
        "val_size = len(dataset) - train_size\n",
        "# 90% of the samples for training and 10% for validation\n",
        "\n",
        "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
        "# \"random_split\" will randomly separate the two data sets to ensure that the verification results are fair\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
        "# The function of DataLoader is to:\n",
        "# -Cut the data into batches (e.g. BATCH_SIZE = 16)\n",
        "# -Return (inputs, labels) for each batch\n",
        "# -Automatically shuffle the order during training (shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "ClfemNza6muS"
      },
      "outputs": [],
      "source": [
        "# Decoder-only\n",
        "class MiniDecoderModel(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model=128, nhead=4, num_layers=2):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "        decoder_layer = nn.TransformerDecoderLayer(d_model, nhead)\n",
        "        self.decoder = nn.TransformerDecoder(decoder_layer, num_layers)\n",
        "        self.fc_out = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)  # [batch_size, seq_len, d_model]\n",
        "        x = x.permute(1, 0, 2)  # Transformer expects: [seq_len, batch_size, d_model]\n",
        "        tgt_mask = nn.Transformer.generate_square_subsequent_mask(x.size(0)).to(x.device)\n",
        "        output = self.decoder(x, x, tgt_mask=tgt_mask)\n",
        "        output = output.permute(1, 0, 2)  # Back to [batch_size, seq_len, d_model]\n",
        "        return self.fc_out(output)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407
        },
        "id": "dEgToPfF7xJc",
        "outputId": "0e0ef3fc-119b-47ba-a0f6-4a19a354d4ea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5 - Train Loss: 10.0167 - Val Loss: 9.3827\n",
            "Epoch 2/5 - Train Loss: 8.9734 - Val Loss: 8.5459\n",
            "Epoch 3/5 - Train Loss: 8.1644 - Val Loss: 7.7465\n",
            "Epoch 4/5 - Train Loss: 7.3912 - Val Loss: 6.9782\n",
            "Epoch 5/5 - Train Loss: 6.6558 - Val Loss: 6.2553\n"
          ]
        }
      ],
      "source": [
        "# Train the model and log it with wandb\n",
        "# Initialize wandb:\n",
        "wandb.init(project=\"tiny-transformer\", name=\"decoder-from-scratch\")\n",
        "\n",
        "# Models and Optimizers\n",
        "model = MiniDecoderModel(vocab_size=tokenizer.vocab_size).to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(EPOCHS):\n",
        "    model.train()\n",
        "    total_train_loss = 0\n",
        "\n",
        "    for x, y in train_loader:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        logits = model(x)\n",
        "        loss = loss_fn(logits.view(-1, logits.size(-1)), y.view(-1))\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_train_loss += loss.item()\n",
        "\n",
        "    avg_train_loss = total_train_loss / len(train_loader)\n",
        "\n",
        "    # validation\n",
        "    model.eval()\n",
        "    total_val_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for x, y in val_loader:\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            logits = model(x)\n",
        "            loss = loss_fn(logits.view(-1, logits.size(-1)), y.view(-1))\n",
        "            total_val_loss += loss.item()\n",
        "\n",
        "    avg_val_loss = total_val_loss / len(val_loader)\n",
        "\n",
        "    wandb.log({\n",
        "        \"epoch\": epoch,\n",
        "        \"train_loss\": avg_train_loss,\n",
        "        \"val_loss\": avg_val_loss\n",
        "    })\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{EPOCHS} - Train Loss: {avg_train_loss:.4f} - Val Loss: {avg_val_loss:.4f}\")\n",
        "\n",
        "# Result:\n",
        "# Epoch 1/5 - Train Loss: 10.0167 - Val Loss: 9.3827\n",
        "# Epoch 2/5 - Train Loss: 8.9734 - Val Loss: 8.5459\n",
        "# Epoch 3/5 - Train Loss: 8.1644 - Val Loss: 7.7465\n",
        "# Epoch 4/5 - Train Loss: 7.3912 - Val Loss: 6.9782\n",
        "# Epoch 5/5 - Train Loss: 6.6558 - Val Loss: 6.2553\n",
        "# This is good. it shows that the model is learning and \n",
        "# -its performance is improving both on the training data and on unseen validation data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DDReWI_M9KH2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "CommitInfo(commit_url='https://huggingface.co/WenWebProjekt/mini-decoder/commit/cb8823f0ebecdbbb69ce678e13fd1d7063042b59', commit_message='Upload folder using huggingface_hub', commit_description='', oid='cb8823f0ebecdbbb69ce678e13fd1d7063042b59', pr_url=None, repo_url=RepoUrl('https://huggingface.co/WenWebProjekt/mini-decoder', endpoint='https://huggingface.co', repo_type='model', repo_id='WenWebProjekt/mini-decoder'), pr_revision=None, pr_num=None)"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Upload the model to Hugging Face Hub\n",
        "# log in \n",
        "from huggingface_hub import login\n",
        "login()  # 输入你在 Hugging Face 网站上的 token\n",
        "\n",
        "# Define the directory where the model will be saved locally\n",
        "model_dir = \"mini_decoder_model\"\n",
        "\n",
        "# Save the model weights and tokenizer\n",
        "torch.save(model.state_dict(), \"mini_decoder_model.pth\")\n",
        "tokenizer.save_pretrained(\"mini_decoder_model\")\n",
        "\n",
        "# Upload the Model to Hugging Face\n",
        "from huggingface_hub import HfApi\n",
        "\n",
        "# Instantiate the Hugging Face API object\n",
        "api = HfApi()\n",
        "\n",
        "# Create a new repository or use an existing one\n",
        "api.create_repo(repo_id=\"WenWebProjekt/mini-decoder\", exist_ok=True)\n",
        "\n",
        "# Upload the model directory to the repository\n",
        "api.upload_folder(\n",
        "    repo_id=\"WenWebProjekt/mini-decoder\",\n",
        "    folder_path=\"mini_decoder_model\"     # Path to the local directory containing the model and tokenizer\n",
        ")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
