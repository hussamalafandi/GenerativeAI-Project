{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f29c9145",
   "metadata": {},
   "source": [
    "### Import der wichtigsten Bibliotheken für Modelltraining und Monitoring \n",
    "In diesem Abschnitt werden PyTorch-Module für neuronale Netzwerke und Datenverarbeitung, Hugging Face-Tools für Tokenisierung und das Laden von Datensätzen sowie Weights & Biases (wandb) für das Tracking und die Visualisierung von Experimenten importiert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d153247e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import GPT2Tokenizer\n",
    "from datasets import load_dataset\n",
    "import wandb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b84f674",
   "metadata": {},
   "source": [
    "### Laden des Wikitext-2-Datensatzes\n",
    "Hier wird der Wikitext-2-raw-v1-Datensatz mit Hugging Face's load_dataset-Funktion geladen, um ihn für das Training oder die Evaluierung eines Modells vorzubereiten.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f2149e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74bc823a",
   "metadata": {},
   "source": [
    "### Initialisierung und Anpassung des GPT-2-Tokenizers \n",
    "Der vortrainierte GPT-2-Tokenizer wird geladen und das Padding-Token wird auf das End-of-Sequence-Token (eos_token) gesetzt, um die Konsistenz bei der Textverarbeitung sicherzustellen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b0c6d5d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token  # Ensure EOS is used as pad\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b77deef7",
   "metadata": {},
   "source": [
    "### Tokenisierung und Vorbereitung der Trainings- und Validierungsdaten\n",
    "Eine Tokenisierungsfunktion wird definiert, die Texte auf eine maximale Länge von 256 Tokens zuschneidet und auffüllt. Anschließend werden der Trainings- und Validierungsdatensatz mithilfe dieser Funktion verarbeitet und die ursprüngliche Textspalte entfernt.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ec14f971",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=256)\n",
    "\n",
    "train_dataset = dataset[\"train\"].map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
    "val_dataset = dataset[\"validation\"].map(tokenize_function, batched=True, remove_columns=[\"text\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29d0a1c4",
   "metadata": {},
   "source": [
    "### Formatierung der Datensätze für PyTorch\n",
    "Die Trainings- und Validierungsdatensätze werden so formatiert, dass nur die input_ids im PyTorch-Format ausgegeben werden – ideal für das direkte Training mit Modellen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "88fab4ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.set_format(type=\"torch\", columns=[\"input_ids\"])\n",
    "val_dataset.set_format(type=\"torch\", columns=[\"input_ids\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe95df6",
   "metadata": {},
   "source": [
    "### Definition der Positional Encoding Klasse\n",
    "Diese Klasse implementiert Positional Encoding, um Positionsinformationen durch Sinus- und Kosinusfunktionen zu den Eingabe-Embeddings hinzuzufügen – ein essenzieller Bestandteil bei der Verarbeitung von Sequenzdaten in Transformermodellen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f3fa82bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=512):\n",
    "        super().__init__()\n",
    "        self.encoding = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1).float()\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(torch.log(torch.tensor(10000.0)) / d_model))\n",
    "        self.encoding[:, 0::2] = torch.sin(position * div_term)\n",
    "        self.encoding[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.encoding = self.encoding.unsqueeze(0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.encoding[:, :x.size(1)].to(x.device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "571d9c67",
   "metadata": {},
   "source": [
    "### Implementierung eines einfachen Transformer-Decodermodells\n",
    "Diese Klasse definiert ein Transformer-basiertes Decoder-Modell mit Embedding-, Positionskodierungs- und Decoder-Schichten. Sie verarbeitet Eingabesequenzen autoregressiv und projiziert die Ausgaben zurück auf den Wortschatzraum für Aufgaben wie Sprachmodellierung.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c728323d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTransformerDecoderModel(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model=128, nhead=4, num_layers=2, max_seq_len=256):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_encoding = PositionalEncoding(d_model, max_len=max_seq_len)\n",
    "        \n",
    "        decoder_layer = nn.TransformerDecoderLayer(d_model=d_model, nhead=nhead)\n",
    "        self.transformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_layers)\n",
    "        \n",
    "        self.output_layer = nn.Linear(d_model, vocab_size)\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def generate_square_subsequent_mask(self, sz):\n",
    "        return torch.triu(torch.full((sz, sz), float('-inf')), diagonal=1)\n",
    "\n",
    "    def forward(self, tgt_ids):\n",
    "        \"\"\"\n",
    "        tgt_ids: [batch_size, seq_len] - token ids\n",
    "        \"\"\"\n",
    "        device = tgt_ids.device\n",
    "        x = self.embedding(tgt_ids) * (self.d_model ** 0.5)  # scale embeddings\n",
    "        x = self.pos_encoding(x).transpose(0, 1)  # [seq_len, batch_size, d_model]\n",
    "\n",
    "        # Causal mask\n",
    "        seq_len = x.size(0)\n",
    "        tgt_mask = self.generate_square_subsequent_mask(seq_len).to(device)\n",
    "\n",
    "        # Fake memory — just pass zeros to satisfy TransformerDecoder API\n",
    "        memory = torch.zeros_like(x)\n",
    "\n",
    "        output = self.transformer_decoder(tgt=x, memory=memory, tgt_mask=tgt_mask)\n",
    "        output = output.transpose(0, 1)  # [batch_size, seq_len, d_model]\n",
    "        return self.output_layer(output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c60504",
   "metadata": {},
   "source": [
    "### Trainingsschleife für das Transformer-Decodermodell\n",
    "Diese Funktion führt das Training des Modells durch, indem sie Eingabesequenzen vorbereitet, Vorhersagen erzeugt, den Verlust berechnet, Gradienten zurückpropagiert und die Modellparameter optimiert.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9912db53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader, optimizer, criterion):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in dataloader:\n",
    "        input_ids = batch[\"input_ids\"]\n",
    "        inputs = input_ids[:, :-1]\n",
    "        targets = input_ids[:, 1:]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(inputs)  # Pass only inputs\n",
    "        loss = criterion(output.view(-1, output.size(-1)), targets.reshape(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(dataloader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b559f31",
   "metadata": {},
   "source": [
    "### Evaluierungsfunktion für das Transformer-Decodermodell\n",
    "Diese Funktion bewertet das Modell auf Validierungsdaten, indem sie den Verlust ohne Gradientenberechnung ermittelt, um die Trainingsqualität objektiv zu überprüfen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aabdac79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, dataloader, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch[\"input_ids\"]\n",
    "            inputs = input_ids[:, :-1]\n",
    "            targets = input_ids[:, 1:]\n",
    "\n",
    "            output = model(inputs)  # Only inputs\n",
    "            loss = criterion(output.view(-1, output.size(-1)), targets.reshape(-1))\n",
    "            total_loss += loss.item()\n",
    "    return total_loss / len(dataloader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9389c274",
   "metadata": {},
   "source": [
    "### Initialisierung von Weights & Biases für Experiment-Tracking\n",
    "Mit wandb.login() wird die Verbindung zu Weights & Biases hergestellt. Danach wird die Konfiguration für das Experiment festgelegt, einschließlich Hyperparametern und Modellarchitektur. wandb.init() startet das Tracking des Trainingsprozesses unter dem Projekt \"language-model\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "af551924",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mm_reza\u001b[0m (\u001b[33mm_reza-hochschule-hannover\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "creating run (0.0s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\Reza\\Desktop\\new proj\\wandb\\run-20250426_131518-g0ya1anh</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/m_reza-hochschule-hannover/language-model/runs/g0ya1anh' target=\"_blank\">glamorous-sea-14</a></strong> to <a href='https://wandb.ai/m_reza-hochschule-hannover/language-model' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/m_reza-hochschule-hannover/language-model' target=\"_blank\">https://wandb.ai/m_reza-hochschule-hannover/language-model</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/m_reza-hochschule-hannover/language-model/runs/g0ya1anh' target=\"_blank\">https://wandb.ai/m_reza-hochschule-hannover/language-model/runs/g0ya1anh</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/m_reza-hochschule-hannover/language-model/runs/g0ya1anh?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x1c079f21ad0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login()  # Only required once per session/machine\n",
    "\n",
    "config = {\n",
    "    \"epochs\": 5,\n",
    "    \"batch_size\": 4,\n",
    "    \"learning_rate\": 5e-4,\n",
    "    \"architecture\": \"TransformerDecoder\",\n",
    "    \"dataset\": \"WikiText-2\",\n",
    "    \"vocab_size\": len(tokenizer),\n",
    "    \"embedding_dim\": 128,\n",
    "    \"nhead\": 4,\n",
    "    \"num_layers\": 2,\n",
    "    \"max_seq_len\": 256\n",
    "}\n",
    "\n",
    "wandb.init(project=\"language-model\", config=config)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b63f0083",
   "metadata": {},
   "source": [
    "### Modellinitialisierung und Setup von Optimizer und Dataloader\n",
    "Ein SimpleTransformerDecoderModel wird mit den aus Weights & Biases (wandb.config) entnommenen Hyperparametern erstellt. Der Adam-Optimizer und die Kreuzentropie-Verlustfunktion werden festgelegt. Zudem werden Dataloader für Training und Validierung vorbereitet, um die Daten in Batches zu laden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "92267849",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleTransformerDecoderModel(\n",
    "    vocab_size=wandb.config.vocab_size,\n",
    "    d_model=wandb.config.embedding_dim,\n",
    "    nhead=wandb.config.nhead,\n",
    "    num_layers=wandb.config.num_layers,\n",
    "    max_seq_len=wandb.config.max_seq_len\n",
    ")\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=wandb.config.learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=wandb.config.batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=wandb.config.batch_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9466fda",
   "metadata": {},
   "source": [
    "### Training und Evaluierung über Epochen mit Weights & Biases Logging\n",
    "Für jede Epoche wird das Modell trainiert und evaluiert. Der Trainings- und Validierungsverlust wird berechnet und ausgedruckt. Diese Werte werden dann zu Weights & Biases geloggt, um den Fortschritt des Experiments zu überwachen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4e512745",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "Train Loss: 1.5456 | Val Loss: 1.5019\n",
      "Epoch 2\n",
      "Train Loss: 1.3494 | Val Loss: 1.4632\n",
      "Epoch 3\n",
      "Train Loss: 1.2790 | Val Loss: 1.4399\n",
      "Epoch 4\n",
      "Train Loss: 1.2364 | Val Loss: 1.4298\n",
      "Epoch 5\n",
      "Train Loss: 1.2056 | Val Loss: 1.4304\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(wandb.config.epochs):\n",
    "    print(f\"Epoch {epoch + 1}\")\n",
    "    train_loss = train(model, train_loader, optimizer, criterion)\n",
    "    val_loss = evaluate(model, val_loader, criterion)\n",
    "\n",
    "    print(f\"Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "    \n",
    "    wandb.log({\n",
    "        \"epoch\": epoch + 1,\n",
    "        \"train_loss\": train_loss,\n",
    "        \"val_loss\": val_loss\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1eaca17",
   "metadata": {},
   "source": [
    "### Speichern des Modells und Tokenizers\n",
    "Das Modell wird in einem angegebenen Verzeichnis gespeichert, indem die Gewichtungen mit torch.save gesichert werden. Zudem wird der Tokenizer im gleichen Verzeichnis mit save_pretrained abgelegt, um das Modell später wieder zu laden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "506e51fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./simple_transformer_model\\\\tokenizer_config.json',\n",
       " './simple_transformer_model\\\\special_tokens_map.json',\n",
       " './simple_transformer_model\\\\vocab.json',\n",
       " './simple_transformer_model\\\\merges.txt',\n",
       " './simple_transformer_model\\\\added_tokens.json')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "# Create directory if it doesn't exist\n",
    "model_save_path = \"./simple_transformer_model\"\n",
    "os.makedirs(model_save_path, exist_ok=True)\n",
    "\n",
    "# Save model weights\n",
    "torch.save(model.state_dict(), model_save_path + \"/pytorch_model.bin\")\n",
    "\n",
    "# Save tokenizer\n",
    "tokenizer.save_pretrained(model_save_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb33eba",
   "metadata": {},
   "source": [
    "### Laden des gespeicherten Modells und Setzen auf Evaluierungsmodus\n",
    "Das Modell wird mit der gleichen Architektur neu erstellt und die gespeicherten Gewichtungen werden mit load_state_dict geladen. Anschließend wird das Modell in den Evaluierungsmodus versetzt, um Vorhersagen zu treffen.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fbba29f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Reza\\AppData\\Local\\Temp\\ipykernel_9104\\2177450218.py:11: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_save_path + \"/pytorch_model.bin\"))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SimpleTransformerDecoderModel(\n",
       "  (embedding): Embedding(50257, 128)\n",
       "  (pos_encoding): PositionalEncoding()\n",
       "  (transformer_decoder): TransformerDecoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-1): 2 x TransformerDecoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (multihead_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=128, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=2048, out_features=128, bias=True)\n",
       "        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm3): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        (dropout3): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (output_layer): Linear(in_features=128, out_features=50257, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Rebuild the model class\n",
    "model = SimpleTransformerDecoderModel(\n",
    "    vocab_size=len(tokenizer),\n",
    "    d_model=128,\n",
    "    nhead=4,\n",
    "    num_layers=2,\n",
    "    max_seq_len=256\n",
    ")\n",
    "\n",
    "# Load weights\n",
    "model.load_state_dict(torch.load(model_save_path + \"/pytorch_model.bin\"))\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d7a598b",
   "metadata": {},
   "source": [
    "### Textgenerierung mit dem Transformer-Modell\n",
    "Diese Funktion verwendet das Modell, um Text basierend auf einem Eingabe-Prompt zu generieren. Sie tokenisiert den Prompt, verwendet das Modell zur Vorhersage der nächsten Tokens und dekodiert schließlich die generierten Tokens zurück in lesbaren Text.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5970681a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time , the game 's first game 's first game 's first game 's first game , and the game 's first game 's first game 's first game . The game 's first game 's first game 's first game 's first game 's first game 's first game 's first game 's first game 's first game 's first game , and the game 's first game 's first game 's first game 's first game 's\n"
     ]
    }
   ],
   "source": [
    "def generate_text(model, tokenizer, prompt, max_length=100):\n",
    "    model.eval()\n",
    "    # Tokenize the prompt text\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors='pt')\n",
    "\n",
    "    # Generate text (we only use `tgt` part, no memory required for decoding)\n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_length):\n",
    "            output = model(input_ids)\n",
    "            next_token_logits = output[:, -1, :]\n",
    "            \n",
    "            # Sample from the distribution of possible next tokens\n",
    "            next_token_id = torch.argmax(next_token_logits, dim=-1).unsqueeze(-1)\n",
    "            input_ids = torch.cat((input_ids, next_token_id), dim=-1)\n",
    "    \n",
    "    # Decode the generated tokens\n",
    "    generated_text = tokenizer.decode(input_ids[0], skip_special_tokens=True)\n",
    "    return generated_text\n",
    "\n",
    "# Example usage:\n",
    "prompt = \"Once upon a time\"\n",
    "generated_text = generate_text(model, tokenizer, prompt)\n",
    "print(generated_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b29c7bcd",
   "metadata": {},
   "source": [
    "### Authentifizierung bei Hugging Face Hub\n",
    "Mit der login()-Funktion von Hugging Face wird der Benutzer mit einem API-Token authentifiziert, um auf Modelle und Datasets vom Hugging Face Hub zuzugreifen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8911a7a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4164a9e3",
   "metadata": {},
   "source": [
    "### Modell-Upload zum Hugging Face Hub\n",
    "Mit der upload_folder-Funktion von Hugging Face wird ein Modellordner (lokal gespeicherte Dateien) auf das Hugging Face Model Hub hochgeladen, sodass das Modell unter dem angegebenen repo_id öffentlich zugänglich gemacht wird.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbfbec0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/mreeza/simple-transformer-model/commit/a69ddb4e27e829d7f8afee2b6d5d95b1017376b8', commit_message='Upload folder using huggingface_hub', commit_description='', oid='a69ddb4e27e829d7f8afee2b6d5d95b1017376b8', pr_url=None, repo_url=RepoUrl('https://huggingface.co/mreeza/simple-transformer-model', endpoint='https://huggingface.co', repo_type='model', repo_id='mreeza/simple-transformer-model'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import HfApi\n",
    "\n",
    "api = HfApi()\n",
    "api.upload_folder(\n",
    "    folder_path=\"./simple_transformer_model\",   # Local model folder\n",
    "    repo_id=\"mreeza/simple-transformer-model\",  # Your model repo ID\n",
    "    repo_type=\"model\"                            # It is a model, not a dataset\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "686fe67a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
