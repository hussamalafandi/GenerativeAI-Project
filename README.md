ðŸ§  MiniGPT: Autoregressive Language Model
=======


ðŸ”— Weitere Links:

https://wandb.ai/altkachenko11-hochschule-hannover/projects
https://huggingface.co/altkachenko11

ðŸ“š Projektbeschreibung

In diesem Projekt habe ich ein kleines autoregressives Decoder-Only Sprachmodell (Ã¤hnlich wie GPT) mit PyTorch gebaut und trainiert.

Ich habe zwei Aufgaben umgesetzt:

Eigenes MiniGPT Modell: selbst gebaut, trainiert und auf Hugging Face hochgeladen.

Feintuning von GPT-2: ein vortrainiertes GPT-2 Modell auf den Tiny Shakespeare Datensatz angepasst.

Das Projekt erfÃ¼llt alle Anforderungen aus dem Kursauftrag.

ðŸš€ Modell 1: Eigenes MiniGPT

âœ¨ Details:

Architektur: Transformer Decoder mit 2 Layern, 4 Attention-Heads

Training: Auf Tiny Shakespeare Datensatz

Tokenizer: GPT-2 Tokenizer (AutoTokenizer)

Optimierer: Adam

Loss Function: Cross Entropy Loss (mit Padding Ignorierung)

Training Loop: komplett von Hand geschrieben

Logging: via Weights & Biases (wandb)

Parameterzahl: < 1 Million (trainierbar auf CPU)

ðŸ”¥ Training:

5 Epochen

Trainings- und Validierungsloss werden nach jeder Epoche geloggt

Loss sichtbar gesunken

ðŸ› ï¸ Wichtige Dateien:

MiniGPT-Modell: altkachenko11/my-mini-gpt

ðŸš€ Modell 2: Feintuning GPT-2

âœ¨ Details:

Basismodell: GPT-2 (gpt2 von Hugging Face)

Training: Auf Shakespeare-DatensÃ¤tzen

Framework: Hugging Face Trainer API

Logging: ebenfalls via wandb

Tokenizer: GPT2Tokenizer

Loss: automatisch durch Trainer API

Callbacks: eigener W&B Callback fÃ¼r Evaluation Loss

ðŸ”¥ Training:

4 Epochen

Evaluation alle 20 Schritte

Verlust sinkt wÃ¤hrend des Trainings

ðŸ› ï¸ Wichtige Dateien:

Fine-Tuned GPT-2 Modell: altkachenko11/gpt2-finetuned

ðŸ›’ Genutzte Tools:

PyTorch

Hugging Face Transformers

Weights & Biases (wandb)

Datasets: Tiny Shakespeare

ðŸ› ï¸ Beispiel: Textgenerierung mit MiniGPT:

from transformers import AutoTokenizer, AutoModelForCausalLM

tokenizer = AutoTokenizer.from_pretrained("altkachenko11/my-mini-gpt")
model = AutoModelForCausalLM.from_pretrained("altkachenko11/my-mini-gpt")

prompt = "Hallo, mein Name ist"
inputs = tokenizer(prompt, return_tensors="pt")

outputs = model.generate(
    **inputs,
    max_length=50,
    do_sample=True,
    top_k=50,
    temperature=0.9
)

generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
print(generated_text)


ðŸ“¦ VerÃ¶ffentlichung:

Beide Modelle wurden erfolgreich auf den Hugging Face Model Hub hochgeladen.

Model Card und Tags hinzugefÃ¼gt

Modelle sind Ã¶ffentlich verfÃ¼gbar

âœ… ErfÃ¼llt:

Modell bauen

Eigenen Trainingsloop schreiben

Logging mit Weights & Biases

Modell auf Hugging Face hochladen

Pull Request im eigenen Branch erstellen

README.md schreiben

ðŸŽ‰ Vielen Dank fÃ¼rs Anschauen!

=======
=======
>>>>>>> ca60a7dc7597e825a9917490aa61fccc09869101
# ðŸ§  Abschlussprojekt: Entwicklung eines eigenen Sprachmodells
>>>>>>> aed5f8490a98a9cc8a011cfb482eb175b6e99455

- [Hugging Face](https://huggingface.co/altkachenko11)
- [WandB](https://wandb.ai/altkachenko11)

ðŸ“š Projektbeschreibung

In diesem Projekt habe ich ein kleines autoregressives Decoder-Only Sprachmodell mit PyTorch gebaut und trainiert.

Ich habe zwei Aufgaben umgesetzt:

1. Eigenes MiniGPT Modell:  gebaut, trainiert und auf Hugging Face hochgeladen.

2. Feintuning von GPT-2: ein vortrainiertes GPT-2 Modell auf den Tiny Shakespeare Datensatz angepasst.

Das Projekt erfÃ¼llt alle Anforderungen aus dem Kursauftrag.

ðŸš€ Modell 1: Eigenes MiniGPT

âœ¨ Details:

Architektur: Transformer Decoder mit 2 Layern, 4 Attention-Heads nhead=4 â†’ 4 attention-heads

num_layers=2 â†’ 2 Transformer.

Training: Auf Tiny Shakespeare Datensatz

Tokenizer: GPT-2 Tokenizer (AutoTokenizer)

Optimierer: Adam

Loss Function: Cross Entropy Loss (mit Padding Ignorierung)

Training Loop: komplett von Hand geschrieben

Logging: via Weights & Biases (wandb)

Parameterzahl: < 1 Million (trainierbar auf CPU)

ðŸ”¥ Training:

5 Epochen

Trainings- und Validierungsloss werden nach jeder Epoche geloggt

Loss sichtbar gesunken

ðŸ› ï¸ Wichtige Dateien:

MiniGPT-Modell: altkachenko11/my-mini-gpt

ðŸš€ Modell 2: Feintuning GPT-2

âœ¨ Details:

Basismodell: GPT-2 (gpt2 von Hugging Face)

Training: Auf Shakespeare-DatensÃ¤tzen

Framework: Hugging Face Trainer API

Logging: ebenfalls via wandb

Tokenizer: GPT2Tokenizer

Loss: automatisch durch Trainer API

Callbacks: eigener W&B Callback fÃ¼r Evaluation Loss

ðŸ”¥ Training:
4 Epochen

Evaluation alle 20 Schritte

Verlust sinkt wÃ¤hrend des Trainings

ðŸ› ï¸ Wichtige Dateien:
Fine-Tuned GPT-2 Modell: altkachenko11/gpt2-finetuned

ðŸ›’ Genutzte Tools
PyTorch

Hugging Face Transformers

Weights & Biases (wandb)

Datasets: Tiny Shakespeare

ðŸ› ï¸ Beispiel: Textgenerierung mit MiniGPT

from transformers import AutoTokenizer, AutoModelForCausalLM

tokenizer = AutoTokenizer.from_pretrained("altkachenko11/my-mini-gpt")
model = AutoModelForCausalLM.from_pretrained("altkachenko11/my-mini-gpt")

prompt = "Hallo, mein Name ist"
inputs = tokenizer(prompt, return_tensors="pt")

outputs = model.generate(
    **inputs,
    max_length=50,
    do_sample=True,
    top_k=50,
    temperature=0.9
)

generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
print(generated_text)

ðŸ“¦ VerÃ¶ffentlichung
Beide Modelle wurden erfolgreich auf den Hugging Face Model Hub hochgeladen

Model Card und Tags hinzugefÃ¼gt

Modelle sind Ã¶ffentlich verfÃ¼gbar.


âœ… ErfÃ¼llt:
 Modell bauen

 Eigenen Trainingsloop schreiben

 Logging mit Weights & Biases

 Modell auf Hugging Face hochladen

 Pull Request im eigenen Branch erstellen

 README.md schreiben


 ðŸŽ‰ Vielen Dank fÃ¼rs Anschauen!


Viel Erfolg! ðŸš€



