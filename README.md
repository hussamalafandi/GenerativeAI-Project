# Abschlussprojekt: Entwicklung eines eigenen Sprachmodells

Dieses Projekt implementiert ein kleines autoregressives Sprachmodell mit PyTorch, basierend auf dem Decoder-Teil des Transformer-Modells.

## ğŸ“š Datensatz
- [Tiny Shakespeare](https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt)

## ğŸ§  Modellarchitektur
- TransformerDecoder (2 Layer, 4 Heads, d_model=64)
- Positionale Einbettungen
- VokabulargrÃ¶ÃŸe basierend auf GPT2Tokenizer

## ğŸ”§ Training
- 3 Epochen
- Verlustfunktion: CrossEntropy
- Optimierer: Adam (lr=3e-4)
- EingabesequenzlÃ¤nge: 64 Token
- BatchgrÃ¶ÃŸe: 32
- Validierung nach jeder Epoche

## ğŸ“ˆ Logging
Trainings- und Validierungsverluste wurden mit [Weights & Biases (wandb)](https://wandb.ai/) protokolliert.

## â˜ï¸ VerfÃ¼gbarkeit
Das Modell ist auf dem [Hugging Face Hub](https://huggingface.co/kullaniciadi/sprachmodell-final) verfÃ¼gbar.

## â–¶ï¸ AusfÃ¼hrung
```bash
pip install -q transformers datasets wandb
