# ðŸ§  Abschlussprojekt: Entwicklung eines eigenen Sprachmodells

Willkommen zum Abschlussprojekt dieses Kurses! In diesem Projekt setzt du dein Wissen Ã¼ber Sprachmodelle in die Praxis um und entwickelst dein eigenes autoregressives Modell auf Basis von PyTorch. ZusÃ¤tzlich lernst du Tools wie Weights & Biases (wandb) und den Hugging Face Model Hub kennen â€“ genau wie im echten ML-Workflow.

---

## âœ… Projektanforderungen

### 1. Modell
- Erstelle ein **Decoder-only Sprachmodell** mit Modulen aus `torch.nn`.
- Du darfst z.â€¯B. `nn.TransformerDecoder`, `nn.TransformerDecoderLayer` usw. verwenden.
- Das Modell soll autoregressiv funktionieren (wie GPT).

### 2. Tokenizer
- Verwende einen Tokenizer aus der Hugging Face `transformers`-Bibliothek.
- Beispiel: `AutoTokenizer` oder `GPT2Tokenizer`.

### 3. Training
- Trainiere dein Modell fÃ¼r mindestens **3 Epochen** (5 empfohlen).
- Nutze einen kleinen Datensatz wie **Tiny Shakespeare**, **WikiText-2** oder einen eigenen.
- Dein Modell sollte auch auf einer CPU trainierbar sein (< 1 Mio Parameter).
- Schreibe den Trainingsloop komplett selbst in PyTorch (kein `Trainer` verwenden).

### 4. Evaluation
- Berechne nach jeder Epoche den Loss auf einem Validierungsdatensatz.
- Der Loss muss wÃ¤hrend des Trainings **sichtbar sinken**.

### 5. Logging
- Verwende [wandb](https://wandb.ai), um Trainings- und Eval-Loss zu loggen.

### 6. VerÃ¶ffentlichung
- Lade dein Modell am Ende auf den [Hugging Face Model Hub](https://huggingface.co/).
- FÃ¼ge eine kurze Model Card mit Beschreibung und Tags hinzu.

### 7. Abgabe
- Forke dieses Repository.
- Erstelle einen Branch mit deinem Namen, z.â€¯B. `max-mustermann-final`.
- FÃ¼ge deine `.py`-Datei oder dein Jupyter-Notebook sowie eine `README.md` hinzu.
- Erstelle einen Pull Request **bis spÃ¤testens 23:59 Uhr am 25.04.2025**.

---

## ðŸŒŸ Bonus (optional)

Wenn du mÃ¶chtest, kannst du zusÃ¤tzlich ein vortrainiertes Modell wie GPT-2 mithilfe der Hugging Face `transformers`-Bibliothek finetunen:

- Lade ein GPT-2-Modell und den passenden Tokenizer (`GPT2Tokenizer`) mit `from_pretrained`.
- Trainiere es auf deinem Datensatz mit der `Trainer` API.
- Logge mit wandb und lade auch dieses Modell auf Hugging Face hoch.

---

## ðŸ“ Wichtige Hinweise

- Logging mit wandb, das Hochladen auf den Hugging Face Hub und der Pull Request auf GitHub sind **Pflicht**.
- Die ModellqualitÃ¤t ist nicht entscheidend, aber **der Loss muss sinken**.
- Du wirst am **Montag, den 28.04.2025** dein Projekt prÃ¤sentieren und deinen Code erklÃ¤ren.

---

Viel Erfolg! ðŸš€


Ergebnisse

Ein kompakter, autoregressiver Transformer (GPTâ€‘Stil), finetuned auf dem **Tinyâ€‘Shakespeare**â€‘Datensatz.  
Trainiert in 5Â Epochen mit wandbâ€‘Logging.

## Modellâ€‘Links
* ðŸ¤— **HuggingÂ Face Hub**: 	https://huggingface.co/vladimir707/gpt-mini1
Bonusaufgabe 			https://huggingface.co/vladimir707/my-fancy-gpt2
* ðŸ“Š **WeightsÂ &Â Biases Run**: 	https://wandb.ai/vovanew707-hsh/tiny-gpt?nw=nwuservovanew707
* ðŸ“Š **W&BÂ ProjektÃ¼bersicht**: 	https://wandb.ai/vovanew707-hsh/tiny-gpt/runs/5juy23aa?nw=nwuservovanew707

## Architektur
| Hyperparameter | Wert |
|----------------|------|
| Typ            | Decoderâ€‘only Transformer |
| Ebenen         | 2 |
| Heads          | 4 |
| Embeddingâ€‘Dim  | 128 |
| FFâ€‘Dim         | 256 |
| Parameter      | <Â 1Â M |

## Trainingsdetails
* OptimizerÂ &Â LR: **AdamW**,Â 3eâ€‘4  
* Batchâ€‘GrÃ¶ÃŸe: 4  
* Epochen: 5  
* Lossâ€‘Funktion: Crossâ€‘Entropy (languageâ€‘modeling)  
* WandBâ€‘Run: siehe Link oben (Train/Valâ€‘Loss, Beispiele, Generierungen)

## BeispielÂ â€“ Textgenerierung

