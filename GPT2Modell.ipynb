{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea72ae7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\hshakademie9\\anaconda3\\lib\\site-packages (2.6.0)\n",
      "Requirement already satisfied: transformers in c:\\users\\hshakademie9\\anaconda3\\lib\\site-packages (4.51.3)\n",
      "Requirement already satisfied: datasets in c:\\users\\hshakademie9\\anaconda3\\lib\\site-packages (3.5.0)\n",
      "Requirement already satisfied: wandb in c:\\users\\hshakademie9\\anaconda3\\lib\\site-packages (0.19.9)\n",
      "Requirement already satisfied: huggingface_hub in c:\\users\\hshakademie9\\anaconda3\\lib\\site-packages (0.30.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\hshakademie9\\anaconda3\\lib\\site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\hshakademie9\\anaconda3\\lib\\site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\hshakademie9\\anaconda3\\lib\\site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\hshakademie9\\anaconda3\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\hshakademie9\\anaconda3\\lib\\site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\hshakademie9\\anaconda3\\lib\\site-packages (from torch) (75.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\hshakademie9\\anaconda3\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\hshakademie9\\anaconda3\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\hshakademie9\\anaconda3\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\hshakademie9\\anaconda3\\lib\\site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\hshakademie9\\anaconda3\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\hshakademie9\\anaconda3\\lib\\site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in c:\\users\\hshakademie9\\anaconda3\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\hshakademie9\\anaconda3\\lib\\site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\hshakademie9\\anaconda3\\lib\\site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\hshakademie9\\anaconda3\\lib\\site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\hshakademie9\\anaconda3\\lib\\site-packages (from datasets) (16.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\hshakademie9\\anaconda3\\lib\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in c:\\users\\hshakademie9\\anaconda3\\lib\\site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: xxhash in c:\\users\\hshakademie9\\anaconda3\\lib\\site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in c:\\users\\hshakademie9\\anaconda3\\lib\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\hshakademie9\\anaconda3\\lib\\site-packages (from datasets) (3.10.5)\n",
      "Requirement already satisfied: click!=8.0.0,>=7.1 in c:\\users\\hshakademie9\\anaconda3\\lib\\site-packages (from wandb) (8.1.7)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in c:\\users\\hshakademie9\\anaconda3\\lib\\site-packages (from wandb) (0.4.0)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in c:\\users\\hshakademie9\\anaconda3\\lib\\site-packages (from wandb) (3.1.43)\n",
      "Requirement already satisfied: platformdirs in c:\\users\\hshakademie9\\anaconda3\\lib\\site-packages (from wandb) (3.10.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in c:\\users\\hshakademie9\\anaconda3\\lib\\site-packages (from wandb) (4.25.3)\n",
      "Requirement already satisfied: psutil>=5.0.0 in c:\\users\\hshakademie9\\anaconda3\\lib\\site-packages (from wandb) (5.9.0)\n",
      "Requirement already satisfied: pydantic<3 in c:\\users\\hshakademie9\\anaconda3\\lib\\site-packages (from wandb) (2.8.2)\n",
      "Requirement already satisfied: sentry-sdk>=2.0.0 in c:\\users\\hshakademie9\\anaconda3\\lib\\site-packages (from wandb) (2.26.0)\n",
      "Requirement already satisfied: setproctitle in c:\\users\\hshakademie9\\anaconda3\\lib\\site-packages (from wandb) (1.3.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\hshakademie9\\anaconda3\\lib\\site-packages (from click!=8.0.0,>=7.1->wandb) (0.4.6)\n",
      "Requirement already satisfied: six>=1.4.0 in c:\\users\\hshakademie9\\anaconda3\\lib\\site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\hshakademie9\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (2.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\hshakademie9\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\hshakademie9\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\hshakademie9\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\hshakademie9\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\hshakademie9\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.11.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in c:\\users\\hshakademie9\\anaconda3\\lib\\site-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.7)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\hshakademie9\\anaconda3\\lib\\site-packages (from pydantic<3->wandb) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in c:\\users\\hshakademie9\\anaconda3\\lib\\site-packages (from pydantic<3->wandb) (2.20.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\hshakademie9\\anaconda3\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hshakademie9\\anaconda3\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\hshakademie9\\anaconda3\\lib\\site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hshakademie9\\anaconda3\\lib\\site-packages (from requests->transformers) (2025.1.31)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\hshakademie9\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\hshakademie9\\anaconda3\\lib\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\hshakademie9\\anaconda3\\lib\\site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\hshakademie9\\anaconda3\\lib\\site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: smmap<5,>=3.0.1 in c:\\users\\hshakademie9\\anaconda3\\lib\\site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (4.0.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch transformers datasets wandb huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99578584",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2Tokenizer,GPT2LMHeadModel, Trainer, TrainingArguments, TextDataset, TrainerCallback, DataCollatorForLanguageModeling\n",
    "import wandb\n",
    "import os\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b0cd74",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "wandb: Currently logged in as: altkachenko11 (altkachenko11-hochschule-hannover) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\hshakademie9\\Desktop\\Projekt1\\wandb\\wandb\\run-20250428_154930-t88a23o8</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/altkachenko11-hochschule-hannover/gpt2-finetuned/runs/t88a23o8' target=\"_blank\">warm-butterfly-4</a></strong> to <a href='https://wandb.ai/altkachenko11-hochschule-hannover/gpt2-finetuned' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/altkachenko11-hochschule-hannover/gpt2-finetuned' target=\"_blank\">https://wandb.ai/altkachenko11-hochschule-hannover/gpt2-finetuned</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/altkachenko11-hochschule-hannover/gpt2-finetuned/runs/t88a23o8' target=\"_blank\">https://wandb.ai/altkachenko11-hochschule-hannover/gpt2-finetuned/runs/t88a23o8</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# WANDB SETUP\n",
    "os.environ['WANDB_API_KEY'] = \"xxx\"\n",
    "wandb.login()\n",
    "run = wandb.init(\n",
    "    project=\"gpt2-finetuned\",\n",
    "    config={\n",
    "        \"learning_rate\": 1e-5,\n",
    "        \"epochs\": 5,\n",
    "        \"batch_size\": 8\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f9e4121",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Tokenizer und Modell laden\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bb845f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Datensatz vorbereiten (z.B. tiny-shakespeare.txt)\n",
    "# Diese Funktion lädt den Text und wandelt ihn in Token um\n",
    "def load_dataset(file_path, tokenizer, block_size=128):\n",
    "    return TextDataset(\n",
    "        tokenizer=tokenizer,\n",
    "        file_path=file_path,\n",
    "        block_size=block_size\n",
    "    )\n",
    "\n",
    "train_dataset = load_dataset(\"C:/Users/hshakademie9/Desktop/Projekt_Hussam/GenerativeAI-Project/data/shakespeare_train.txt\", tokenizer)\n",
    "val_dataset = load_dataset(\"C:/Users/hshakademie9/Desktop/Projekt_Hussam/GenerativeAI-Project/data/shakespeare_val.txt\", tokenizer)\n",
    "\n",
    "\n",
    "# Trainings- und Validierungsdatensätze laden\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=False,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7d1ff0ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callback für das Logging von eval_loss\n",
    "class LogEvalLossCallback(TrainerCallback):\n",
    "    def on_evaluate(self, args, state, control, metrics, **kwargs):\n",
    "        eval_loss = metrics.get(\"eval_loss\")\n",
    "        if eval_loss is not None:\n",
    "            wandb.log({\"eval_loss\": eval_loss})  # Loggt eval_loss in Wandb\n",
    "\n",
    "# Trainingseinstellungen festlegen\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./gpt2-finetuned\",  # Ausgabeordner\n",
    "    overwrite_output_dir=True,  # Überschreibt den Ausgabeordner\n",
    "    num_train_epochs=4,  # Reduzierte Anzahl an Epochen für kleines Dataset\n",
    "    per_device_train_batch_size=4,  # Batch-Größe für Training\n",
    "    per_device_eval_batch_size=4,  # Batch-Größe für Evaluation\n",
    "    logging_steps=10,  # Wie oft Logs geschrieben werden\n",
    "    save_steps=500,  # Speichert das Modell alle 500 Schritte\n",
    "    eval_steps=20,  # Evaluation alle 20 Schritte\n",
    "    logging_dir=\"./logs\",  # Speicherort der Logs\n",
    "    do_eval=True,  # Evaluation während des Trainings aktivieren\n",
    "    report_to=\"wandb\",   # Logging an Weights & Biases senden\n",
    "    learning_rate=1e-5,  \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d46464bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hshakademie9\\AppData\\Local\\Temp\\ipykernel_24664\\2582585996.py:2: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "# 6. Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    callbacks=[LogEvalLossCallback]  # Callbacks zum Loggen von eval_loss\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "50664073",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='36' max='36' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [36/36 02:09, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>4.575600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>4.423300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>4.339700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=36, training_loss=4.4164808591206866, metrics={'train_runtime': 135.8349, 'train_samples_per_second': 1.001, 'train_steps_per_second': 0.265, 'total_flos': 8883929088000.0, 'train_loss': 4.4164808591206866, 'epoch': 4.0})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()  # Modell trainieren\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3037e52c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5/5 00:03]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 4.199766159057617, 'eval_runtime': 5.6919, 'eval_samples_per_second': 3.162, 'eval_steps_per_second': 0.878, 'epoch': 4.0}\n"
     ]
    }
   ],
   "source": [
    "# Auswertung des Modells nach dem Training\n",
    "eval_results = trainer.evaluate()\n",
    "wandb.log({\"eval_loss\": eval_results[\"eval_loss\"]})  # Loggt den eval_loss explizit\n",
    "print(eval_results)  # Gibt die Evaluierungs-Ergebnisse aus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dec93196",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96a7e3abb99141f6ac20f9c06374749b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 2 LFS files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40944262d727449682f17520c794ec83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/498M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0041d36531974db8a7a576cfd5458ae5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training_args.bin:   0%|          | 0.00/5.24k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21bc1cde7aba4bcfa2895b5cea9c0189",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/1.24k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hshakademie9\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:144: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\hshakademie9\\.cache\\huggingface\\hub\\models--altkachenko11--gpt2-finetuned. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "No files have been modified since last commit. Skipping to prevent empty commit.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/altkachenko11/gpt2-finetuned/commit/ba6886d43cfbf00aab8eff4a6a4d715894796614', commit_message='Upload tokenizer', commit_description='', oid='ba6886d43cfbf00aab8eff4a6a4d715894796614', pr_url=None, repo_url=RepoUrl('https://huggingface.co/altkachenko11/gpt2-finetuned', endpoint='https://huggingface.co', repo_type='model', repo_id='altkachenko11/gpt2-finetuned'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 8. Modell und Tokenizer im Hugging Face Hub speichern\n",
    "trainer.push_to_hub()  # Speichert das Modell im Hugging Face Hub\n",
    "tokenizer.push_to_hub(\"gpt2-finetuned\")  # Speichert den Tokenizer im Hugging Face Hub\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fd77830f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f33575ee8f4448c3ba09bd16918f4c11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/579 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a79c86dfcbe843bf819cb0cea273c9fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.05M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7a1a4da1cdc4be6803ab576e0dd0bbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/506k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39a101cad38e4873bd5d3a0e0ed17325",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/494 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c37295144eba4d959d453129f3fe552d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/918 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31cf5fd1cbb8497884ff052752ceeb58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/498M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "185f7b19be0e4a12ba827ca5e3f8cead",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D(nf=2304, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=768)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D(nf=3072, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=3072)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Modell vom Hugging Face Hub laden\n",
    "model_name = \"altkachenko11/gpt2-finetuned\"\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "model.eval()  # Setzt das Modell in den Evaluierungsmodus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "04ef0772",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time the human race grew up.\n",
      "\n",
      "A hundred years ago, this planet was known only as Atlantis. It was a vast, fertile land that was the center of civilization and trade. A hundred years ago, it would be called\n"
     ]
    }
   ],
   "source": [
    "# Setze den pad_token_id (wird auf eos_token_id gesetzt, da GPT2 standardmäßig kein pad_token hat)\n",
    "model.config.pad_token_id = model.config.eos_token_id\n",
    "\n",
    "# Prompt definieren\n",
    "prompt = \"Once upon a time\"\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True).input_ids\n",
    "\n",
    "# Attention mask erstellen (für korrektes Arbeiten mit Padding)\n",
    "attention_mask = (input_ids != model.config.pad_token_id).long()  # Konvertiert in Typ long\n",
    "\n",
    "# Text generieren\n",
    "with torch.no_grad():  # Keine Gradientenberechnung während der Generierung\n",
    "    output_ids = model.generate(\n",
    "        input_ids,\n",
    "        attention_mask=attention_mask,  # Übergibt die Attention-Maske\n",
    "        max_length=50,  # Maximale Länge des generierten Texts\n",
    "        num_return_sequences=1,  # Anzahl der generierten Sequenzen\n",
    "        do_sample=True,  # Aktiviert Sampling für zufällige Textgenerierung\n",
    "        top_k=50,  # Beschränkt die Anzahl der Kandidaten pro Schritt auf die Top-k\n",
    "        top_p=0.95,  # Beschränkt die Auswahl auf die Top-p (nucleus sampling)\n",
    "        temperature=0.9  # Steuert die Zufälligkeit der Generation (niedrigerer Wert -> deterministischere Ausgabe)\n",
    "    )\n",
    "\n",
    "# Ausgabe dekodieren\n",
    "generated_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "print(generated_text)  # Gibt den generierten Text aus"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
